---
title: Cross Entropy and KL Divergence
date: Mon, 05 Sep 2016 14:12:00 +0000
math: true
---
[As we saw in an earlier post](https://tdhopper.com/blog/entropy-of-a-discrete-probability-distribution/), the entropy of a discrete probability
distribution is defined to be

![](https://images.squarespace-cdn.com/content/v1/5ea0e0117d85a34f661e1b52/1588343149060-T9GA37DIMC8HT35OT165/eq0.png)

Kullback and Leibler defined a similar measure now known as _KL divergence_.
This measure quantifies how similar a probability distribution $p$ is to a
candidate distribution $q$.

![](https://images.squarespace-cdn.com/content/v1/5ea0e0117d85a34f661e1b52/1588343165837-SWS02H9OESVQCAX2H57L/eq1.png)

$D_{KL}$ is non-negative and zero if and only if $p_i = q_i$ for all $i$.
However, it is important to note that it is not in general symmetric:

![](https://images.squarespace-cdn.com/content/v1/5ea0e0117d85a34f661e1b52/1588343178930-OSQLSXOB9DH1MRC03FGP/eq2.png)

[Jonathon Shlens explains](http://arxiv.org/pdf/1404.2000v1.pdf) that KL
Divergence can be interpreted as measuring the likelihood that samples
represented by the empirical distribution $p$ were generated by a fixed
distribution $q$. If $D_{KL}(p\| q)=0$, we can guarantee that $p$ is generated
by $q$. As $D_{KL}(p\| q)->âˆž$, we can say that it is increasingly unlikely that
$p$ was generated by $q$.

Algebraically, we can rewrite the definition as

View fullsize

![](https://images.squarespace-cdn.com/content/v1/5ea0e0117d85a34f661e1b52/1588344001784-DZCLL2807TADDMX7VHDJ/eq3.png)

KL Divergence breaks down as something that looks similar to entropy (but
combining $p$ and $q$) minus the entropy of $p$. This first term is often
called _cross entropy_ :

![](https://images.squarespace-cdn.com/content/v1/5ea0e0117d85a34f661e1b52/1588343256112-4OW2PUU9PR1S14LUEOVY/eq4.png)

We could alternatively use this relationship to define cross entropy as:

![](https://images.squarespace-cdn.com/content/v1/5ea0e0117d85a34f661e1b52/1588343268806-VD0C0F1C54MFS2XMAA6J/eq5.png)

Intuatively, the cross entropy is the uncertainty implicit in $H(p)$ plus the
likelihood that $p$ could have be generated by $q$. If we consider $p$ to be a
fixed distribution, $H(p, q)$ and $D_{KL}(p \| q)$ differ by a constant factor
for all $q$.
