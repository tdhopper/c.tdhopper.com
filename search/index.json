[{"content":"I started college as a physics major, but I switched to computer science my sophomore year when I realized I didn\u0026rsquo;t want to take more lab classes. After a year as a CS major, I realized I loved the theory classes but didn\u0026rsquo;t care as much about the practical and programming portions of the curriculum. Going into my junior year, I switched‚Äîfor the last time‚Äîto major in math.\nDuring the winter of my junior year, I applied to a bunch of math Research Experience for Undergraduates programs at schools around the country. I was thrilled to be accepted to the program at Rochester Institute of Technology organized by Darren Narayan for the summer of 2007.\nI headed to Rochester starry eyed about spending the summer doing math research. On my first day, I found out I\u0026rsquo;d be working with the inimitable mathematician Stanis≈Çaw P. Radziszowski on computational combinatorics with another student researcher, Evan Heidtmann. The first question Dr. Radziszowski had for us was \u0026ldquo;How are your programming skills?\u0026rdquo; This was not what I expected\u0026ndash;or wanted\u0026ndash;to hear at a math research program.\nWe spent the summer investigating the Ramsey number $R_4(C_4)$. This is the smallest number $n$ such that a complete graph with $n$ vertices where the edges are colored by four colors is guaranteed to have a monochromatic cycle with 4 vertices.\nAt the time, the number was known to be either 18 or 19. Due to combinatorial explosion, it\u0026rsquo;s not possible to enumerate every graph 4-coloring for graphs of this size. A graph with 18 vertices has 153 edges which could be colored in $4^{153}$ ways (ignoring isomorphisms). This is a very, very big number.\nSince the number was one of two options, one way to prove it had to be 19 would be to find a complete graph with 18 vertices where the edges are colored by four colors while not containing a 4-cycle subgraph in one color. Evan and I spent the summer hunting for this mythical graph coloring by writing C code to generate and check graphs.\nBefore showing up in Rochester, I had no experience writing C code. Fortunately, my brilliant and patient research partner Evan did, and he quickly got me up to speed enough to manipulate and generate graphs using nauty.\nI had also never used version control, but Evan was running a version control (I don\u0026rsquo;t recall which one) platform on his home server, so we were able to easily version and collaborate on our code.\nWe also had access to fifty Unix machines in the RIT computing lab, so this summer introduced me to the Unix command line, shell scripting, and distributed computing. I wrote bash scripts that would send our processing jobs out to these machines and aggregate results back to a host node.\nWell, we failed to solve our problem. It turns out the solution was actually published by Chinese mathematicians while we were working on it. $R_4(C_4)=18$, so the graph we looked for didn\u0026rsquo;t even exist.\nNonetheless, that experience ended up being one of the most important of my higher education. I learned some of the most fundamental skills of my career (version control, working in terminal, writing fast code, and persevering through computer challenges), and I learned to enjoy writing code.\nI didn\u0026rsquo;t write much code between 2007 and starting my operations research master\u0026rsquo;s degree research in 2011, but when I started again, the skills and tenacity I developed at that REU set me up for success in grad school and joining industry as a data scientist in 2012. I wouldn\u0026rsquo;t have picked that research project had I been given a choice; I\u0026rsquo;m so glad I wasn\u0026rsquo;t asked.\n","date":"2024-09-19T13:20:00Z","image":"https://tdhopper.com/images/graphtheory.jpg","permalink":"https://tdhopper.com/blog/my-summer-doing-math-research/","title":"My summer doing math (?) research"},{"content":"Listen Links Ravi Mody on LinkedIn ChatGPT Claude.ai BBEdit Cursor Drew Conway\u0026rsquo;s Data Science Venn Diagram Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2024-09-18T00:00:00Z","image":"https://tdhopper.com/images/robot-assistant.png","permalink":"https://tdhopper.com/blog/ai-assisted-programming-with-ravi-mody/","title":"AI Assisted Programming with Ravi Mody"},{"content":"I consider myself a productive and organized person. At a minimum, people come to me for things because they know I get things done. This doesn\u0026rsquo;t come naturally to me, and it\u0026rsquo;s a skill I\u0026rsquo;ve developed over the past 15 years.\nHere are some of the things I do to be productive:\nWrite things down I loosely follow the Getting Things Done method developed by David Allen. The most important aspect to me has been capturing tasks into a single inbox. Capturing tasks means I don\u0026rsquo;t try to keep them in my head (which is unreliable and burns a lot of brain power trying to remember things). Capturing to a single inbox means tasks won\u0026rsquo;t get lost in piles of Post-Its and napkins.\nI use the Drafts app on iOS and macOS, which lets me capture notes with zero friction. If it\u0026rsquo;s a task I\u0026rsquo;m writing down, I use a Drafts action to send it to my Omnifocus inbox. I regularly triage my inbox to organize my tasks into relevant projects and contexts (e.g., tasks that can be done on the computer and tasks to do at home).\nUsing nagging reminders Tasks that must be done at a certain time aren\u0026rsquo;t fit for my Omnifocus task management systems. For example, my trash must go out on Monday nights, and I need to be reminded until it happens. I love the Due app on iOS, which will remind me repeatedly until the task is complete.\nTame email I keep my email tidy. A big part of this is using Sanebox to keep unimportant emails out of my inbox; Sanebox automatically detects, labels, and archives inbox clutter like receipts, marketing, and newsletters. These typically don\u0026rsquo;t require any action and can be triaged as necessary. I aggressively hit the \u0026ldquo;Unsubscribe\u0026rdquo; button on things I don\u0026rsquo;t need to be bothered with (including most marketing).\nWhen an email can be handled quickly (within a minute or two), I try to handle it as soon as I receive it. If an email requires work to follow up on, I forward it to my Omnifocus inbox and make it a task.\nWhen this works well, my email inbox is normally empty.\nForget passwords I have hundreds (thousands?) of logins to websites and apps. Even if I could use the same password for all of them, this is a notorious security risk.\nEvery password I use lives in my 1Password account. I also use it to manage 2-factor authentication pins, credit cards, WiFi passwords, and personal information (social security numbers, driver\u0026rsquo;s license scans, etc.). Using the 1Password keyboard shortcuts, I can quickly log in to any of my accounts.\nI know my 1Password master passwords and almost no other passwords to my accounts. They\u0026rsquo;re typically long, randomly generated mixtures of characters.\n1Password also lets me have a shared vault with my wife, which is invaluable for our shared services.\nKeep Slack under control Like many tech workers, much of my day is spent in Slack. A few years ago, I stumbled upon Michael Lopp\u0026rsquo;s guide to optimizing Slack. His approach greatly improved my experience with Slack and kept it from being a constant distraction. Go read his post.\nDon\u0026rsquo;t waste 15 minutes Like the next guy, I\u0026rsquo;m often inclined to spend 15 minutes of downtime scrolling Instagram. However, I once heard advice from a productive person that a key to productivity is not wasting those short intervals.\nWhen I have free minutes (sitting in a waiting room or during a short interval between meetings), I look for a task I can knock off my to-do list. I\u0026rsquo;m regularly surprised that daunting tasks can be finished in little time.\nDon\u0026rsquo;t waste idle brain cycles Some tasks (like writing this blog post) are hard to complete in one sitting. I have difficulty developing new ideas in front of my computer with all its distractions. On the other hand, I\u0026rsquo;m a strong believer in the power of noodling on ideas.\nWhen developing ideas, I prime my brain by dumping my thoughts on the topic into a Drafts app note and then thinking about it as I go about my day. When I have things to add, I pull up the note and jot down my ideas.\n","date":"2024-05-13T14:14:00Z","image":"https://tdhopper.com/images/todo.png","permalink":"https://tdhopper.com/blog/how-i-am-productive/","title":"How I am Productive"},{"content":" When someone says, ‚ÄúI want a programming language in which I need only say what I wish done,‚Äù give him a lollipop. (Alan Perlis, Epigrams in Programming)\nPeople love to talk trash about programming languages on Twitter. Every day there\u0026rsquo;s a new viral tweet about the inadequacies of Python.\nI have worked with many programming languages and studied others in school, but I\u0026rsquo;ve spent most of the last ten years deep into Python. Of course, Python has warts and wrinkles; I\u0026rsquo;m intimately familiar with many of them! At the same time, people worldwide effectively use Python to solve all kinds of problems. As it turns out, many common frustrations can be set aside with a little effort (e.g., by integrating modern tooling like Ruff).\nI\u0026rsquo;m regularly convinced that Brian Kernighan knew everything there is to know about software engineering practice, and he wrote it down before I was born in 1986. He doesn\u0026rsquo;t miss the mark in his 1979 paper with Plauger:\n\u0026hellip;many people try to excuse badly written programs by blaming inadequacies of the language that must be used. We have seen repeatedly that even Fortran can be tamed with proper discipline. The presence of bad features is not an invitation to use them, nor is the absence of good features an excuse to avoid simulating them as cleanly as possible. Good languages are nice, but not vital.\nPeople forget that two of the top three most visited websites (Youtube and Facebook) were originally implemented in PHP, a language rarely considered a Platonic ideal.\nBjarne Stroustrup, creator of C++, says, \u0026ldquo;There are only two kinds of languages: the ones people complain about and the ones nobody uses.\u0026rdquo; He\u0026rsquo;s right, and I\u0026rsquo;m guessing we\u0026rsquo;ll never move entirely beyond that (even as language improvement continues). I, for one, am willing to embrace this and learn to do the best with the tools we have.\n","date":"2024-04-03T11:46:00Z","image":"https://tdhopper.com/images/complaining.png","permalink":"https://tdhopper.com/blog/good-programming-languages-are-nice-but-not-vital/","title":"Good programming languages are nice but not vital"},{"content":"I‚Äôve been a professional Python developer for over a decade. Like many Python developers, I‚Äôve faced the many challenges of Python packaging and dependency management. I‚Äôve also come to love helping developers find the right tools to make their work easier and more productive.\nI\u0026rsquo;m excited to announce that I\u0026rsquo;m writing the Python Developer Tooling Handbook, a free ebook on Python developer tooling. This handbook covers a wide range of topics, including build tools, linting, formatting, dependency management, virtual environments, and more.\nThe book is currently a work in progress, and I\u0026rsquo;m excited to share it with you when it\u0026rsquo;s ready. If you want to be notified as soon as it\u0026rsquo;s released, please consider signing up for my mailing list here.\n","date":"2024-02-10T16:31:16.906Z","image":"https://tdhopper.com/images/pydevtools-announcement.png","permalink":"https://tdhopper.com/blog/announcing-the-python-developer-tooling-handbook/","title":"Announcing the Python Developer Tooling Handbook"},{"content":"Good ideas are hard to come by, but I find I\u0026rsquo;m often looking in the wrong places.\nWhen I\u0026rsquo;m trying to develop new ideas, sitting at my desk staring at my computer (and all the world\u0026rsquo;s distractions) seems to be the worst place. Instead, I like to prime my mind by writing down whatever thoughts I have on the topic and then trying to ruminate as I go about life.\nI\u0026rsquo;m inspired by stories of brilliant people coming up with ideas at surprising times:\nHirotugu Akaike said the idea for his eponymous Akaike Information Criterion while taking his seat on the bus. Grant Wood, most famous for painting American Gothic, once said \u0026ldquo;All the really good ideas I\u0026rsquo;ve ever had came to me while I was milking a cow.\u0026rdquo; Stanis≈Çaw Ulam, a key physicist in the Manhattan Project, recounted how he thought of Monte Carlo methods while home sick and playing solitaire. (Monte Carlo methods essential in modern artificial intelligence model training!) Paul Dirac, a founder of quantum mechanics, said, \u0026ldquo;I would like to mention that I found the best ideas usually came, not when one was actively striving for them, but when one was in a more relaxed state.\u0026rdquo; Of course, this method can be abused. I had a classmate in grad school who used to use \u0026ldquo;you can\u0026rsquo;t force inspiration\u0026rdquo; as an excuse to never do any work. I try to balance these anecdotes with Stephen King\u0026rsquo;s proverb: \u0026ldquo;Amateurs sit and wait for inspiration, the rest of us just get up and go to work.\u0026rdquo;\n","date":"2024-02-05T23:48:54.527Z","image":"https://tdhopper.com/images/inspiration.png","permalink":"https://tdhopper.com/blog/on-inspiration/","title":"On Inspiration"},{"content":"In 1986, Fred Brooks published \u0026ldquo;No Silver Bullet‚ÄîEssence and Accident in Software Engineering\u0026rdquo; where he argues that there is no silver bullet that \u0026ldquo;to make software costs drop as rapidly as computer hardware costs do\u0026rdquo;.\nSoftware progress doesn\u0026rsquo;t follow Moore\u0026rsquo;s Law.\nManagers characterize software projects as \u0026ldquo;usually innocent and straightforward, but\u0026hellip; capable of becoming a monster of missed schedules, blown budgets, and flawed products.\u0026rdquo;\nOuch.\nThe beginning of hope [in creating software], however, is realizing \u0026ldquo;that progress would be made stepwise, at great effort, and that a persistent, unremitting care.\u0026rdquo;\nSoftware\u0026rsquo;s difficulty has two aspects \u0026ldquo;essence, the difficulties inherent in the nature of software, and accidents, those difficulties that today attend its production but are not inherent.\u0026rdquo;\n‚ö†Ô∏èBrooks\u0026rsquo; central thesis:\nI  ô·¥á ü…™·¥á·¥†·¥á ·¥õ ú·¥á  ú·¥Ä Ä·¥Ö ·¥ò·¥Ä Ä·¥õ ·¥è“ì  ô·¥ú…™ ü·¥Ö…™…¥…¢ s·¥è“ì·¥õ·¥°·¥Ä Ä·¥á to be the specification, design, and testing of this conceptual construct, not the labor of representing it and testing the fidelity of the representation.\n‚ñ∂ Essential challenges of software have 4 aspects: complexity, conformity, changeability, and invisibility.\nComplexity: \u0026ldquo;Software entities are more complex for their size than perhaps any other human construct because no two parts are alike.\u0026rdquo; Because complexity is essential to software, in trying to abstract away complexity, you \u0026ldquo;often abstract away its essence.\u0026rdquo; Conformity: Our software is often forced to conform to other interfaces (perhaps because it comes after hardware, or perhaps simply because its \u0026ldquo;perceived as the most conformable\u0026rdquo;). \u0026ldquo;This complexity cannot be simplified out by any redesign of the software alone.\u0026rdquo; Changeability: \u0026ldquo;A ü ü s·¥ú·¥Ñ·¥Ñ·¥áss“ì·¥ú ü s·¥è“ì·¥õ·¥°·¥Ä Ä·¥á …¢·¥á·¥õs ·¥Ñ ú·¥Ä…¥…¢·¥á·¥Ö.\u0026rdquo; When software succeeds, people push the boundaries of what it was designed for. And the physical world around software changes, so software must adapted (e.g. a new model of hardware is released). Invisibility: \u0026ldquo;The reality of software is not inherently embedded in space.\u0026rdquo; Representations like directed graphs generally prove inadequate for fully representing software. Progress has been made in removing ùëéùëêùëêùëñùëëùëíùëõùë°ùëéùëô difficulties of software:\nHigh-level languages Time sharing systems Unified programming environments Brooks saw other things coming that could improve software building, but he was skeptical about how monumental they would be: artificial intelligence, graphical programming, better code editors (e.g. language specific features), faster workstations.\nSome means of attacking the ùëíùë†ùë†ùëíùëõùë°ùëñùëéùëô complexity\nBuy vs. build: \u0026ldquo;The cost of software has always been development cost, not replication cost.\u0026rdquo; Thus, if we ùëêùëéùëõ buy it, we probably should.\nRequirements refinement and rapid prototyping: \u0026ldquo;T ú·¥á  ú·¥Ä Ä·¥Ö·¥ás·¥õ s…™…¥…¢ ü·¥á ·¥ò·¥Ä Ä·¥õ ·¥è“ì  ô·¥ú…™ ü·¥Ö…™…¥…¢ ·¥Ä s·¥è“ì·¥õ·¥°·¥Ä Ä·¥á s ès·¥õ·¥á·¥ç …™s ·¥Ö·¥á·¥Ñ…™·¥Ö…™…¥…¢ ·¥ò Ä·¥á·¥Ñ…™s·¥á ü è ·¥° ú·¥Ä·¥õ ·¥õ·¥è  ô·¥ú…™ ü·¥Ö.\u0026rdquo; \u0026ldquo;Therefore, the most important function that the software builder performs for the client is the ùëñùë°ùëíùëüùëéùë°ùëñùë£ùëí ùëíùë•ùë°ùëüùëéùëêùë°ùëñùëúùëõ ùëéùëõùëë ùëüùëíùëìùëñùëõùëíùëöùëíùëõùë° ùëúùëì ùë°‚Ñéùëí ùëùùëüùëúùëëùë¢ùëêùë° ùëüùëíùëûùë¢ùëñùëüùëíùëöùëíùëõùë°ùë†.\u0026rdquo;\nBrooks again:\nI would go a step further and assert that it is really impossible for a client, even working with a software engineer, to specify completely, precisely, and correctly the exact requirements of a modern software product before trying some versions of the product.\nTo serve iterative requirements gathering, a goal should be getting something working as quickly as possible.\n(A side benefit of this is it improves morale for developers. \u0026ldquo;Enthusiasm jumps when there is a running system, even a simple one.\u0026rdquo;)\nGreat software design comes from great designers. Software is a creative process. \u0026ldquo;Sound methodology can empower and liberate the creative mind; it cannot inflame or inspire the drudge.\u0026rdquo; Organizations would do well to focus on identifying great software designers and working hard to \u0026ldquo;grow great designers\u0026rdquo;.\nBrooks:\nSkepticism is not pessimism, however. \u0026hellip; A disciplined, consistent effort to develop, propagate, and exploit these innovations should indeed yield an order-of-magnitude improvement. There is no royal road, but there is a road.\nRead all of Brooks\u0026rsquo; \u0026ldquo;No Silver Bullet\u0026rdquo; here.\nsilver bullet flickr photo by eschipul shared under a Creative Commons (BY-SA) license\n","date":"2022-11-18T20:15:17Z","image":"https://tdhopper.com/silver_bullet.png","permalink":"https://tdhopper.com/blog/no-silver-bullet/","title":"No Silver Bullet"},{"content":"Listen Links Jowanza\u0026rsquo;s Pulsar talks Jowanza\u0026rsquo;s Twitter Jowanza\u0026rsquo;s website Pulsar homepage The Log: What every software engineer should know about real-time data\u0026rsquo;s unifying abstraction by Jay Kreps StreamNative - Hosted Pulsar Datastax Astra Streaming - Hosted Pulsar Cloud Native Computing Foundation Pulsar Functions Pulsar IO Pulsar SQL Apache ZooKeeper Apache BookKeeper Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2022-02-07T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/apache-pulsar-with-jowanza-joseph/","title":"Apache Pulsar with Jowanza Joseph"},{"content":"Over the last 6 years, I\u0026rsquo;ve been able to help the teams I have been part of develop guidelines for code review. Many teams require \u0026ldquo;code review\u0026rdquo; without putting any effort into establishing a common understanding of what that means. This post is adapted from proposed guidelines I prepared for one of my teams. Of course, you don\u0026rsquo;t have to adopt my guidelines for review, but I would encourage your team to set aside some time to make sure you all mean the same thing by \u0026ldquo;code review\u0026rdquo;.\nWhat is a code review for? Code review has multiple benefits and objectives including:\nCode correctness : someone seeing your code with fresh eyes may help uncover bugs.\nCode familiarity : reading one another\u0026rsquo;s code keeps everyone familiar with the codebase.\nDesign feedback : a constantly evolving code base is a fight against complexity; reviewers can guide one another on keeping the codebase coherent and maintainable.\nMutual learning : the reviewer and author will inevitably learn from one another.\nRegression protection : future contributors to the code base have checks against breaking essential functionality; importantly, this reduces fear of making necessary improvements to the code.\nWhat code reviews aren\u0026rsquo;t An opportunity for the reviewer to impose their idiosyncrasies.\nAn opportunity for the developer to push off responsibility (e.g. correctness) for their code to the reviewer.\nAn opportunity to demand perfection (Per Google‚Äôs Code Review Guidelines: A key point here is that there is no such thing as \u0026ldquo;perfect\u0026rdquo; code‚Äîthere is only better code ).\nOpening Pull Requests Take care to write informative commit messages. This helps your reviewer understand the decisions you made.\nConsider your contribution in the broader context of the code base. Do you need to take extra steps to make the code healthier and manage complexity?\nKeep pull requests short whenever possible. git --shortstat origin/main will show you the size of your branch \u0026rsquo;s diff from main; under 400 lines changed is a great goal.\nWrite a pull request description that sets your reviewer up for success by helping them understand what the PR intends to accomplish.\nIf you have a particularly complex PR, consider doing a code walk-through with a reviewer first. New code should ordinarily come with new tests.\nReviewing Pull Requests Have a positive, constructive, helpful attitude.\nWait for continuous integration tasks to complete. Let the author resolve any test failures before beginning your review.\nAs much as possible, configure your continuous integration to enforce your team\u0026rsquo;s style guidelines and look for line-level bugs. In Python, this might include running mypy, flake8, black, and isort. Automation like this has multiple benefits: they\u0026rsquo;re often better than humans at this task, they reduce cognative load on the reviewer, and they reduce interpersonal tension that results from interviewers nitpicking code. Things to evaluate:\nDoes the code appear to do what it claims to do? (This requires you understanding what the code claims to do; you may need to ask the code author to write a better description.)\nWas the new code put in the right place?\nIs the new code unnecessarily complex‚Äîor unnecessarily clever?\n\u0026ldquo;Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it. \u0026quot; ‚Äì Brian Kernighan Does the new code do all it can to avoid adding to the overall complexity of our codebase?\nDid the author write tests for the new code?\nClarify when a comment is minor or not essential for merging (for example, preface with \u0026ldquo;Nit: \u0026ldquo;).\nIf a PR is too large for you to reasonably review, you can ask the author to split it into multiple PRs.\nAdditional Reading Google \u0026rsquo;s excellent guide to code review (Note: CL=change list=pull request)\nCurated list of articles about code review\nGitlab \u0026rsquo;s Code Review Guidelines\nProven Code Review Best Practices from Microsoft\nThoughtbot Code Review Guidelines\nCode reviews at Slack\n[Computers](https://flickr.com/photos/duanestorey/2721991488 \"Computers\") flickr photo by [Duane Storey](https://flickr.com/people/duanestorey) shared under a [Creative Commons (BY-NC-ND) license](https://creativecommons.org/licenses/by-nc-nd/2.0/) ","date":"2021-12-03T15:37:00Z","image":"https://tdhopper.com/images/code-review.png","permalink":"https://tdhopper.com/blog/code-review-guidelines/","title":"Code Review Guidelines for Data Science Teams"},{"content":"Listen Links Brenton\u0026rsquo;s Twitter Brenton\u0026rsquo;s Instagram Brenton\u0026rsquo;s Website Ocean Engineering at Florida Atlantic University Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2021-09-15T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/ocean-engineering-to-data-science/","title":"Ocean Engineering to Data Science"},{"content":"Listen Links Roy\u0026rsquo;s Twitter Roy\u0026rsquo;s Book Data Scientists at Work Data Scientist: The Sexiest Job of the 21st Century by Thomas H. Davenport and D.J. Patil Building Data Science Teams by DJ Patil (2011) Adversarial Learning: Stories of Degradation and Humiliation ‚Äî Podcast about bad interview experiences Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2021-06-30T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/hiring-data-scientists-with-roy-keyes/","title":"Hiring Data Scientists with Roy Keyes"},{"content":"On January 7, 2011, I tweeted, \u0026ldquo;Trying to learn Python. We\u0026rsquo;ll see if this keeps up once classes start.\u0026rdquo;\nOn January 20, 2011, ten years ago today, I bought Mark Lutz\u0026rsquo;s Learning Python from O\u0026rsquo;Reilly Books. Over the next month, I read it on my Kindle on the Stairmaster at the school gym. It changed the course of my adult life: since reading that book, I\u0026rsquo;ve written Python on more days than I haven\u0026rsquo;t.\nBefore discovering the data science Twitter community in late 2010, I\u0026rsquo;d only heard of Python through another student researcher in my 2007 REU, who had used Python for some graphics programming research. On Twitter, people like Hilary Mason, John Cook, and Chris Fonnesbeck talked warmly about using Python in their scientific work.\nIn 2011, I was a first-year operations research student at North Carolina State University, and I realized that I should pick up programming again to improve my career prospects. I\u0026rsquo;d taught myself some PHP in high school (circa 2003). In undergrad, I did a computer science minor and learned a good bit of C++ and did a lot of Mathematica scripting in my math coursework, but from 2008 to 2011, I basically didn\u0026rsquo;t program.\nI struggled to use Python in practice for ML/scientific computing in those days before wheels (binary installs); libraries like Scipy and Numpy required brittle compilation of C++ and Fortran dependencies. During the summer of 2011, I gave up using Python (after segfaulting Orange) for my internship at Kiva Systems and dove into R, where I could install packages more reliably and use ggplot2.\nFor some reason (probably because of Twitter), I returned to Python a year later and used it to write my research code in grad school (despite my advisor\u0026rsquo;s wishes that I use C++). Scipy Superpack for installing the Scipy stack was invaluable (thankfully now replaced by Wheels and Conda).\nMy experience with Python was a big reason I got hired at RTI International when I left my PhD program in October. They were looking to reduce their SAS dependency (and costs) and wanted people experienced with open-source tools. I taught my colleagues a \u0026ldquo;Python for statisticians\u0026rdquo; seminar soon after joining RTI.\nFrom RTI, I joined Parsely, which was like a Python boot camp working with Andrew Montalenti and others. Parsely uses Python across their full stack, and it was an eye-opening and educational year for me.\nSince then, I\u0026rsquo;ve worked at a variety of companies where I\u0026rsquo;ve been able to use Python for training machine learning models, building machine learning platforms, writing Gibbs samplers for nonparametric Bayes, building data engineering pipelines, software testing, etc.\nI use Python almost every single day for work and a lot of personal projects. I\u0026rsquo;ve been able to speak at 3 PyData conferences, a Scipy conference, and a number Triangle Python Users Groups. I had a contribution merged into CPython in 2019 and have contributed to many other open-source projects.\nI\u0026rsquo;m grateful to the countless people who have taught me (through tweets, code reviews, conference talks, etc) about Python and the many who have built the wonderful language with its incredible ecosystem of tools and packages that enable me and others to do so many things.\n","date":"2021-01-20T13:58:16Z","image":"https://tdhopper.com/images/minimalist-landscape-with-mouse-and-snake.png","permalink":"https://tdhopper.com/blog/learning-python/","title":"The Programming Book That Made My Career"},{"content":"Listen Links Willem\u0026rsquo;s Twitter Adam\u0026rsquo;s Twitter Feast: feature store for Machine Learning (2020 talk) Feast Tecton GoJek on Wikipedia featurestore.org Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2021-01-16T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/feature-stores-with-willem-pienaar/","title":"Feature Stores with Willem Pienaar"},{"content":"Listen Links Adam\u0026rsquo;s Twitter Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2020-10-10T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/evolution-of-a-data-scientist-with-adam-laiacano/","title":"Evolution of a Data Scientist with Adam Laiacano"},{"content":"Git has a git check-ignore -v \u0026lt;pathname\u0026gt; option which will explain to you why a given file is ignored by the repository. This is helpful for debugging .gitignore and other exclude options.\nI hope you never need this, but you\u0026rsquo;ll be glad if you do.\n","date":"2020-09-18T13:10:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/git-check-ignore/","title":"git check-ignore"},{"content":"Listen Links Tala Security Mike\u0026rsquo;s Twitter Mike\u0026rsquo;s LinkedIn How I Became a Data Scientist Despite Being a Math Major Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2020-07-23T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/customer-focused-data-science-with-mike-rogers/","title":"Customer Focused Data Science with Mike Rogers"},{"content":"Listen Links Fizz Buzz in Tensorflow Livecoding Madness - Let\u0026rsquo;s Build a Deep Learning Library I don\u0026rsquo;t like notebooks Live Coding the Advent of Code Data Science from Scratch Second Edition Fizz Buzz Book: Meditations on Python, mathematics, science, engineering, and design Joel\u0026rsquo;s Twitter Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2020-07-15T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/fizz-buzz-book-with-joel-grus/","title":"Fizz Buzz Book with Joel Grus"},{"content":"In the great #general room\nThere was idle chatter\nAnd a red build\nAnd a gif of-\nThe cow jumping over the moon\nAnd there were three little interns sitting with mentors\nAnd two little commits\nAnd a backlog of tickets\nAnd a little blocker\nAnd a fresh Docker\nAnd a PM and a DM and a glass full of beer\nAnd a loud young salesman who was typing @here\nGoodnight Slack room\nGoodnight Zoom\nGoodnight manager hopping on Zoom\nGoodnight Jira\nAnd the red build\nGoodnight interns\nGoodnight mentors\nGoodnight AWS\nAnd goodnight Bash\nGoodnight PMs\nAnd goodnight DMs\nGoodnight little blocker\nAnd goodnight Docker\nGoodnight commits\nAnd goodnight tickets\nGoodnight TypeError: null is not an object\nGoodnight project\nAnd a loud young salesman who was typing ‚Äú@here‚Äù\nGoodnight Github stars\nGoodnight asserts\nGood night pager duty alerts\n","date":"2020-06-23T11:30:43Z","image":"https://tdhopper.com/goodnight-zoom.png","permalink":"https://tdhopper.com/blog/goodnight-zoom/","title":"Goodnight Zoom"},{"content":"I\u0026rsquo;ve been working remotely for tech startups for six years, so I am sharing some tips for those working from home for the first time.\nStart Working Early\nIf you can\u0026rsquo;t find someone to chat with for 30 minutes while making coffee, consider starting your workday instead. Simulate Office Noise\nIf you miss the background noise of your office, try listening to 8 hours of power drill noises here. For a more realistic ambiance, you can overlay it with the most annoying laugh ever here. Afternoon Activities\nWithout a ping-pong table at home, you might find yourself with nothing to do but work in the afternoons. Familiarize Yourself with Remote Tools\nProgrammers, take some time to get used to the essential tools for remote work, such as Jira, GitHub, Slack, and Zoom. Engage in Local Politics\nIf the quarantine lasts longer than expected and you start to miss petty workplace politics, consider joining your local HOA. ","date":"2020-03-16T08:04:00Z","image":"https://tdhopper.com/images/office_ping_pong_game_illustration.png","permalink":"https://tdhopper.com/blog/tips-for-working-from-home/","title":"Tips for Working from Home"},{"content":"Listen Links Josh\u0026rsquo;s twitter Oscar\u0026rsquo;s twitter Josh on Software Engineering Daily Scalding Summingbird Apache Crunch Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician.\n\u0026mdash; Josh Wills (@josh_wills) May 3, 2012 Subscribe RSS Feed Apple Podcasts Spotify Overcast Google Podcasts ","date":"2020-02-19T00:00:00Z","image":"https://tdhopper.com/images/podcast.png","permalink":"https://tdhopper.com/blog/ten-years-of-data-science-with-josh-wills-and-oscar-boykin/","title":"Ten Years of Data Science with Josh Wills and Oscar Boykin"},{"content":"A few weeks back, I was browsing Google Maps and noticed the word \u0026ldquo;DEMO\u0026rdquo; written in faint green letters in rural South Sudan. A journalist came across this tweet and wrote about it for bbc.com.\nü§î https://t.co/TOJvHsxdD9 pic.twitter.com/UtHG6Jpgxz\n\u0026mdash; Tim Hopper (@tdhopper) May 28, 2019 Way back in 2015, I tweeted \u0026ldquo;Give a man a fish and you feed him for a day. Write a program to fish for him and you maintain it for a lifetime.\u0026rdquo; Today, that tweet appeared in a nature.com article about the challenges of releasing open source software.\nGive a man a fish and you feed him for a day.\nWrite a program to fish for him and you maintain it for a lifetime.\n\u0026mdash; Tim Hopper (@tdhopper) November 3, 2015 ","date":"2019-07-01T13:40:00Z","image":"https://tdhopper.com/images/computer-fishing.png","permalink":"https://tdhopper.com/blog/tweets-in-the-news/","title":"My Tweets in the News"},{"content":"I use pyenv to manage Python versions on my Mac. I recently have gotten errors like\n1 WARNING: The Python sqlite3 extension was not compiled. Missing the SQLite3 lib? and\n1 zipimport.ZipImportError: can\u0026#39;t decompress data; zlib not available The solution seems to be setting LDFLAGS and CPPFLAGS to point to the sqlite3 and zlib libraries, e.g.:\n1 2 3 4 5 6 brew install sqlite3 brew install zlib export LDFLAGS=\u0026#34;-L/usr/local/opt/zlib/lib -L/usr/local/opt/sqlite/lib\u0026#34; export CPPFLAGS=\u0026#34;-I/usr/local/opt/zlib/include -I/usr/local/opt/sqlite/include\u0026#34; pyenv install 3.7.0 ","date":"2019-05-30T00:00:00Z","permalink":"https://tdhopper.com/blog/installing-python/","title":"Installing Python on Mohave with pyenv"},{"content":"I\u0026rsquo;m a big fan of using pdb, the Python interactive debugger in conjunction with Pytest as I\u0026rsquo;m writing code.1 With the --pdb flag, you can have Pytest drop into pdb when a test fails. With pytest.set_trace(), you can selectively enter pdb while running your tests. (I use this where I might\u0026rsquo;ve just added print statements in the past.)\nA colleague uses IPython for most of his interactive development and asked me for help with some of the friction in his workflow. I recommended pdb, but he wanted to stick with the familiar IPython repl for much of his work.2 He figured how to drop into an IPython repl from pdb with from IPython import embed; embed(). You can\u0026rsquo;t move up and down the call stack until you exit the repl, but you have access to all the local state you have in pdb.2\nTo simplify this, I recommended leveraging two little-known pdb features: alias and .pdbrc. alias allows you to set alias for statements in pdb; in this case ipy as an alias that drops into IPython:\n1 alias ipy from IPython import embed; embed() pdb also as the ability to load a config file from the user\u0026rsquo;s home directory or the current working directory. Each line in the file is just a pdb statement. Thus, if you can create ~/.pdbrc and add the alias statement above, the ipy command becomes available in every pdb session for your user.\nI highly recommend pip installing pdbpp, which replaces the default pdb with an enhanced debugger with syntax highlighting, tab completion, and more.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThere\u0026rsquo;s also an alternative debugger for Python based on IPython: ipdb.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-02-13T00:00:00Z","permalink":"https://tdhopper.com/blog/ipython-pdb/","title":"Access a IPython repl from pdb"},{"content":"I gave a talk at Pydata DC 2018 where I tried to articulate some reasons why companies building machine learning products under-invest in engineering and architecture. I\u0026rsquo;m very interested in feedback, pointers to other resources on this topic, and a general discussion about how to make more effective ML products.\nOpen in Google Docs\nAnd the video:\n","date":"2019-01-29T00:00:00Z","permalink":"https://tdhopper.com/blog/right-code/","title":"Right Code, Right Place, Right Time"},{"content":"I gave a talk at Scipy 2018 loosely based on my Ansible tutorial. Here are my slides:\nOpen in Google Docs\nAnd the video:\n","date":"2018-07-12T00:00:00Z","permalink":"https://tdhopper.com/blog/ansible-talk/","title":"Devops Empowered Data Science with Ansible"},{"content":"I gave a talk called Challenges in Applying Machine Learning to Cybersecurity at BSides Asheville. Here are my slides:\nAnd the video:\n","date":"2018-06-23T00:00:00Z","permalink":"https://tdhopper.com/blog/ml-for-security/","title":"Challenges in Applying Machine Learning to Cybersecurity"},{"content":" On two occasions I have been asked, ‚Äî \u0026lsquo;Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?\u0026rsquo; In one case a member of the Upper, and in the other a member of the Lower, House [of British Parliament] put this question.\nI am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.\n‚Äî Charles Babbage, Passages from the Life of a Philosopher (1864)\n","date":"2018-05-29T00:00:00Z","permalink":"https://tdhopper.com/blog/wrong-figures/","title":"Wrong Figures, Right Answers?"},{"content":"In 2014, Sebastian Gutierrez published a collection of interviews entitled Data Scientists at Work. My friend and former boss Eric Jonas posted his interview on his website. It\u0026rsquo;s full of gems.\nOn engineering skills required for data science work, Eric says,\nOn the industry side, I think that the ability to do software engineering is something that is very important, but isn‚Äôt really taught. You don‚Äôt actually learn it as a computer science undergraduate, and you certainly don‚Äôt learn it as a graduate student. So for me it‚Äôs very important that someone has learned it somehow‚Äîeither by themselves or from someone else. I basically can‚Äôt hire people who don‚Äôt know Git.\nOn someone trained in pure mathematics learning to analysis of real-world data, Eric says:\n\u0026hellip;data analysis is so much messier than actual math. I have friends who work on these topology-based approaches, and I‚Äôm like, ‚ÄúYou realize these manifolds totally evaporate when you actually throw noise into the system. How do you think this is really going to play out here?‚Äù So I would much rather someone be computationally skilled. I‚Äôm willing to trade off what their Putnam score was for how many open source GitHub projects they‚Äôve committed to in the past.\nI tried to argue this same point in an earlier post.\nOn applying academic research, Eric observes:\nFor example, when I evaluate machine learning papers, what I am looking to find out is whether the technique worked or not. This is something that the world needs to know‚Äîmost papers don‚Äôt actually tell you whether the thing worked. It‚Äôs really infuriating because most papers will show five dataset examples and then show that they‚Äôre slightly better on two different metrics when comparing against something from 20 years ago. In academia, it‚Äôs fine. In industry, it‚Äôs infuriating, because you need to know what actually works and what doesn‚Äôt.\nI have suggested before that we need a good website for sharing implementations of academic algorithms and providing a forum for discussion of whether or not the algorithm actually works.\nI highly recommend reading Eric\u0026rsquo;s full interview.\n","date":"2018-05-22T00:00:00Z","permalink":"https://tdhopper.com/blog/data-scientists-at-work/","title":"I Basically Can't Hire People Who Don't Know Git"},{"content":"Ron Chernow on George Washington\u0026rsquo;s struggle with mail:\nWashington devoted far more time to the onerous task of draft letters than leading men into battle. Running an embryonic government, he protested to Congress that he and his aides \u0026ldquo;are confined from morn till eve, hearing and answering the applications and letters of one and another,\u0026rdquo; leaving him with \u0026ldquo;no hours for recreation.\u0026rdquo; He groaned at the huge stacks of correspondence and felt besieged by supplicants for various favors. At times the enormous quantity of paperwork must have seemed more daunting than British arms.\n","date":"2018-03-06T13:34:00Z","image":"https://tdhopper.com/images/george-washington.png","permalink":"https://tdhopper.com/blog/george-washington-inbox-zero/","title":"George Washington's Struggle for Inbox Zero"},{"content":"Three years ago, I wrote a post called \u0026ldquo;How I Became a Data Scientist Despite Being a Math Major\u0026rdquo;.\nWhen I wrote the post, I thought I was explaining that that was about all I know about how someone can become a data scientist: that is, I shared my subjective experience. I intended to communicate my uncertainty about the path others should take. But, given the number of people who have read the post and emailed asking for my advice on becoming a data scientist, that message wasn\u0026rsquo;t clear. With most of these emails, I have felt bad because I simply don\u0026rsquo;t know the answers to the questions people have. But because so many people seem to have these questions, I decided to consolidate my uncertainty here. I hope you find it helpful.\nA note before the Q\u0026amp;A: one reason it is so hard to advise someone on becoming a \u0026ldquo;data scientist\u0026rdquo; is that \u0026ldquo;data scientist\u0026rdquo; is an ill-defined job title. At some companies, data scientists are building machine learning models and running them in a high performance production system. At other data companies, data scientists are business analysts running SQL queries and visualizing the results with Tableau. Some data scientist are doing complex experimental design while others are mostly moving files around S3 buckets. Some data science roles require deep domain expertise, others only need some programming skill and knowledge of Scikit-Learn. As you shape your own path to data science, take some time to think specifically about the kind of work you are interested in doing; this will shape your preparation.\nShould I get a masters degree? My sense is that a good masters degree in a technical field is a valuable degree. It takes a fraction of the time a Ph.D. takes, yet you can learn a lot in 3 or 4 semesters of coursework, and the degree is viewed favoribly by employers. The quality and curriculum of masters programs varies wildly, so you\u0026rsquo;ll want to do your due diligence before diving in.\nThat said, I\u0026rsquo;m wary of people going into deep debt for a masters degree. If you can get a teaching or research position that waives your tuition, do it. Consider going a good school on in-state tuition instead of a great school for $100,000. If you have publicly subsidized graduate education (as in many European countries), go for it!\nWill a masters degree help me become/be a data scientist? I don\u0026rsquo;t know, but it has helped me. As I said in my earlier post, I learned a lot about algorithms, probability models, math, and machine learning that has been invaluable. Grad school also gave me the time and inspiration to learn R and Python which have played important roles in my career?\nShould I get a masters degree in computer science? Will it help me become a data scientist? I think a masters degree in computer science will be likely to pay off in the long run. It may not help you get a job as a data scientist, but it would undoubtedly help you in a data science job.\nI have half of a computer science masters, and I sometimes wish I had finished it.\nShould I get a masters degree in operations research? I\u0026rsquo;m ambivalent about operations research. As a discipline, operations research has been in an identity crisis. As a curriculum, many operations research programs are full of content valuable for data scientists. As a signal to potential employers, operations research is relatively unknown and won\u0026rsquo;t mean as much as a degree in \u0026ldquo;machine learning\u0026rdquo; or similar.\nDid you feel that operations research was too theoretical and that you had to hustle outside the classroom and build stat/ML skills separately? Operations research programs have wildly different curriculums. Mine didn\u0026rsquo;t do a great job preparing me for real-world applications, but that might be better learned on the job anyway. My program allowed me to build statistics and ML expertise only because I had flexibility in the courses I could select; I was able to take a handful of stats/ML related classes. Other programs might not offer that.\nShould I get a masters degree in statistics? If you have strong programming/software engineering skills (or have another means of building them), a statistics degree could be valuable; as with anything, I\u0026rsquo;m sure the quality of statistics masters degrees varies greatly, and it\u0026rsquo;s worth trying to find a good one.\nShould I get a Ph.D.? I don\u0026rsquo;t think it\u0026rsquo;s worth it for most people. It\u0026rsquo;s also not necessary for the vast majority of data science jobs. I have a dedicated website to help you answer this question.\nThere are a lot of people with Ph.D.\u0026rsquo;s in data science roles. It\u0026rsquo;s possible that is more a result of the large difference in the number Ph.D.\u0026rsquo;s verses the small number of permanent faculty positions in American universities. People with Ph.D.\u0026rsquo;s need jobs; many have skills overlapping with data science; they make their way from academia to data science.\nCan I become a data scientist with only an undergraduate degree? Many others have. I have known several who don\u0026rsquo;t even have college degrees.\nI\u0026rsquo;m in school should I take X class? If it\u0026rsquo;s linear algebra, definitely. Otherwise, I\u0026rsquo;m not sure. Among other reasons, I\u0026rsquo;ve been often surprised how classes I never expected to apply have helped me years later; it\u0026rsquo;s hard for me to know what other classes would\u0026rsquo;ve helped me had I taken them.\nTake the best professors you can (note I didn\u0026rsquo;t say easiest ). Talk to older students you admire about different classes and professors. Don\u0026rsquo;t let your schooling interfere with your education.\nCan you evaluate my qualifications for being a data scientist? Not very well. In fact, I think it\u0026rsquo;s pretty challenging even for people who interview data science candidates. I have tried to share the things that helped qualify me, and I imagine those things would be valuable for you as well.\nI would suggest trying to evaluate your qualifications against specific jobs (or job descriptions) you are interested in. Are you more interested in analysis or production systems? Are you interested cybersecurity applications? Ad markets? Social good? Journalism? Finance? Healthcare? Self-driving cars? Find job postings for roles and look at the qualifications. Find people in these roles on Linkedin and look at their qualifications and job history.\nOne other note: just because you\u0026rsquo;re qualified, doesn\u0026rsquo;t mean you will get job offers. Not getting an offer after interviewing might reflect more on poor interviewers than being a poor candidate.\nHow do I show-on-my-resume/demonstrate-to-employers that I am qualified to be a data scientist? My best advice is to work on interesting and relevant things and tell people about them. I don\u0026rsquo;t know how to be more specific.\nHow can I get a job where I can do more applied math? Even in data science jobs, a lot of the work is far removed from interesting math. My hypothesis is that relatively few people get to spend a substantial part of their job thinking about interesting math. I\u0026rsquo;ve almost never gotten to spend as much time doing math as I would like.\nWhat skills would you recommend I develop if I hope to become a data scientist? You can never be a good enough writer, communicator, software engineer, linear algebraist, or applied statistician. Tenacity is important too, though I\u0026rsquo;m not sure how you develop it.\nAm I doing the right things to build a successful career in data science? I don\u0026rsquo;t know the answer to that question. I have tried to share the things that have been valuable for me in the preceding answer and in my blog post.\nI\u0026rsquo;m in a career as a teacher/developer/analyst/etc. Can you advise me on how I can transition to be a data scientist? I have tried to share the things that worked for me; you might be able to emulate them, but I can\u0026rsquo;t guarantee they\u0026rsquo;ll work for you. I would encourage you to stay curious, keep learning, network (via the internet and face to face), and keep applying for jobs.\nCan we find a time to talk on the phone about this? Unfortunately, I don\u0026rsquo;t have the time and energy to do this.\nThanks to Roy Keyes, Vicki Boykis, and Justin Bozonier for helpful feedback on a draft of this post.\n","date":"2018-03-05T16:59:00Z","image":"https://tdhopper.com/becoming.png","permalink":"https://tdhopper.com/blog/faq/","title":"A Subjective and Anecdotal FAQ on Becoming a Data Scientist"},{"content":" I have always believed that scientific research is another domain where a form of optimism is essential to success: I have yet to meet a successful scientist who lacks the ability to exaggerate the importance of what he or she is doing, and I believe that someone who lacks a delusional sense of significance will wilt in the face of repeated experiences of multiple small failures and rare successes, the fate of most researchers.\n‚Äî Daniel Kahneman, [Thinking, Fast and Slow](Thinking Fast and Slow)\n","date":"2018-02-13T00:00:00Z","permalink":"https://tdhopper.com/blog/optimism/","title":"Optimism is essential to scientific success"},{"content":"I created a new website devoted to my photography.\nI named it dothopper photography in honor of my late grandmother, Dot Hopper, who fell in love with painting watercolors in retirement.\n","date":"2018-02-09T00:00:00Z","image":"https://tdhopper.com/images/camera.png","permalink":"https://tdhopper.com/blog/dothopper-photography/","title":"dothopper photography"},{"content":"Ten years ago today, John Cook published his first blog post entitled Moore‚Äôs law and software bloat, a brief observation on how \u0026ldquo;Software bloat has increased at roughly the same rate as Moore‚Äôs law\u0026rdquo;.\nSince then, he‚Äôs written over 2,700 posts (nearly 1 per day) on math, computing, software development, statistics, science, and more. His posts are rarely long, but they always give me something to think about. Over the last six years since I discovered his blog, John has encouraged me to\nengage with dead writers not look to diplomas for magic take care in inverting matrices cherish analytic functions be slow to say \u0026ldquo;they has too much time on their hands‚Äù second guess the need to always be in front of a screen to be a programmer use Bayesian methods to better order search results of customer-rated items consider the benefits of portable, open-source tools have humility when analyzing data be skeptical of ‚Äúelementary‚Äù statistical books not feel dumb for being frustrated with statistics notation think carefully about how read code reevaluate what it would take for software to be bug free appreciate the massive importance of organization in software projects understand the real challenges of effectively applying mathematics wonder if time-spent-at-desk is a good proxy for getting work done better understand why software projects fail realize that it‚Äôs sometimes better not to produce more code realize automating might help me more by saving mental energy than by saving time John earned his Ph.D. in applied math at UT Austin in 1992 with a dissertation on partial differential equations entitled Diffusion models with microstructure and secondary flux. In the years since, he has worked as a math professor at Vanderbilt, a software developer, a research statistician at M.D. Anderson Cancer Center, and, since 2013, an independent consultant in statistics, mathematics, and HIPAA expert determination. His breadth of career experience reflect his breadth of interests that make his blog what it is.\nThough John has (wisely) avoided labeling himself a data scientist (preferring very applied mathematician), his writings have shaped every aspect of my trajectory from being a operations research grad student to a practicing data scientist. John is an expert in Bayesian statistics, yet has taught me much humility about the power of statistics. John is a expert programmer, yet points me back to the importance of soft skills for the success of software projects. John loves applications of mathematics, yet reminds me to step back and see the intrinsic beauty of mathematics.\nThe most common tags on his posts are bayesian, creativity, differential equations, education, history, latex, math, networks, number theory, probability and statistics, productivity, programming, python, quotes, scipy, and special functions. He also brings a unique approach in his posts on music and has a neat collection of interviews he\u0026rsquo;s done with Fred Brooks, Michael Atiyah and others.\nJohn‚Äôs blog is not his only educational outreach. He maintains 17 (!) Twitter accounts where he teaches about technical topics in 140 280 characters or less. I\u0026rsquo;m always learning little tidbits from @UnixToolTip and find my schooling refreshed by @AlgebraFact and @AnalysisFact.\nOne of John‚Äôs areas of expertise is implementing numerical algorithms in code. On his website, he has a handful of technical notes on this that many people have found invaluable. Two years ago, I did a quick survey of John‚Äôs articles mentioned in Github repos. Hundreds of repos cited his Accurately computing running variance article. No doubt countless more have used his resources without citing him!\nIf you don\u0026rsquo;t follow John\u0026rsquo;s blog, you should subscribe today.\nHere\u0026rsquo;s to 10 more years, John!\nCongrats to @JohnDCook on 10 years of blogging!https://t.co/DdMBjI89eV\n‚Äî Tim Hopper üÜò (@tdhopper) January 9, 2018\n","date":"2018-01-10T01:28:00Z","permalink":"https://tdhopper.com/blog/ten-year-endeavor/","title":"John Cook‚Äôs Ten Year Blogging Endeavour"},{"content":" The downside of this is you have to have two production stacks\u0026hellip; which is kind of expensive, but it\u0026rsquo;s dirt cheap compared to amount of money people spend on developers hacking away at things that are making them crazy. This is one of those things that drives me crazy about companies\u0026hellip; We will gladly spend a million dollars in overtime in developers to sit and work on our antiquated infrastructure, but we won\u0026rsquo;t spend a thousand dollars to buy a new machine to make them not have to do do that.\n‚Äî Neal Ford, \u0026ldquo;Automation\u0026rdquo;, Neal Ford on Agile Engineering Practices\n","date":"2017-10-16T00:00:00Z","permalink":"https://tdhopper.com/blog/neal-ford-on-companies-ignoring-developer-costs/","title":"Neal Ford on Companies Ignoring Developer Costs"},{"content":"A few years ago, I wrote a posted called [/how/](How I Became a Data Scientist Despite Having Been a Math Major: The Talk). I recently gave a talk on the same topic at the Demystify Data Science Conference.\n","date":"2017-10-06T00:00:00Z","permalink":"https://tdhopper.com/blog/how-i-became-a-data-scientist-the-talk/","title":"How I Became a Data Scientist Despite Having Been a Math Major: The Talk"},{"content":"$ git checkout this Zen of Git Ugly is better than beautiful. Explicit is better than implicit. Complex is better than simple. Complicated is better than complex. Flat is better than nested. Readability is meaningless. Special cases are everything. Errors should never be comprehensible. In the face of ambiguity, copy and paste from Stack Overflow. There should be no obvious way to do it. Although there may be endless non obvious ways to do it. If the documentation is hard to understand, it's a great idea. If the documentation is easy to understand, it's probably for another tool. (With apologies to Tim Peters.)\n","date":"2017-09-30T00:26:00Z","image":"https://tdhopper.com/images/zen.png","permalink":"https://tdhopper.com/blog/zen-of-git/","title":"Zen of Git"},{"content":"Plotting is an essential component of data analysis. As a data scientist, I spend a significant amount of my time making simple plots to understand complex data sets (exploratory data analysis) and help others understand them (presentations).\nIn particular, I make a lot of bar charts (including histograms), line plots (including time series), scatter plots, and density plots from data in Pandas data frames. I often want to facet these on various categorical variables and layer them on a common grid.\nTo that end, I made pythonplot.com, a brief introduction to Python plotting libraries and a \u0026ldquo;rosetta stone\u0026rdquo; comparing how to use them. I also included comparison to ggplot2, the R plotting library that I and many others consider a gold standard.\n","date":"2017-06-26T00:00:00Z","permalink":"https://tdhopper.com/blog/python-plotting-for-exploratory-data-analysis/","title":"Python Plotting for Exploratory Data Analysis"},{"content":"Do you ever want to be able to run a Python function in parallel on a set of inputs? Have you ever gotten frustrated with the GIL, the multiprocessing library, or joblib?\nTry this:\nInstall Python Fire to run your command from the command line Install Python Fire with $ pip install fire.\nAdd this snippet to the bottom of your file:\n1 2 3 if __name__ == \u0026#39;__main__\u0026#39;: import fire fire.Fire() Install GNU Parallel $ brew install parallel or $ sudo apt-get install parallel may work for you. Otherwise, see this.\nRun your function from the command line $ parallel -j3 \u0026quot;python python_file.py function_name {1} \u0026quot; ::: input1 input2 input3 input4 input5\nparallel is the command for GNU Parallel. -j3 tells Parallel to run at most 3 processes at once. {1} fills in each item after the ::: as an argument to the function_name. For example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (lazy) ~ $ cat python_file.py from time import sleep def function_name(arg1): print(\u0026#34;Starting to run with\u0026#34;, arg1) sleep(2) print(\u0026#34;Finishing to run with\u0026#34;, arg1) if __name__ == \u0026#39;__main__\u0026#39;: import fire fire.Fire() (lazy) ~ $ parallel -j3 --lb \u0026#34;python -u python_file.py function_name {1} \u0026#34; ::: input1 input2 input3 input4 input5 Starting to run with input2 Starting to run with input1 Starting to run with input3 Finishing to run with input2 Finishing to run with input1 Finishing to run with input3 Starting to run with input4 Starting to run with input5 Finishing to run with input4 Finishing to run with input5 I added --lb and -u to keep Python and Parallel from buffering the output so you can see it being run in parallel.\n","date":"2017-06-07T00:00:00Z","permalink":"https://tdhopper.com/blog/parallelizing-a-python-function-for-the-extremely-lazy/","title":"Parallelizing a Python Function for the Extremely Lazy"},{"content":"I was getting this message when I tried to install packages from conda-forge with Conda:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Fetching package metadata ... CondaHTTPError: HTTP 401 UNAUTHORIZED for url \u0026lt;https://conda.anaconda.org/conda-forge/osx-64/repodata.json\u0026gt; Elapsed: 00:00.920954 CF-RAY: 36ad7cbd5d1c23d8-IAD The remote server has indicated you are using invalid credentials for this channel. If the remote site is anaconda.org or follows the Anaconda Server API, you will need to (a) remove the invalid token from your system with `anaconda logout`, optionally followed by collecting a new token with `anaconda login`, or (b) provide conda with a valid token directly. Further configuration help can be found at \u0026lt;https://conda.io/docs/config.html\u0026gt;. I tried to do $ anaconda logout but didn\u0026rsquo;t have a program called anaconda installed.\nYou can install the Anaconda Cloud Client with $ conda install anaconda-client.\nAfter that, I was able to do $ anaconda logout followed by $ anaconda login where I used my old Binstar credentials (now anaconda.org).\nI\u0026rsquo;m not the only one having this problem.\n","date":"2017-06-06T00:00:00Z","permalink":"https://tdhopper.com/blog/condahttperror-http-401-unauthorized-for-url/","title":"CondaHTTPError: HTTP 401 UNAUTHORIZED for url"},{"content":"Richard Bellman quoted by Stuart Dreyfus via Garrett Jones:\nI spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. ‚ÄúAn interesting question is, ‚ÄòWhere did the name, dynamic programming, come from?‚Äô The 1950s were not good years for mathematical research.\nWe had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word, research. I‚Äôm not using the term lightly; I‚Äôm using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term, research, in his presence. You can imagine how he felt, then, about the term, mathematical.\nThe RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons.\nI decided therefore to use the word, ‚Äòprogramming.‚Äô I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying‚ÄîI thought, let‚Äôs kill two birds with one stone. Let‚Äôs take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it‚Äôs impossible to use the word, dynamic, in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It‚Äôs impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.\n","date":"2017-05-26T00:00:00Z","permalink":"https://tdhopper.com/blog/choice-of-the-name-dynamic-programming/","title":"Choice of the Name Dynamic Programming"},{"content":"My friends Andrew and Joel were kind enough to have me back on their podcast Adversarial Learning. We shared our tales of bad data science interviews. Enjoy!\n","date":"2017-05-22T00:00:00Z","image":"https://tdhopper.com/images/interview.png","permalink":"https://tdhopper.com/blog/stories-of-degradation-and-humiliation/","title":"Adversarial Learning: Stories of Degradation and Humiliation"},{"content":"I gave a talk at the Data Science Conference on on building a realtime machine learning system with Kafka, Streamparse, and Storm. You can see the video on Youtube\n","date":"2017-05-17T00:00:00Z","permalink":"https://tdhopper.com/blog/realtime-machine-learning/","title":"Build a Real Time Machine Learning System"},{"content":"From the Autobiography of Benjamin Franklin:\nThomas Godfrey, a self-taught mathematician, great in his way, and afterward inventor of what is now called Hadley\u0026rsquo;s Quadrant. But he knew little out of his way, and was not a pleasing companion; as, like most great mathematicians I have met with, he expected universal precision in every-thing said, or was for ever denying or distinguishing upon trifles, to the disturbance of all conversation.\nI\u0026rsquo;m a recovering Godfrey Precisionist.\n","date":"2017-04-26T00:00:00Z","permalink":"https://tdhopper.com/blog/like-most-great-mathematicians-he-expects-universal-precision/","title":"Like most great mathematicians, he expects universal precision"},{"content":"This Software Engineering Radio interview with Neal Ford on Success Skills for Architects is full of gems about building effective software.\nHe talks a lot about how coders love to solve problems, and that love can lead them to invent interesting, but unnecessary, problems to solve. This is true.\nMetawork is more interesting than work. It\u0026rsquo;s so hard to get back to simplicity, because we love complicated little puzzles to solve, so we keep overengineering everything.\nAnyone who\u0026rsquo;s developing software would benefit from listening.\n","date":"2017-04-14T00:00:00Z","permalink":"https://tdhopper.com/blog/metawork-is-more-interesting-than-work/","title":"Metawork is more interesting than work"},{"content":"Staying focused while working in front of a computer and within reach of a smartphone is hard.\nIn 2017, teaching people to focus is becoming a industry.\nI\u0026rsquo;ve been trying to rethink distractions in my own life, particularly in my work environment. Here are some things that have helped:\nWorking from Home Working in an office, especially an open-floor plan office, is disastrous for staying focused. DeMarco and Lister wrote about this in Peopleware 30 years ago, and yet open offices are the norm for startups today.\nI\u0026rsquo;m much more productive by working from home in my quiet office or on my back patio. I\u0026rsquo;m finally able to spend my time thinking about hard problems rather than ways of silencing Constant Throat Clearer or Perpetual Annoying Laugher.\nNotifications Every app and website these days wants to send you notifications. I\u0026rsquo;m aggressive about reducing notifications down to those that I need see, and I let almost nothing notify me with sound. I use Do Not Disturb mode on my phone and Mac whenever I need to stop notifications altogether.\nSlack Slack has become the new normal for company communication. Some would say Slack itself is ruining our focus, but having it regularly available has been essential for my own work.\nI\u0026rsquo;ve come up with a few ways to take control of Slack:\nOnly show \u0026ldquo;My unread, along with everything I\u0026rsquo;ve starred\u0026rdquo; in the sidebar. See Michael Lopp\u0026rsquo;s excellent post on Slack for more here. Enable notifications selectively. Sign out of distracting avocational Slacks. Social Media I\u0026rsquo;ve started using an app called Focus to block distracting websites (including Facebook and Twitter.com) and apps on my work computer from 9 AM to 5:30 PM. I use Focus\u0026rsquo;s scheduling feature so blocking isn\u0026rsquo;t optional for me.\nI\u0026rsquo;ve decided not to block Tweetbot. Though it can be distracting, Twitter is an invaluable way for me to learn from my professional colleagues, bounce ideas off of them, and have a good laugh.\nOn my iPhone, iPad, and personal Laptop, I\u0026rsquo;ve started using Freedom to block all social media during the day. This has stopped me from instinctively checking Instagram every time I walk to the bathroom or get suck on a hard problem. I highly recommend it.1\nI also use Freedom to block social media for the first hour I\u0026rsquo;m up in the morning and before I go to bed.\nEmail I have two main tactics to keep email from being distracting.\nI aggressively unsubscribe from mailing lists and ads. I use Sanebox to filter low priority messages out of my inbox. When emails only need a brief reply, I tend to write responses as soon as possible. At the moment, I\u0026rsquo;m trying to break people of the expectation that I\u0026rsquo;ll respond quickly. Using services like Boomerang which lets me write emails now and have them sent later helps here.\nReading Long-form reading at the computer is terrible for comprehension. As Doug Lemov has argued, you have to get away from your computer and other devices to read deeply. I do this by printing articles or reading on my iPad with Freedom blocking enabled. I take my printouts or iPad and walk away from my desk to read.\nTodo Items I\u0026rsquo;m a firm believer in the Getting Things Done principle of reducing the cognitive overhead of tracking to-do items in my head. I use Omnifocus for task management. Mail Drop and this Alfred workflow help me to quickly add tasks to my Omnifocus inbox. When I think of something I need to take care of outside of work, I drop that thought into Omnifocus; this keeps those personal to-do items from distracting me while I\u0026rsquo;m working.\nStaying focused is hard. I\u0026rsquo;m still learning how to do it well, and I\u0026rsquo;m sure I\u0026rsquo;m not the only one struggling to improve here. If you have any tips to share, I\u0026rsquo;d love to hear them!\nI can\u0026rsquo;t use Freedom on my work computer, because it acts as a VPN which conflicts with my work VPN.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2017-04-13T00:00:00Z","permalink":"https://tdhopper.com/blog/towards-reducing-distractions-while-working/","title":"Towards Reducing Distractions while Working"},{"content":"I\u0026rsquo;ve been tinkering with websites for nearly 20 years. My friend Hunter and I were big into making terrible Angelfire sites as pre-teens. In high school, my dad paid me to make him a webpage for his doctor\u0026rsquo;s office (I used Frontpage). A year or two after that, I read Kevin Yank\u0026rsquo;s \u0026ldquo;Build Your Own Database Driven Website Using PHP \u0026amp; MySQL\u0026rdquo; and hacked together a PHP back-end for a Lord of the Rings fan site.\nIn recent years, I\u0026rsquo;ve put together this blog, shouldigetaphd.com, and a few other simple web-based side projects. However, I haven\u0026rsquo;t kept up with modern web development, and my projects have been hacked together from boilerplate or templates. I\u0026rsquo;ve programmed professionally since 2011, I\u0026rsquo;ve spent very little of that writing anything close to graphical user interfaces.\nI have a number of other side projects that I\u0026rsquo;d like to do at some point, and most of them would require some sort of graphical interface. While I could work on app development, I think web-based implementations would be a great starting place.\nA few months back, I decided to stop watching Netflix on the treadmill and instead use those 45 minutes each morning to learn; in particular, I\u0026rsquo;ve been trying to learn more about modern(ish) web design and development. My work has a subscription to Safari Books Online which gives me access to copious technical books and video tutorials.\nThe number of resources available on Safari (along with YouTube, blog posts, etc) is astounding. I started many video tutorials on Safari that I quickly realized weren\u0026rsquo;t going to be useful. Yet there many gems to be found, which I share here with you.\nWhat follows is an overview of the technologies I\u0026rsquo;ve realized I need to learn more about and links to the resources I\u0026rsquo;ve found valuable in learning about them. If you think there are gaps I haven\u0026rsquo;t yet filled or better resources than I\u0026rsquo;ve listed below, I\u0026rsquo;d love your feedback.\nWhat I Knew Going In I\u0026rsquo;ve been a professional software developer and data scientist since 2012. I mostly write Python, but I\u0026rsquo;ve programmed in a number of different languages.\nI have a pretty good grasp on how HTML and CSS work. I\u0026rsquo;ve used enough Javascript over the years to be dangerous; I understood how it runs in the browsers. I understand what a DOM is and how it relates to the page source.\nI\u0026rsquo;ve used the Python Flask web framework for several projects. I understand how to repond to HTTP requests with server- generated content. I had some idea of how to run my own web server on AWS.\nI\u0026rsquo;ve used Jekyll, Hugo, and Pelican to create statically generated sites.\nI understood DNS at a high level, but never really learned what all the different DNS types were, and I didn\u0026rsquo;t understand why name server changes take so long to propagate.\nI had some idea of what node.js and npm are.\nI\u0026rsquo;m a committed Sublime Text user.\nA Meta Tutorial on Web Development A great place to start is Andrew Montalenti\u0026rsquo;s lengthy tutorial on using Python, Flask, Bootstrap, and Mongo to rapidly prototype a website. The tutorial is out of date, but the principles still stand.\nAnother great resource is Cody Lindley\u0026rsquo;s free Front-End Developer\u0026rsquo;s Handbook. This is a substantial list meta-resource that organizes links for learning all angles of front-end development. \u0026ldquo;It is specifically written with the intention of being a professional resource for potential and currently practicing front-end developers to equip themselves with learning materials and development tools.\u0026rdquo;\nChrome Developer Tools One of the most important tools for me in learning more about web development has been the Chrome Developer Tools. You can live edit the DOM elements and style sheets and watch how a website changes. I\u0026rsquo;ve mostly learned Developer Tools through exploring it myself, but there are lots of tutorials for it on Youtube.\nHTML, CSS, and Bootstrap Many modern websites are responsive: they automatically adapt to various size screens and devices, from phones to desktops. Writing responsive websites from scratch requires deep knowledge of HTML, CSS, Javascript, and browsers. Unless you\u0026rsquo;re doing this professionally, you probably don\u0026rsquo;t want to write a responsive site from scratch.\nFor several projects, I\u0026rsquo;ve used the lightweight Skeleton project to create simple, responsive pages.\nRecently, I decide to dive deep into the more robust Bootstrap framework originally developed at Twitter.\nI watched Brock Nunn\u0026rsquo;s Building a Responsive Website with Bootstrap (Safari), a two hour tutorial on getting started with Bootstrap. The documentation for Bootstrap is clear (if terse) and worth reading through.\nOnce you have a basic idea of how Bootstrap works, the best thing you can do is start playing with it. Since I was familiar with the Pelican static site generator, I decided to switch this blog to Bootstrap theme starting with pelican-bootstrap3.\nI\u0026rsquo;ve worked with Bootstrap 3 until now. Bootstrap 4 is about to come out. Bootstrap 4 moves the style sheets from LESS to SASS and adds Flexbox functionality. Unless you understand what those mean (more below), you\u0026rsquo;d be fine using version 3.\nI wanted to get a better grasp on CSS Selectors, so I read Eric Meyer\u0026rsquo;s brief Selectors, Specificity, and the Cascade: Applying CSS3 to Documents (Safari)\nI watched Marty Hall\u0026rsquo;s JavaScript, jQuery, and jQuery UI tutorial). I was able to skip big chunks where I already understood certain parts, but it helped me fill in lots of gaps.\nAdvanced Stylesheets (LESS, SASS, and Flexbox) There are several alternatives to writing raw CSS. Two popular ones are Less and SASS. These \u0026ldquo;preprocessors\u0026rdquo; allow you to write CSS-like stylesheets but with constructs such as variables, nesting, inheritance, and mathematical operators.\nI found this brief tutorial on Less (Safari) helpful, and I\u0026rsquo;ve enjoyed Less a lot. I haven\u0026rsquo;t used SASS yet, but it\u0026rsquo;s very similar. I\u0026rsquo;ll probably switch to SASS when I start using Bootstrap 4.\nAnother modern innovation is the Flexbox layout model for CSS. Stone River Learning has a great tutorial on Flexbox (Safari). It seems that Flexbox is the future of CSS-based layouts, and it\u0026rsquo;s worth learning about.\nAdvanced JavaScript (Elm, React, Angular, Backbone, Ember) The JavaScript web framework space has exploded. Many of these are implementations of the Model, View, Controller pattern, including React, Angular, and Ember. These tools allow the creation of complex web apps (as well as mobile apps).\nWeb Server Operations and DNS I learned a ton form Linux Web Operations (Safari) by Ben Whaley. \u0026ldquo;The videos discuss the relationship between web and application servers, load balancers, and databases and introduce configuration management, monitoring, containers, cryptography, and DNS.\u0026rdquo;\nI\u0026rsquo;ve struggled with DNS configuration over the years, so I watched Cricket Liu\u0026rsquo;s Learning DNS series (Safari). I still wouldn\u0026rsquo;t want to be responsible for a company\u0026rsquo;s complex DNS infrastructure, but I can now configure my own sites DNS with a little more understanding.\nDevelopment Automation Package Managers It\u0026rsquo;s likely that any modern web project will have some external Javascript dependencies. Package managers (analogous to Pypi or Anaconda.org on Python) have emerged to help support this. Node.js comes with the npm package manager, but Bower seems to make more sense for front-end development.[^yarn] Cody Lindley has a nice introduction to npm and Bower. Bower is well documented and easy to start using. There is a nice Flask extension to help you integrate Bower with your Python project. (Update: since writing this, Yarn has come to dominate this scene.)\nTask Automation Web development comes with lots of build-style tasks that have to happen repeatedly. For example, before you can render a webpage in the browser, you might need to convert the Less to CSS and start a local web server. Before deploying to production, you might want to also run tests and minify your Javascript.\nThere\u0026rsquo;s a GUI application called Codekit that can do a lot of these tasks. You can also do it through a Node.js program called Grunt. I haven\u0026rsquo;t used it yet, but it looks like following the documentation would be the best way to get started.\nGulp is a popular alternative to Grunt.\nDesign Visual Design Design has never been my strong point. One way to compensate for that is to rely on the work of others. There are copious Bootstrap themes available, and some are even free.\nI enjoyed Software Engineering Daily\u0026rsquo;s interview with Tracy Osborn on Design for Non-designers. She has some blog posts on the topic. Tracy recommends COLOURLovers for color ideas and Font Pair for selecting fonts from Google Fonts.\nUser Experience Design On the topic of UX, I finally read Steve Krug\u0026rsquo;s classic Don\u0026rsquo;t Make Me Think (Safari); it\u0026rsquo;s great. Ginny Redish\u0026rsquo;s Letting Go of Words (Safari) is similarly excellent.\nConclusion I\u0026rsquo;ve learned a lot in the past few months. I\u0026rsquo;ve filled in some gaps about how CSS works. I\u0026rsquo;ve gotten a better grasp on the Javascript prototype model. I\u0026rsquo;ve learned that I can start with higher level tools (e.g. Bootstrap and JQuery) to rapidly build my side projects with some amount of visual appeal. I\u0026rsquo;m learning how to use available tools to reduce the boilerplate I have to write, automate tedious tasks, and reduce my personal technical debt.\nI still have a lot of learning and a lot of practicing ahead of me, but I\u0026rsquo;m starting to feel confident that I could make headway on some of my projects. The modern frontend development landscape is massive, varied, and ever changing, but that shouldn\u0026rsquo;t prohibit you from diving in if you want to.\n","date":"2017-03-31T18:42:00Z","permalink":"https://tdhopper.com/blog/web-development-and-design-for-the-backend-developer/","title":"Web Development and Design for the Backend Developer"},{"content":"I wrote a few months back about how data scientists need more automation. In particular, I suggested that data scientists would be wise to learn more about automated system configuration and automated deployments.\nIn an attempt to take my own advice, I\u0026rsquo;ve finally been making myself learn Ansible. It turns out that a great way to learn it is to sit down and read through the docs, front to back; I commend that tactic to you. I also put together this tutorial to walk through a practical example of how a working data scientist might use this powerful tool.\nWhat follows is an Ansible guide that will take you from installing Ansible to automatically deploying a long-running Python to a remote machine and running it in a Conda environment using supervisord. It presumes your development machine is on OS X and the remote machine is Debian-like; however, it shouldn\u0026rsquo;t require too many changes to run it on other systems.\nI wrote this post in a Jupyter notebook with a Bash kernel. You can find the notebook, Ansible files, and installation directions on my Github.\nAnsible Ansible provides \u0026ldquo;human readable automation\u0026rdquo; for \u0026ldquo;app deployment\u0026rdquo; and \u0026ldquo;configuration management\u0026rdquo;. Unlike tools like Chef, it doesn\u0026rsquo;t require an agent to be running on remote machines. In short, it translates declarative YAML files into shell commands and runs them on your machines over SSH.\nAnsible is backed by Red Hat and has a great website.\nInstalling Ansible with Homebrew First, you\u0026rsquo;ll need to install Ansible. On a Mac, I recommend doing this with Homebrew.\nbrew install ansible Warning: ansible-2.1.0.0 already installed Warning: You are using OS X 10.12. We do not provide support for this pre-release version. You may encounter build failures or other breakages. Quickstart Soon, I\u0026rsquo;ll show you how to put write an Ansible YAML file. However, Ansible also allows you specify tasks from the command line.\nHere\u0026rsquo;s how we could use Ansible ping our local host:\nansible -i 'localhost,' -c local -m ping all ansible -i 'localhost,' -c local -m ping all localhost | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } This command calls ansible and tells it:\nTo use localhost as it\u0026rsquo;s inventory (-i). Inventory is Ansible speak for machine or machines you want to be able to run commands on. To connect (-c) locally (local) instead of over SSH. To run the ping module (-m) to test the connection. To run the command on all hosts in the inventory (in this case, our inventory is just the localhost). Michael Booth has a post that goes into more detail about this command.\nBehind the scenes, Ansible is turning this -m ping command into shell commands. (Try running with the -vvv flag to see what\u0026rsquo;s happening behind the scenes.) It can also execute arbitrary commands; by default, it\u0026rsquo;ll use the Bourne shell sh.\nansible all -i 'localhost, ' -c local -a \u0026quot;/bin/echo hello\u0026quot; Setting up an Ansible Inventory Instead of specifying our inventory with the -i flag each time, we should specify an Ansible inventory file. This file is a text file specifying machines you have SSH access to; you can also group machines under bracketed headings. For example:\nmail.example.com [webservers] foo.example.com bar.example.com [dbservers] one.example.com two.example.com three.example.com Ansible has to be able to connect to these machines over SSH, so you will likely need to have relevant entries in your .ssh/config file.\nBy default, the Ansible CLI will look for a system-wide Ansible inventory file in /etc/ansible/hosts. You can also specify an alternative path for an intentory file with the -i flag.\nFor this tutorial, I\u0026rsquo;d like to have an inventory file specific to the project directory without having to specify it each time we call Ansible. We can do this by creating a file called ./ansible.cfg and set the name of our local inventory file:\ncat ./ansible.cfg cat ./ansible.cfg [defaults] inventory = ./hosts You can check that Ansible is picking up your config file by running ansible --version.\nansible --version ansible --version ansible 2.1.0.0 config file = /Users/tdhopper/repos/automating_python/ansible.cfg configured module search path = Default w/o overrides For this example, I just have one host, a Digital Ocean VPS. To run the examples below, you should create a VPS instance on Digital Ocean, Amazon, or elsewhere; you\u0026rsquo;ll want to configure it for passwordless authentication. I have an entry like this in my ~/.ssh/hosts file:\nHost digitalocean HostName 45.55.395.23 User root Port 22 IdentityFile /Users/tdhopper/.ssh/id_rsa ForwardAgent yes and my intentory file (~/hosts) is just\ndigitalocean Before trying ansible, you should ensure that you can connect to this host:\nssh digitalocean echo 1 ssh digitalocean echo 1 1 Now I can verify that Ansible can connect to my machine by running the ping command.\nansible all -m ping ansible all -m ping digitalocean | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } We told Ansible to run this command on all specified hosts in the inventory. It found our inventory by loading the ansible.cfg which specified ./hosts as the inventory file.\nIt\u0026rsquo;s possible that this will fail for you even if you can SSH into the machine. If the error is something like /bin/sh: 1: /usr/bin/python: not found, this is because your VPS doesn\u0026rsquo;t have Python installed on it. You can install it with Ansible, but you may just want to manually run sudo apt-get -y install python on the VPS to get started.\nWriting our first Playbook While adhoc commands will often be useful, the real power of Ansible comes from creating repeatable sets of instructions called Playbooks.\nA playbook contains a list of \u0026ldquo;plays\u0026rdquo;. Each play specifies a set of tasks to be run and which hosts to run them on. A \u0026ldquo;task\u0026rdquo; is a call to an Ansible module, like the \u0026ldquo;ping\u0026rdquo; module we\u0026rsquo;ve already seen. Ansible comes packaged with about 1000 modules for all sorts of use cases. You can also extend it with your own modules and roles.\nOur first playbook will just execute the ping module on all our hosts. It\u0026rsquo;s a playbook with a single play comprised of a single task.\ncat ping.yml cat ping.yml --- - hosts: all tasks: - name: ping all hosts ping: We can run our playbook with the ansible-playbook command.\nansible-playbook ping.yml ansible-playbook ping.yml ____________ \u0026lt; PLAY [all] \u0026gt; ------------ \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || ______________ \u0026lt; TASK [setup] \u0026gt; -------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || ok: [digitalocean] _______________________ \u0026lt; TASK [ping all hosts] \u0026gt; ----------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || ok: [digitalocean] ____________ \u0026lt; PLAY RECAP \u0026gt; ------------ \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || digitalocean : ok=2 changed=0 unreachable=0 failed=0 You might wonder why there are cows on your screen. You can find out here. However, the important thing is that our task was executed and returned successfully.\nWe can override the hosts list for the play with the -i flag to see what the output looks like when Ansible fails to run the play because it can\u0026rsquo;t find the host.\nLet\u0026rsquo;s work now on installing the dependencies for our Python project.\nInstalling supervisord \u0026ldquo;Supervisor is a client/server system that allows its users to monitor and control a number of processes on UNIX-like operating systems.\u0026rdquo; We\u0026rsquo;ll use it to run and monitor our Python process.\nOn a Debian-like system, we can install it with APT. In the Ansible DSL that\u0026rsquo;s just:\n- name: Install supervisord sudo: yes apt: name: supervisor state: present update_cache: yes You can read more about the apt module here.\nOnce we have it installed, we can start it with this task:\n- name: Start supervisord sudo: yes service: name: \u0026quot;supervisor\u0026quot; state: running enabled: yes This uses the service module.\nWe could add these these tasks to a playbook file (like ping.yml), but what maybe we will want to share it among multiple playbooks? For this, Ansible has a construct called Roles. A role is a collection of \u0026ldquo;variable values, certain tasks, and certain handlers ‚Äì or just one or more of these things\u0026rdquo;. (You can learn more about variables and handlers in the Ansible docs.)\nRoles are organized as subfolders of a folder called \u0026ldquo;Roles\u0026rdquo; in the working directory. The rapid proliferation of folders in Ansible organization can be overwhelming, but a very simple rule is just a file called main.yml nestled several folders deep. In our case, it\u0026rsquo;s in ./roles/supervisor/tasks/main.yml.\nCheck out the docs to learn more about role organization.\nHere\u0026rsquo;s what our role looks like:\ncat ./roles/supervisor/tasks/main.yml cat ./roles/supervisor/tasks/main.yml --- - name: Install supervisord become: true apt: name: supervisor state: present update_cache: yes tags: supervisor - name: Start supervisord become: true service: name: \u0026quot;supervisor\u0026quot; state: running enabled: yes tags: supervisor Note that I added tags: to the task definitions. Tags just allow you to run a portion of a playbook instead of the whole thing with the --tags flag for ansible-playbook.\nNow that we have the supervisor install encapsulated in a role, we can write a simple playbook to run the role.\ncat supervisor.yml cat supervisor.yml --- - hosts: digitalocean roles: - role: supervisor ansible-playbook supervisor.yml ansible-playbook supervisor.yml _____________________ \u0026lt; PLAY [digitalocean] \u0026gt; --------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || ______________ \u0026lt; TASK [setup] \u0026gt; -------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || ok: [digitalocean] _________________________________________ \u0026lt; TASK [supervisor : Install supervisord] \u0026gt; ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || changed: [digitalocean] _______________________________________ \u0026lt; TASK [supervisor : Start supervisord] \u0026gt; --------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || changed: [digitalocean] ____________ \u0026lt; PLAY RECAP \u0026gt; ------------ \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || digitalocean : ok=3 changed=2 unreachable=0 failed=0 Installing Conda with Ansible Galaxy Next we want to ensure that Conda installed on our system. We could write our own role to follow the recommended process. However, Ansible has a helpful tool to help us avoid reinventing the wheel by allowing users to share roles; this is called Ansible Galaxy.\nYou can search the Galaxy website for miniconda and see that a handful of roles for installing Miniconda exist. I liked this one.\nWe can install the role locally using the ansible-galaxy command line tool.\nansible-galaxy install -f andrewrothstein.miniconda You can have the role installed wherever you want (run ansible-galaxy install --help to see how, but by default they\u0026rsquo;ll go to /usr/local/etc/ansible/roles/.\nls -lh /usr/local/etc/ansible/roles/andrewrothstein.miniconda ls -lh /usr/local/etc/ansible/roles/andrewrothstein.miniconda total 32 -rw-rw-r-- 1 tdhopper admin 1.1K Jan 16 16:52 LICENSE -rw-rw-r-- 1 tdhopper admin 666B Jan 16 16:52 README.md -rw-rw-r-- 1 tdhopper admin 973B Jan 16 16:52 circle.yml drwxrwxr-x 3 tdhopper admin 102B Mar 21 11:33 defaults drwxrwxr-x 3 tdhopper admin 102B Mar 21 11:33 handlers drwxrwxr-x 4 tdhopper admin 136B Mar 21 11:33 meta drwxrwxr-x 3 tdhopper admin 102B Mar 21 11:33 tasks drwxrwxr-x 3 tdhopper admin 102B Mar 21 11:33 templates -rw-rw-r-- 1 tdhopper admin 57B Jan 16 16:52 test.yml drwxrwxr-x 3 tdhopper admin 102B Mar 21 11:33 vars You can look at the tasks/main.yml to see the core logic of installing Miniconda. It has tasks to download the installer, run the installer, delete the installer, run conda update conda, and make conda the default system Python.\ncat /usr/local/etc/ansible/roles/andrewrothstein.miniconda/tasks/main.yml /main.ymllocal/etc/ansible/roles/andrewrothstein.miniconda/tasks --- # tasks file for miniconda - name: download installer... become: yes become_user: root get_url: url: '{{miniconda_installer_url}}' dest: /tmp/{{miniconda_installer_sh}} timeout: '{{miniconda_timeout_seconds}}' checksum: '{{miniconda_checksum}}' mode: '0755' - name: installing.... become: yes become_user: root command: /tmp/{{miniconda_installer_sh}} -b -p {{miniconda_parent_dir}}/{{miniconda_name}} args: creates: '{{miniconda_parent_dir}}/{{miniconda_name}}' - name: deleting installer... become: yes become_user: root when: miniconda_cleanup file: path: /tmp/{{miniconda_installer_sh}} state: absent - name: link miniconda... become: yes become_user: root file: dest: '{{miniconda_parent_dir}}/miniconda' src: '{{miniconda_parent_dir}}/{{miniconda_name}}' state: link - name: conda updates become: yes become_user: root command: '{{miniconda_parent_dir}}/miniconda/bin/conda update -y --all' - name: make system default python etc... when: miniconda_make_sys_default become: yes become_user: root with_items: - etc/profile.d/miniconda.sh template: src: '{{item}}.j2' dest: /{{item}} mode: 0644 Overriding Ansible Variables Once a role is installed locally, you can add it to a play just like you can with roles you wrote. Installing Miniconda is now as simple as:\nroles: - role: andrewrothstein.miniconda Before we add that to a playbook, I want to customize where miniconda is installed. If you look back at the main.yml file above, you see a bunch of things surrounded in double brackets. These are variables (in the Jinja2 template language). From the play, we can see that Miniconda will be installed at {{miniconda_parent_dir}}/{{miniconda_name}}. The role defines these variables in /andrewrothstein.miniconda/defaults/main.yml. We can override the default variables by specifying them in our play.\nA play to install miniconda could look like this:\n--- - hosts: digitalocean vars: conda_folder_name: miniconda conda_root: /root roles: - role: andrewrothstein.miniconda miniconda_parent_dir: \u0026quot;{{ conda_root }}\u0026quot; miniconda_name: \u0026quot;{{ conda_folder_name }}\u0026quot; I added this to playbook.yml.\nWe now know how to use Ansible to start and run supervisord and to install Miniconda. Let\u0026rsquo;s see how to use it to deploy and start our application.\nDeploy Python Application There are countless ways to deploy a Python application. We\u0026rsquo;re going to see how to use Ansible to deploy from Github.\nI created a little project called long_running_python_application. It has a main.py that writes a log line to stdout every 30 seconds; that\u0026rsquo;s it. It also includes a Conda environment file specifying the dependencies and a shell script that activates the environment and runs the program.\nWe\u0026rsquo;re going to use Ansible to\nClone the repository into our remote machine. Create a Conda environment based on the environment.yml file. Create a supervisord file for running the program. Start the supervisord job. Clone the repository Cloning a repository with Ansible is easy. We just use the git module. This play will clone the repo into the specified directory. The update: yes flag tells Ansible to update the repository from the remote if it has already been cloned.\n--- - hosts: digitalocean vars: project_repo: git://github.com/tdhopper/long_running_python_process.git project_location: /srv/long_running_python_process tasks: - name: Clone project code. git: repo: \u0026quot;{{ project_repo }}\u0026quot; dest: \u0026quot;{{ project_location }}\u0026quot; update: yes Creating the Conda Environment Since we\u0026rsquo;ve now installed conda and cloned the repository with an environment.yml file, we just need to run conda env update from the directory containing the environment spec. Here\u0026rsquo;s a play to do that:\n--- - hosts: digitalocean vars: project_location: /srv/long_running_python_process tasks: - name: Create Conda environment from project environment file. command: \u0026quot;{{conda_root}}/{{conda_folder_name}}/bin/conda env update\u0026quot; args: chdir: \u0026quot;{{ project_location }}\u0026quot; It uses the command module which just executes a shell command in the desired directory.\nCreate a Supervisord File By default, supervisord will look in /etc/supervisor/conf.d/ for configuration on which programs to run.\nWe need to put a file in there that tells supervisord to run our run.sh script. Ansible has an integrated way of setting up templates which can be placed on remote machines.\nI put a supervisord job template in the ./templates folder.\ncat ./templates/run_process.j2 cat ./templates/run_process.j2 [program:{{ program_name }}] command=sh run.sh autostart=true directory={{ project_location }} stderr_logfile=/var/log/{{ program_name }}.err.log stdout_logfile=/var/log/{{ program_name }}.out.log This is a is normal INI-style config file, except it includes Jinja2 variables. We can use the Ansible template module to create a task which fills in the variables with information about our program and copies it into the conf.d folder on the remote machine.\nThe play for this would look like:\n- hosts: digitalocean vars: project_location: /srv/long_running_python_process program_name: long_running_process supervisord_configs_path: /etc/supervisor/conf.d tasks: - name: Copy supervisord job file to remote template: src: ./templates/run_process.j2 dest: \u0026quot;{{ supervisord_configs_path }}/run_process.conf\u0026quot; owner: root Start the supevisord job Finally, we just need to tell supervisord on our remote machine to start the job described by run_process.conf.\nInstead of issuing our own shell commands via Ansible, we can use the supervisorctl module. The task is just:\n- name: Start job supervisorctl: name: \u0026quot;{{ program_name }}\u0026quot; state: present state: present ensures that Ansible calls supervisorctl reread to load a new config. Because our config has autostart=true, supervisor will start it as soon as the task is added.\nThe Big Playbook! We can take everything we\u0026rsquo;ve described above and put it in one playbook.\nThis playbook will:\nInstall Miniconda using the role from Ansible Galaxy. Install and start Supervisor using the role we created. Clone the Github project we want to run. Create a Conda environment based on the environment.yml file. Create a supervisord file for running the program. Start the supervisord job. All of this will be done on the host we specify (digitalocean).\ncat playbook.yml cat playbook.yml --- - hosts: digitalocean vars: project_repo: git://github.com/tdhopper/long_running_python_process.git project_location: /srv/long_running_python_process program_name: long_running_process conda_folder_name: miniconda conda_root: /root supervisord_configs_path: /etc/supervisor/conf.d roles: - role: andrewrothstein.miniconda miniconda_parent_dir: \u0026quot;{{ conda_root }}\u0026quot; miniconda_name: \u0026quot;{{ conda_folder_name }}\u0026quot; tags: miniconda - role: supervisor tasks: - name: Clone project code. git: repo: \u0026quot;{{ project_repo }}\u0026quot; dest: \u0026quot;{{ project_location }}\u0026quot; update: yes tags: git - name: Create Conda environment from project environment file. command: \u0026quot;{{conda_root}}/{{conda_folder_name}}/bin/conda env update\u0026quot; args: chdir: \u0026quot;{{ project_location }}\u0026quot; tags: conda - name: Copy supervisord job file to remote template: src: ./templates/run_process.j2 dest: \u0026quot;{{ supervisord_configs_path }}/run_process.conf\u0026quot; owner: root tags: conf - name: Start job supervisorctl: name: \u0026quot;{{ program_name }}\u0026quot; state: present tags: conf To configure our machine, we just have to run ansible-playbook playbook.yml.\nANSIBLE_NOCOWS=1 ansible-playbook playbook.yml ANSIBLE_NOCOWS=1 ansible-playbook playbook.yml PLAY [digitalocean] ************************************************************ TASK [setup] ******************************************************************* ok: [digitalocean] TASK [andrewrothstein.unarchive-deps : resolve platform specific vars] ********* TASK [andrewrothstein.unarchive-deps : install common pkgs...] ***************** changed: [digitalocean] =\u0026gt; (item=[u'tar', u'unzip', u'gzip', u'bzip2']) TASK [andrewrothstein.bash : install bash] ************************************* ok: [digitalocean] TASK [andrewrothstein.alpine-glibc-shim : fix alpine] ************************** skipping: [digitalocean] TASK [andrewrothstein.miniconda : download installer...] *********************** changed: [digitalocean] TASK [andrewrothstein.miniconda : installing....] ****************************** changed: [digitalocean] TASK [andrewrothstein.miniconda : deleting installer...] *********************** skipping: [digitalocean] TASK [andrewrothstein.miniconda : link miniconda...] *************************** changed: [digitalocean] TASK [andrewrothstein.miniconda : conda updates] ******************************* changed: [digitalocean] TASK [andrewrothstein.miniconda : make system default python etc...] *********** skipping: [digitalocean] =\u0026gt; (item=etc/profile.d/miniconda.sh) TASK [supervisor : Install supervisord] **************************************** ok: [digitalocean] TASK [supervisor : Start supervisord] ****************************************** ok: [digitalocean] TASK [Clone project code.] ***************************************************** changed: [digitalocean] TASK [Create Conda environment from project environment file.] ***************** changed: [digitalocean] TASK [Copy supervisord job file to remote] ************************************* changed: [digitalocean] TASK [Start job] *************************************************************** changed: [digitalocean] PLAY RECAP ********************************************************************* digitalocean : ok=13 changed=9 unreachable=0 failed=0 See that the PLAY RECAP shows that everything was OK, no systems were unreachable, and no tasks failed.\nWe can verify that the program is running without error:\nssh digitalocean sudo supervisorctl status ssh digitalocean sudo supervisorctl status long_running_process RUNNING pid 4618, uptime 0:01:34 ssh digitalocean cat /var/log/long_running_process.out.log ssh digitalocean cat /var/log/long_running_process.out.log INFO:root:Process ran for the 1th time INFO:root:Process ran for the 2th time INFO:root:Process ran for the 3th time INFO:root:Process ran for the 4th time If your lucky (i.e. your systems and networks were setup sufficiently similar to mine), you can run this exact same command to configure and start a process on your own system. Moreover, you could use this exact same command to start this program on an arbitrary number of machines by simply adding more hosts to your inventory and play spec!\nConclusion Ansible is a powerful, customizable tool. Unlike some similar tools, it requires very little setup to start using it. As I\u0026rsquo;ve learned more about it, I\u0026rsquo;ve seen more and more ways in which I could\u0026rsquo;ve used it in copious projects in the past; I intend to make it a regular part of my toolkit. (Historically I\u0026rsquo;ve done this kind of thing with hacky combinations of shell scripts and Fabric; Ansible would often be better.)\nThis tutorial just scratches the surface of the Ansible functionality. If you want to learn more, I again recommend reading through the docs; they\u0026rsquo;re very good. Of course, you should start writing and running your own playbooks as soon as possible! I also liked this tutorial from Server Admin for Programmers. If you want to compare Ansible to alternatives, the Taste Test book by Matt Jaynes looks promising. For more on Supervisor, serversforhackers.com has a nice tutorial, and its docs are thorough.\nI wrote a tutorial on using @ansible and supervisor to deploy a long running Python process to a @digitalocean VPS.https://t.co/uPC8bY5haD\n‚Äî Tim Hopper üî≠ (@tdhopper) March 24, 2017\n","date":"2017-03-23T15:11:00Z","permalink":"https://tdhopper.com/blog/automating-python-with-ansible/","title":"Automating Python with Ansible"},{"content":" ","date":"2017-03-14T21:29:00Z","permalink":"https://tdhopper.com/blog/naive-bayes/","title":"Naive Bayes Meme"},{"content":" üëâ The decision was close, but the team has decided to keep looking for someone who might have more direct neural net experience.\nüëâ Honestly, I think the way you communicated your thought process and results was confusing for some people in the room.\nüëâ He\u0026rsquo;s needing someone with an image analysis background for data scientist we\u0026rsquo;re hiring now.\nüëâ Quite honestly given your questions [about vacation policy] and the fact that you are considering other options, [we] may not be the best choice for you.\nThese quotes above are some of the reasons I\u0026rsquo;ve been given for why I wasn\u0026rsquo;t offered a data science job after interviewing. I\u0026rsquo;ve been told a variety of other reasons as well: company decided against hiring remotes after interviewing (I\u0026rsquo;ve heard this at least 3 times), company thought I changed jobs too frequently, company decided it didn\u0026rsquo;t have necessary data infrastructure in place for data science work. Multiple companies gave no particular reason; some of these were at least kind enough to notify me they weren\u0026rsquo;t interested. One company hired someone with a Ph.D. from MIT soon after turning me down.\nIn the last five years, I\u0026rsquo;ve clearly interviewed for a lot of data science jobs, and I\u0026rsquo;ve also been turned down for a lot of data science jobs. I\u0026rsquo;ve spent a good bit of time reflecting on why I wasn\u0026rsquo;t offered this job or that. Several folks have asked me if I had any advice to share on the experience, and I hope to offer that here.\nYou never really know I learned with graduate school applications years ago: you rarely truly know why you were turned down. Maybe my GRE scores weren\u0026rsquo;t high enough, or maybe the reviewer rushed through my application in the 5 minutes before lunch. Maybe my statement of interest was too weak, or maybe the department needed to accept an alumni\u0026rsquo;s child.\nThe same goes for companies. I\u0026rsquo;m fairly skeptical that the reasons I have been given for why I was passed by are the full story, and I suspect you will rarely (if ever) know the real reasons why you weren\u0026rsquo;t offered a job. I try to use the reasons I hear as a way to help me refine my skills and better present myself, but I don\u0026rsquo;t put too much weight in them.\nSome advice anyway That said, here are a few takeaways from interviewing for probably 20 data science jobs since 2012.\nCompanies often use interviews as a time to figure out what they\u0026rsquo;re really looking for. I suspect this is rarely intentional. But actually interviewing candidates forces a team to talk through what they\u0026rsquo;re actually looking for, and they often realize they had differing perspectives prior to the interview. Companies where \u0026ldquo;data science\u0026rdquo; is a new addition need your help in understanding what data science can do for them. As much as possible, use the interview to sell your vision for what data science can offer at the company, how you\u0026rsquo;ll get it off the ground, and what the ROI might be. Being the wrong fit for what a company needs is not ideal. I\u0026rsquo;ve come to appreciate a company trying to ensure my abilities align with their needs. You\u0026rsquo;d hope this was always the case, but I\u0026rsquo;ve been hired when it wasn\u0026rsquo;t. That said, I hesitate to say you should always look for this: if you need a job, and someone offers you a job, you should feel free to take it! Data infrastructure is important and many companies are lacking it. Many data scientists can attest to being hired at a company only to discover the data they needed wasn\u0026rsquo;t available, and they spent months or years building the tools required for them to start their analysis. Many companies are naive about how much engineering effort is required for effective data science. Don\u0026rsquo;t assume that a company with a grand vision for data science necessarily knows what it will take to accomplish that vision. Many companies are still uneasy about data science being done remotely. I think this is silly, but I\u0026rsquo;m biased. There\u0026rsquo;s little consistency as to what you might be asked in a data science interview. I\u0026rsquo;ve been asked about Java design patterns, how to solve combinatorics problems, to describe my favorite machine learning model, to explain the SMO algorithm, my opinions about the TensorFlow API, how I do software testing, to analyze a never-before-seen dataset and prepare a presentation in a 4 hour window, the list goes on. I spent a flight to the west coast reading up on the statistics of A/B testing only to be asked largely soft-skills type questions for an entire interview. I\u0026rsquo;ve largely given up attempting any special preparation for interviews. Networking is still king. Hiring is hard, and interviewing is hard; having a prior relationship with an applicant is attractive and reduces hiring uncertainty. In my own experience, my friendships and connections with the data science community on Twitter has shaped my career. Don\u0026rsquo;t downplay the benefits of networking. Conclusion So how do you get a data science job? I don\u0026rsquo;t know.\nI\u0026rsquo;ve been unbelievably fortunate to be continuously employed since college, but I\u0026rsquo;m not sure how to tell you to repeat that. The best I have to offer is to reiterate the conclusion of my recent talk about data science as a career. Learn and know the hard stuff: linear algebra, probability, statistics, machine learning, math modeling, data structures, algorithms, distributed systems, etc. You probably won\u0026rsquo;t use this knowledge every day in your job, but interviewers love to ask about it anyway.\nAt the same time, don\u0026rsquo;t forget about the even harder skills: communication, careful thought, prose writing skill, software writing skill, software engineering, tenacity, Stack Overflow. You will use these every day in your job, and they\u0026rsquo;ll help you present yourself well in an interview. (With the exception of Stack Overflow. Using Stack Overflow in an interview is strangely taboo.)\nFurther Reading Trey Causey: What it\u0026rsquo;s like to be on the data science job market Trey Causey: Hiring data scientists Erin Shellman: Crushed it! Landing a data science job Joel Grus: Fizz Buzz in Tensorflow ","date":"2017-03-06T17:02:00Z","image":"https://tdhopper.com/rejected.png","permalink":"https://tdhopper.com/blog/some-reflections-on-being-turned-down-for-a-lot-of-data-science-jobs/","title":"Some Reflections on Being Turned Down for a Lot of Data Science Jobs"},{"content":"Fred Benenson spent 6 years doing data science at Kickstarter. When he left last year, he wrote a fantastic recap of his experience.\nHis \u0026ldquo;list of things I\u0026rsquo;ve discovered over the years\u0026rdquo; is particularly good. Here are a few of the things that resonated with me:\nThe more you can work with someone to help refine their question the easier it will be to answer Conducting a randomized controlled experiment via an A/B test is always better than analyzing historical data Metrics are crucial to the story a company tells itself; it is essential to honestly and rigorously define them Good experimental design is difficult; don't allow a great testing framework to let you get lazy with it Data science (A/B testing, etc.) can help you how to optimize for a particular outcome, but it will never tell you which particular outcome to optimize for Always seek to record and attain data in its rawest form, whether you're instrumenting something yourself or retrieving it from an API I highly recommend reading the whole post.\n","date":"2017-02-21T00:00:00Z","permalink":"https://tdhopper.com/blog/logistic-regression-rules-everything-around-me/","title":"Logistic Regression Rules Everything Around Me"},{"content":"I recently gave to the Duke Big Data Initiative entitled Dr. Hopper, or How I Quit My Ph.D. and Learned to Love Data Science. The talk was well received, and my slides seemed to resonate in the Twitter data science community.\nI\u0026rsquo;ve started a long-form blog post with the same message, but it\u0026rsquo;s not done yet. In the mean time, I wanted to share the slides that want along with the talk.\n","date":"2017-02-14T00:00:00Z","permalink":"https://tdhopper.com/blog/how-i-quit-my-ph.d.-and-learned-to-love-data-science/","title":"How I Quit My Ph.D. and Learned to Love Data Science"},{"content":"A while ago, I published a Bash script that will open a Twitter search page to show your old tweets from this day of the year. I have enjoyed using it to see what I was thinking about in days gone by.\nSo I turned this into a Twitter account.\nFollow me to have a link tweeted @ you each day that will show your old tweets from that day of the year. #experimental\n\u0026mdash; On This Day (@your_old_tweets) January 11, 2017 If you follow @your_old_tweets, it\u0026rsquo;ll tweet a link at you each day that will show you your old tweets from the day. It attempts to send it in the morning (assuming you have your timezone set).\nThis runs on Amazon Lambda. The code is here.\n","date":"2017-01-24T00:00:00Z","permalink":"https://tdhopper.com/blog/your-old-tweets-from-this-day/","title":"Your Old Tweets from This Day"},{"content":"When you add an SSH key to your Github account, Github shows you the hexadecimal form of the MD5 hash of your public key.\nIf you ever need to compare that against a key file on your computer, you can run:\n1 ssh-keygen -E md5 -lf ~/.ssh/id_rsa.pub I learned this from StackOverflow.\n","date":"2017-01-06T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/compare-rsa-key-with-fingerprint-in-github/","title":"Compare RSA Key with Fingerprint in Github"},{"content":"From Daniel Lemire:\nI was an adept, as a teenager and a young adult, of thinkism. Thinkism is the idea that intelligence alone can solve problems. I thought I was smart so that I could just sit down and solve important problems. One after the other. Whatever contributions I ended up making had little to do with sitting down and thinking hard‚Ä¶ and much more to do with getting dirty.\n","date":"2016-12-24T15:01:00Z","image":"https://tdhopper.com/images/reading.png","permalink":"https://tdhopper.com/blog/the-perils-of-thinkism/","title":"The perils of thinkism"},{"content":"I was honored to join my friends Joel and Andrew on the Adversarial Learning podcast to talk about my career in data science (and what it\u0026rsquo;s like to be the tallest one).\n","date":"2016-12-08T00:00:00Z","image":"https://tdhopper.com/images/datascientist.png","permalink":"https://tdhopper.com/blog/tallest-data-scientist/","title":"Tallest Data Scientist"},{"content":"My former colleague\u0026rsquo;s from Parse.ly wrote the fantastic pykafka library with an optional c-backed using rdkafka. I\u0026rsquo;ve had trouble getting it to work, and here are a few things I\u0026rsquo;ve learned:\nThe version of rdkafka installable with apt-get was out of data, and pykafka couldn\u0026rsquo;t find the headers it need. I instead used the simple build instructions in the rdkafka README to build it from head. I was getting the error ImportError: librdkafka.so.1: cannot open shared object file: No such file or directory when trying to use rdkafka from Pykafka. It could be set in the short term by using LD_LIBRARY_PATH=/usr/local/lib. However, I fixed it permanently by running sudo ldconfig after building rdkafka. Pykafka has to be installed after building rdkafka. At the moment, Pykafka tries to build a C-extension to connect to rdkafka, and if that fails, it will install without offering the rdkafka backend. Check the output of pip install pykafka to see if the rdkafka extension built. ","date":"2016-11-18T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/get-pykafka-to-work-with-rdkafka-on-linux/","title":"Get Pykafka to work with rdkafka on Linux"},{"content":"Many data scientists aren\u0026rsquo;t lazy enough.\nWhether we are managing production services or running computations on AWS machines, many data scientists are working on computers besides their laptops.\nFor me, this often takes the form of SSH-ing into remote boxes, manually configuring the system with a combination of apt installs, Conda environments, and bash scripts.\nTo run my service or scripts, I open a tmux window, activate my virtual environement, and start the process. (When I have to do this on multiple machines, I\u0026rsquo;m occasionally clever enough to use tmux to broadcast the commands to multiple terminal windows.)\nWhen I need to check my logs or see the output, I SSH back into each box, reconnect to tmux (after I remember the name of my session), and tail my logs. When running on multiple boxes, I repeat this process N times. If I need to restart a process, I flip through my tmux tabs until I find the correct process, kill it with a Ctrl-C, and use the up arrow to reload the last run command.\nAll of this works, of course. And as we all know, a simple solution that works can be preferable to a fragile solution that requires constant maintenance. That said, I suspect many of us aren\u0026rsquo;t lazy enough. We don\u0026rsquo;t spend enough time automating tasks and processes. Even when we don\u0026rsquo;t save time by doing it, we may save mental overhead.\nI recently introduced several colleagues to some Python-based tools that can help. Fabric is a \u0026ldquo;library and command-line tool for streamlining the use of SSH for application deployment or systems administration tasks.\u0026rdquo; Fabric allows you to encapsulate sequences of commands as you might with a Makefile. It\u0026rsquo;s killer feature is the ease with which it lets you execute those commands on remote machines over SSH. With Fabric, you could tail all the logs on all your nodes with a single command executed in your local terminal. There are a number of talks about Fabric on Youtube if you want to learn more. One of my colleagues reduced his daily workload by writing his system management tasks into a Fabric file.\nAnother great tool is Supervisor. If you run long running processes in tmux/screen/nohup, Supervisor might be for you. It allows you to define the tasks you want to run in an INI file and \u0026ldquo;provides you with one place to start, stop, and monitor your processes\u0026rdquo;. Supervisor will log the stdout and stderr to a log location of your choice. It can be a little confusing to set up, but will likely make your life easier in the longer run.\nA tool I want to learn but haven\u0026rsquo;t is Ansible, \u0026ldquo;a free-software platform for configuring and managing computers which combines multi-node software deployment, ad hoc task execution, and configuration management\u0026rdquo;. Unlike Chef and Puppet, Ansible doesn\u0026rsquo;t require an agent on the systems you need to configure; it does all the configuration over SSH. You can use Ansible to configure your systems and install your dependencies, even Supervisor! Ansible is written in Python and, mercifully, doesn\u0026rsquo;t require learning a Ruby-based DSL (as does Chef).\nRecently I\u0026rsquo;ve been thinking that Fabric, Supervisor, and Ansible combined become a powerful toolset for management and configuration of data science systems. Each tool is also open source and can be installed in a few minutes. Each tool is well documented and offers helpful tutorials on getting started; however, learning to use them effectively may require some effort.\nI would love to see someone create training materials on these tools (and others!) focused on how data scientists can take improve their system management, configuration, and operations. A screencast series may be the perfect thing. Someone please help data scientists be lazier, do less work, and reduce the mental overhead of dealing with computers!\n","date":"2016-11-17T02:00:00Z","permalink":"https://tdhopper.com/blog/data-scientists-need-more-automation/","title":"Data Scientists Need More Automation"},{"content":"I always forget how to do this.\nThe pandas DataFrame.loc method allows for label -based filtering of data frames. The Pandas docs show how it can be used to filter a MultiIndex:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 In [39]: df Out[39]: A B C first second bar one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 baz one -1.206412 0.132003 1.024180 two 2.565646 -0.827317 0.569605 foo one 1.431256 -0.076467 0.875906 two 1.340309 -1.187678 -2.211372 qux one -1.170299 1.130127 0.974466 two -0.226169 -1.436737 -2.006747 In [40]: df.loc[\u0026#39;bar\u0026#39;] Out[40]: A B C second one 0.895717 0.410835 -1.413681 two 0.805244 0.813850 1.607920 In [41]: df.loc[\u0026#39;bar\u0026#39;, \u0026#39;two\u0026#39;] Out[41]: A 0.805244 B 0.813850 C 1.607920 Name: (bar, two), dtype: float64 It turns out you can easily use it to filter a DateTimeIndex level by a single date with df['2016-11-07'] or a range of dates with df['2016-11-07:2016-11-11']. This applies whether or not its a MultiIndex.\nIf you get an error like KeyError: 'Key length (1) was greater than MultiIndex lexsort depth (0)', it\u0026rsquo;s because \u0026ldquo;MultiIndex Slicing requires the index to be fully lexsorted\u0026rdquo;. You may fix your problem by calling df = df.sort_index().\n","date":"2016-11-08T22:17:00Z","permalink":"https://tdhopper.com/blog/filter-by-date-in-a-pandas-multiindex/","title":"Filter by date in a Pandas MultiIndex"},{"content":"I\u0026rsquo;m trying to use the NUTS sampler in PyMC3\nHowever, it was running at 2 iterations per second on my model, while the Metropolis Hastings sampler ran 450x faster.\nI showed my example to some of the PyMC3 devs on Twitter, and Thomas Wiecki showed me this trick:\n@tdhopper @Springcoil You need pm.NUTS(scaling=np.power(model.dict_to_array(v_params.stds), 2), is_cov=True) (terrible syntax, I know).\n\u0026mdash; Thomas Wiecki (@twiecki) November 8, 2016 It resulted in a 25x speedup of the NUTS sampler. The code looks like this\n1 2 3 4 5 6 7 8 9 with pm.Model() as model: # SETUP MODEL HERE mu, sds, elbo = pm.variational.advi(n=200000) step = pm.NUTS(scaling=np.power(model.dict_to_array(sds), 2)) return pm.sample(niter, step=step, is_cov=True, start=mu, ) ","date":"2016-11-08T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/speeding-up-pymc3-nuts-sampler/","title":"Speeding up PyMC3 NUTS Sampler"},{"content":"I gave a talk last week at Research Triangle Analysts on understanding probabilistic topic models (specificly LDA) by using Python for simulation. Here\u0026rsquo;s the description:\nLatent Dirichlet Allocation and related topic models are often presented in the form of complicated equations and confusing diagrams. Tim Hopper presents LDA as a generative model through probabilistic simulation in simple Python. Simulation will help data scientists to understand the model assumptions and limitations and more effectively use black box LDA implementations.\nYou can watch the video on Youtube:\nI gave a shorter version of the talk at PyData NYC 2015.\n","date":"2016-10-25T00:00:00Z","permalink":"https://tdhopper.com/blog/understanding-probabilistic-topic-models-by-simulation/","title":"Understanding Probabilistic Topic Models By Simulation"},{"content":"I was flattered to be asked to be on a burgeoning data science podcast called Undersampled Radio. You can listen here. We recorded the interview on a Google Hangout, so you can also watch it here.\n","date":"2016-10-24T00:00:00Z","permalink":"https://tdhopper.com/blog/undersampled-radio/","title":"Undersampled Radio Interview"},{"content":"First run $ sudo pip install supervisor.\nCreate /etc/supervisord.conf:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ; supervisor config file [unix_http_server] file=/var/run/supervisor.sock ; (the path to the socket file) chmod=0700 ; sockef file mode (default 0700) [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) childlogdir=/var/log/supervisor ; (\u0026#39;AUTO\u0026#39; child log dir, default $TEMP) ; the below section must remain in the config file for RPC ; (supervisorctl/web interface) to work, additional interfaces may be ; added by defining them in separate rpcinterface: sections [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///var/run/supervisor.sock ; use a unix:// URL for a unix socket ; The [include] section can just contain the \u0026#34;files\u0026#34; setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = /etc/supervisor/conf.d/*.conf [program:program1] command=program1command autostart=true stderr_logfile=/var/log/program1.err.log stdout_logfile=/var/log/program1.out.log Now run $ sudo supervisord -c /etc/supervisord.conf. See the status with $ sudo supervisorctl status.\nAfter modifying the config, run modify the config, $ sudo supervisorctl reread \u0026amp; sudo supervisorctl update.\n","date":"2016-09-20T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/setting-up-supervisord/","title":"Setting Up supervisord"},{"content":"Suppose we have a discrete set of possible events \\(1,\\ldots, n\\) that occur with probabilities \\( (p_1, p_2, \\ldots, p_n)\\). Claude Shannon asked the question\nCan we find a measure of how much \u0026ldquo;choice\u0026rdquo; is involved in the selection of the event or of how uncertain we are of the outcome?\nFor example, suppose we have a coin that lands on heads with probability \\(p\\) and tails with probability \\(1-p\\). If \\(p=1\\), the coin always lands on heads. Since there is no uncertainty, we might want to say the uncertainty is 0. However, if the coin is fair and \\(p=0.5\\), we maximize our uncertainty: it\u0026rsquo;s a complete tossup whether the coin is heads or tails. We might want to say the uncertainty in this case is 1.\nIn general, Shannon wanted to devise a function \\(H(p_1, p_2, \\ldots, p_n)\\) describing the uncertainty of an arbitrary set of discrete events (i.e. a \\(n\\)-sided die). He thought that \u0026ldquo;it is reasonable\u0026rdquo; that \\(H\\) should have three properties:\n\\(H\\) should be a continuous function of each \\(p_i\\). A small change in a single probability should result in a similarly small entropy (uncertainty) change.\nIf each event is equally likely (\\(p_i=\\frac{1}{n}\\)), \\(H\\) should increase as a function of \\(n\\): the more events there are, the more uncertain we are.\nFinally, entropy should be additive for independent events. Suppose we generate a random variable \\(x\\) by the following process: Flip a fair coin. If it is heads, \\(x=0\\). However, if the flip was tails, flip the coin again (an independent event from the first flip). If the second flip is heads, \\(x=1\\), if tails \\(x=2\\). These three outcomes occur with probability $1/2$, $1/4$, and $1/4$, respectively.\nWe can compute the entropy of $x$ as \\(H(p_0=1/2, p_1=1/4, p_2=1/4)\\). By the independence property, this relationship holds:\n\\[H\\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}\\right)=H\\left(\\frac{1}{2}, \\frac{1}{2}\\right) + \\frac{1}{2} H\\left(\\frac{1}{2}, \\frac{1}{2}\\right).\\]\nAs David MacKay explains, this is the general claim that\n\\[ H(\\mathbf{p})=H(p_1, 1-p_1)+(1-p_1)H\\left(\\frac{p_2}{1-p_1}, \\frac{p_3}{1-p_1}, \\ldots, \\frac{p_n}{1-p_1}\\right).\\]\nShannon showed that given these three assumptions, there is a unique form that \\(H\\) must take:\n$$ H\\propto -\\sum_{i=1}^n p_i \\log p_i=\\sum_{i=1}^n p_i \\log \\frac{1}{p_i}. $$\nHe named this measure of uncertainty entropy, because the form of \\(H\\) bears striking similarity to that of Gibbs Entropy in statistical thermodynamics.1\nShannon observes that \\(H\\) has many other interesting properties:\nEntropy \\(H\\) is 0 if and only if exactly one event has probability 1 and the rest have probability 0. (Uncertainty vanishes only when we are certain about the outcomes.) Entropy \\(H\\) is maximized when the \\(p_i\\) values are equal. The joint entropy of two events is less than or equal to the sum of the individual entropies. \\(H(x, y)=H(x)+H(y)\\) only when \\(x\\) and \\(y\\) are independent events. You can read more about this in Shannon\u0026rsquo;s seminal paper A Theory of Mathematical Communication.\nCaianiello and Aizerman say the name entropy is thanks to von Neumann who said\nYou should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, nobody knows what entropy really is, so in a debate you will always have the advantage.\nThey argue that the name \u0026ldquo;uncertainty\u0026rdquo; would have been much more helpful since \u0026ldquo;Shannon entropy is simply and avowedly the \u0026lsquo;measure of the uncertainty inherient in a pre-assigned probability scheme.\u0026rsquo;\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2016-09-05T16:05:00Z","image":"https://tdhopper.com/entropy.png","permalink":"https://tdhopper.com/blog/entropy-of-a-discrete-probability-distribution/","title":"Entropy of a Discrete Probability Distribution"},{"content":"As we saw in an earlier post, the entropy of a discrete probability distribution is defined to be\n$$H(p)=H(p_1,p_2,\\ldots,p_n)=-\\sum_{i}p_i \\log p_i.$$\nKullback and Leibler defined a similar measure now known as KL divergence. This measure quantifies how similar a probability distribution $p$ is to a candidate distribution $q$.\n$$D_{\\text{KL}}(p\\ | q)=\\sum_i p_i \\log \\frac{p_i}{q_i}.$$\n$D_\\text{KL}$ is non-negative and zero if and only if $ p_i = q_i $ for all $i$. However, it is important to note that it is not in general symmetric:\n$$ D_{\\text{KL}}(p| q) \\neq D_{\\text{KL}}(q| p).$$\nJonathon Shlens explains that KL Divergence can be interpreted as measuring the likelihood that samples represented by the empirical distribution $p$ were generated by a fixed distribution $q$. If $D_{\\text{KL}}(p| q)=0$, we can guarantee that $p$ is generated by $q$. As $D_{\\text{KL}}(p| q)\\rightarrow\\infty$, we can say that it is increasingly unlikely that $p$ was generated by $q$.\nAlgebraically, we can rewrite the definition as\n$$ \\begin{array}{rl} D_{\\text{KL}}(p| q) \u0026amp;=\\sum_i p_i \\log \\frac{p_i}{q_i} \\\\ \u0026amp;=\\sum_i \\left ( - p_i \\log q_i + p_i \\log p_i \\right)\\\\ \u0026amp;=- \\sum_i p_i \\log q_i + \\sum_i p_i \\log p_i \\\\ \u0026amp;=- \\sum_i p_i \\log q_i - \\sum_i p_i \\log \\frac{1}{p_i} \\\\ \u0026amp;=- \\sum_i p_i \\log q_i-H(p) \\\\ \u0026amp;=\\sum_i p_i \\log \\frac{1}{q_i}-H(p)\\\\ \\end{array} $$\nKL Divergence breaks down as something that looks similar to entropy (but combining $p$ and $q$) minus the entropy of $p$. This first term is often called cross entropy:\n$$H(p, q)=\\sum_i p_i \\log \\frac{1}{q_i}.$$\nWe could alternatively use this relationship to define cross entropy as:\n$$H(p, q)=H(p) + D_\\text{KL}(p| q).$$\nIntuitively, the cross entropy is the uncertainty implicit in $H(p)$ plus the likelihood that $p$ could have be generated by $q$. If we consider $p$ to be a fixed distribution, $H(p, q)$ and $D_\\text{KL}(p | q)$ differ by a constant factor for all $q$.\n","date":"2016-09-05T14:12:00Z","permalink":"https://tdhopper.com/blog/cross-entropy-and-kl-divergence/","title":"Cross Entropy and KL Divergence"},{"content":"A few weeks ago, I introduced my wife to backpacking in the beautiful Grayson Highlands State Park in southestern Virginia. Part of my reason for picking this location was to see the herd of wild ponies that life at 5000\u0026rsquo; on the grassy balds.\nI shared some of my best pictures from the trip on Flicker under a Creative Commons license (CC BY-NC-ND 2.0). On Saturday, I stumbled acrosss an article about the Grayson Highlands ponies on the Smithsonian Magazine website. I was pleasantly surprised to see they selected two of my images for the story! I\u0026rsquo;ve been spending more time lately exploring my longtime interest in wildlife photography, and I\u0026rsquo;m thrilled to see others sharing my work.\nYou can find more of my photography on photos.tdhopper.com or Instagram.\nPonies at Grayson Highlands State Park Ponies at Grayon Highlands State Park ","date":"2016-07-25T00:00:00Z","permalink":"https://tdhopper.com/blog/photos-featured-on-smithsonian-magazine/","title":"Photos Featured on Smithsonian Magazine"},{"content":"I\u0026rsquo;ve been using Bash\u0026rsquo;s find command a lot more regularly lately. I\u0026rsquo;ve always been scared off by its syntax, but it\u0026rsquo;s great once you\u0026rsquo;ve learned it.\nToday I learned how to filter the results by files changed in the last N minutes: the cmin flag:\n1 2 3 4 5 -cmin [-|+]n True if the difference between the time of last change of file status information and the time find was started, rounded up to the next full minute, is more than n (+n), less than n (-n), or exactly n minutes ago. For example:\n1 2 3 find . -cmin +5 # Files modified more than 5 minutes ago find . -cmin -5 # Files modified less than than 5 minutes ago find . -cmin 5 # Files modified exactly 5 minutes ago ","date":"2016-06-22T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/find-files-modified-in-last-n-minutes/","title":"Find Files Modified in last N Minutes"},{"content":"I routinely run Impala queries on a remote machine and want the results to save to a CSV file on that machine.\nI recently realized that I should setup an Impala config file on that machine to configure the connection to the Impala cluster and the output file format.\nSo I created a text file at ~/.impalarc with the following settings:\n1 2 3 4 5 6 7 8 [impala] impalad=CLUSTER-ADDRESS:PORT output_delimiter=, verbose=true ignore_query_failure=false default_db=prd write_delimited=true print_header=true Now when I run a query with impala-shell, I don\u0026rsquo;t have to specify the address of the cluster, or the various flags required to get a CSV with a header.\nOther impala-shell config options are specified here\n","date":"2016-06-15T00:00:00Z","permalink":"https://tdhopper.com/blog/configuring-impala-query-results-with-impalarc/","title":"Configuring Impala Query Results with impalarc"},{"content":"I routinely SSH into a company machine on Openstack to do work. Until yesterday, I\u0026rsquo;d use my Bash history to find the SSH command I needed to access it. I was reading SSH Mastery in a plane yesterday and realized I\u0026rsquo;ve been foolish to neglect the power of ~/ssh/config.\nI added this to my SSH config file:\n1 2 3 4 5 6 7 Host lab HostName MACHINE.IP.ADDRESS User tdhopper Port 22 IdentityFile ~/.ssh/id_rsa ForwardAgent yes RemoteForward 52698 localhost:52698 Now I can connect to the machine by typing $ ssh lab. That\u0026rsquo;s it. I don\u0026rsquo;t have to provide the SSH key, username, or machine IP.\nThe RemoteForward bit also forwards a port to enable me to open files on the remote machine in my local SublimeText editor using rsub. This is great.\n","date":"2016-06-15T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/faster-ssh-access-to-remote-computers/","title":"Faster SSH Access to Remote Computers"},{"content":"I\u0026rsquo;m trying out the beautiful Fish shell. I\u0026rsquo;ve been a Bash user up until now.\nI was bummed to see that source activate ENV (for activating Conda environments didn\u0026rsquo;t work natively in Fish.\nApparently the Conda team is adding native support for Fish, but it\u0026rsquo;s not available as of 2016-05-04.\nThere are several projects developed to help in the meantime:\nconda-workon fish-conda-virtualenv condactivate I\u0026rsquo;m going to wait for the Conda update to be released.\n","date":"2016-05-04T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/using-conda-with-the-fish-shell/","title":"Using Conda with the Fish Shell"},{"content":"I needed to concatenate a bunch of CSV files while skipping the header row. There was a nice solution on Stack Overflow:\n1 find . -name \u0026#34;*.csv\u0026#34; | xargs -n 1 tail -n +2 With the GNU version of tail (sadly not the one installed on OS X by default), you can just use\n1 tail -q -n +2 *.csv or\n1 awk \u0026#39;FNR != 1\u0026#39; *.csv ","date":"2016-05-02T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/concatenate-files-with-header-row/","title":"Concatenate files with header row"},{"content":"This is more of a Today I Taught.\nTurns out, it is actually possible to search my timeline. Thanks @tdhopper for the tip. pic.twitter.com/7IsD76qszD\n\u0026mdash; Ole Zorn (@olemoritz) May 2, 2016 I have a quick search in Alfred to search the tweets of people I follow. I launch Alfred with ‚åò-Space and type following {query}. Restricting a Twitter search to people you follow only requires adding the s=follows query string to a search url:\n1 https://twitter.com/search?s=follows\u0026amp;q={query} ","date":"2016-05-02T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/searching-the-tweets-of-people-you-follow/","title":"Searching the tweets of people you follow"},{"content":"If you want to check whether a Python string is an integer, you can try casting to an int with int() and catching the ValueError if it\u0026rsquo;s not an integer:\n1 2 3 4 5 6 def is_integer(value: str, *, base: int=10) -\u0026gt; bool: try: int(value, base=base) return True except ValueError: return False To check for nonnegative integers, you can use the str.is_digit() method. It will \u0026ldquo;return true if all characters in the string are digits and there is at least one character, false otherwise:\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; \u0026#34;123\u0026#34;.isdigit() True \u0026gt;\u0026gt;\u0026gt; \u0026#34;-123\u0026#34;.isdigit() False Thanks to Jeremy Kahn for reminding me that isdigit only detects positive integers.\n","date":"2016-04-29T15:13:00Z","permalink":"https://tdhopper.com/blog/testing-whether-a-python-string-contains-an-integer/","title":"Testing whether a Python string contains an integer"},{"content":"I created a single page website to collect notes on one of my other hobbies: ultralight backpacking. In particular, notes on ultralight gear for the very tall.\n","date":"2016-04-29T00:00:00Z","permalink":"https://tdhopper.com/blog/backpacking-for-the-very-tall/","title":"Ultralight Backpacking for the Ultratall"},{"content":"I was using [tee](http://man7.org/linux/man-pages/man1/tee.1.html) with a long running Python process, but I wasn\u0026rsquo;t seeing any output. This is a result of Python buffering the stdout stream. You can run force Python to run in unbuffered mode using the -u flag at the command line.\nForce stdin, stdout and stderr to be totally unbuffered. On systems where it matters, also put stdin, stdout and stderr in binary mode.\n","date":"2016-04-26T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/dont-buffer-pythons-stdout/","title":"Don't Buffer Python's stdout"},{"content":"Listening to Russ Roberts\u0026rsquo; Econtalk podcast for the last 5 years has given me a whole new perspective on the world. Roberts has exposed me to a whole new way of economic thinking, refined my scientific skepticism, and introduced me to copious topics and scholars.\nHere are episodes from over the years that I\u0026rsquo;ve particularly enjoyed.\nThe Economics of Organ Donations Making Schools Better: A Conversation with Rick Hanushek Boudreaux on the Economics of \u0026ldquo;Buy Local\u0026rdquo; Deer on Autism, Vaccination, and Scientific Fraud O\u0026rsquo;Donohoe on Potato Chips and Salty Snacks David Owen on the Environment, Unintended Consequences, and The Conundrum Cowen on Food Lisa Turner on Organic Farming Narlikar on Fair Trade and Free Trade Velasquez-Manoff on Autoimmune Disease, Parasites, and Complexity David Skarbek on Prison Gangs and the Social Order of the Underworld Jonah Lehrer on Creativity and Imagine Scott Atlas on American Health Care Terry Anderson on the Environment and Property Rights Leonard Wong on Honesty and Ethics in the Military Eric Topol on the Power of Patients in a Digital World Adam Davidson on Hollywood and the Future of Work Matt Ridley on Climate Change Lee Ohanian, Arnold Kling, and John Cochrane on the Future of Freedom, Democracy, and Prosperity Rachel Laudan on the History of Food and Cuisine William MacAskill on Effective Altruism and Doing Good Better Jayson Lusk on Food, Technology, and Unnaturally Delicious One of my favorite guests is Duke economist Mike Munger. Here are some great interviews with him:\nPrice Gouging On Milk Division of Labor Middlemen Incidentally, Priceonomics recently published a great article on Roberts and Econtalk.\n","date":"2016-04-13T01:42:00Z","permalink":"https://tdhopper.com/blog/econtalk/","title":"Econtalk"},{"content":"Joining two Pandas DataFrames with an equal number of rows is slightly harder than it appears. In R, you just use the cbind function.\nAs this StackOverflow question shows, in Pandas it\u0026rsquo;s easy to end up with something like this:\n1 2 3 4 5 6 7 unique_id lacet_number latitude longitude 0 NaN NaN -93.193560 31.217029 1 NaN NaN -93.948082 35.360874 2 NaN NaN -103.131508 37.787609 15 5570613 TLA-0138365 NaN NaN 24 5025490 EMP-0138757 NaN NaN 36 4354431 DXN-0025343 NaN NaN This results from the indices not being identical. Frustratingly (to me) the ignore_index argument doesn\u0026rsquo;t give the 3-rowed DataFrame I\u0026rsquo;d hope it gives.\nAs the accepted answer on that question shows, the thing to do is reset the indices on the DataFrames before concatenating:\n1 pd.concat([df_a.reset_index(drop=True), df_b.reset_index(drop=True)], axis=1) ","date":"2016-04-11T20:56:00Z","permalink":"https://tdhopper.com/blog/column-binding-two-pandas-dataframes/","title":"Column binding two Panda's Dataframes"},{"content":"Jupyter notebooks nicely render Pandas data frames if they\u0026rsquo;re the last line in a cell. It renders the HTML version of the data frame returned by pandas.DataFrame.to_html(). However, if you call print(df) in a cell, the data frame is rendered in less readable text-based output.\nDespite using Notebooks regularly for years, I\u0026rsquo;d never bothered to figure out a way around this. However, the solution is easy.\nInstead of print(df) you use\n1 2 3 from IPython.display import display display(df) ","date":"2016-03-23T14:18:00Z","image":"https://tdhopper.com/panda.png","permalink":"https://tdhopper.com/blog/printing-pandas-data-frames-as-html-in-jupyter-notebooks/","title":"Printing Pandas Data Frames as HTML in Jupyter Notebooks"},{"content":"Impala Doesn\u0026rsquo;t Have a histogram() function.\nIbis\u0026rsquo;s histogram works by normalizing a column and rounding to integers:\n1 2 3 4 5 6 7 8 9 WITH t0 AS ( SELECT * FROM db.`table_name`) SELECT floor((t0.`column_name` - (t1.`min_1fe5be` - 1e-13)) / ((t1.`max_1fe5be` - (t1.`min_1fe5be` - 1e-13)) / (NUM_BINS - 1))) AS `tmp` FROM t0 CROSS JOIN ( SELECT min(`column_name`) AS `min_1fe5be`, max(`column_name`) AS `max_1fe5be` FROM t0 ) t1 ","date":"2016-03-23T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/impala-doesnt-have-a-histogram-function/","title":"Impala Doesn't Have a histogram() function"},{"content":"Wes Mckinney\u0026rsquo;s Ibis, a Pythonic interface to Impala, has functionality for creating Impala tables from Python Pandas dataframes.\n1 2 3 4 5 6 7 8 9 import pandas as pd import ibis hdfs = ibis.hdfs_connect(host=webhdfs_host, port=webhdfs_port) client = ibis.impala.connect(host=impala_host, port=impala_port, hdfs_client=hdfs) db = c.database(\u0026#39;ibis_testing\u0026#39;) data = pd.DataFrame({\u0026#39;foo\u0026#39;: [1, 2, 3, 4], \u0026#39;bar\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]}) db.create_table(\u0026#39;pandas_table\u0026#39;, data) This functionality, added in Ibis 0.6.0, is much easier that manually move data to HDFS and loading into Impala.\n","date":"2016-03-15T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/creating-impala-tables-from-pandas-dataframes/","title":"Creating Impala Tables from Pandas Dataframes"},{"content":"Galileo\u0026rsquo;s conflict with the Church was not as it is often portrayed:\nIn January of 1616, the month before before the Roman Inquisition would infamously condemn the Copernican theory as being \u0026ldquo;foolish and absurd in philosophy\u0026rdquo;, Monsignor Francesco Ingoli addressed Galileo Galilei with an essay entitled \u0026ldquo;Disputation concerning the location and rest of Earth against the system of Copernicus\u0026rdquo;. \u0026hellip; The essay, upon which the Inquisition condemnation was likely based, lists mathematical, physical, and theological arguments against the Copernican theory. Ingoli asks Galileo to respond to those mathematical and physical arguments that are \u0026ldquo;more weighty\u0026rdquo;, and does not ask him to respond to the theological arguments at all. \u0026hellip; Ingoli\u0026rsquo;s emphasis on the scientific arguments of Brahe, and his lack of emphasis on theological arguments, raises the question of whether the condemnation of the Copernican theory was, in contrast to how it is usually viewed, essentially scientific in nature, following the ideas of Brahe.\nFrom Francesco Ingoli\u0026rsquo;s essay to Galileo: Tycho Brahe and science in the Inquisition\u0026rsquo;s condemnation of the Copernican theory.\n","date":"2016-03-11T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/galileos-conflict-was-science-vs.-science/","title":"Galileo's Conflict was Science vs. Science"},{"content":"I don\u0026rsquo;t use Ruby much, but I wanted to use Jekyll for this blog. I kept getting this error when running gem install jekyll:\n1 2 3 Fetching: colorator-0.1.gem (100%) ERROR: While executing gem ... (Gem::FilePermissionError) You don\u0026#39;t have write permissions for the /Library/Ruby/Gems/2.0.0 directory. I installed rbenv with brew install rbenv and added\n1 [[ -s $HOME/.rbenv/bin ]] \u0026amp;\u0026amp; export PATH=\u0026#34;$HOME/.rbenv/bin:$PATH\u0026#34; \u0026amp;\u0026amp; eval \u0026#34;$(rbenv init -)\u0026#34; to my .bashrc file. Then I changed the default ruby interpreter from the system interpreter to one managed by rbenv with rbenv install 2.2.3 \u0026amp;\u0026amp; rbenv global 2.2.3.\n","date":"2016-03-11T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/using-rbenv-for-ruby-versions/","title":"Using rbenv for Ruby Versions"},{"content":"I enjoy the Stanford Encyclopedia of Philosophy, so I created a Twitter account that tweets links to random articles from it.\n","date":"2016-02-28T00:00:00Z","permalink":"https://tdhopper.com/blog/tweet-your-moon/","title":"Stanford Encyclopedia of Philosophy Bot"},{"content":"People mention John Cook\u0026rsquo;s blog a lot in Github repos.\nI scraped the Github search pages to try to figure out which of his pages are most mentioned. His post Accurately computing running variance gets many more mentions than any other post. It provides C++ code for Knuth\u0026rsquo;s algorithm for computing the mean, sample variance, and standard deviation for a stream of data.\nHere are top 12 pages from his site most linked in Github:\nAccurately computing running variance (377 mentions) Stand-alone code for numerical computing (58 mentions) A Bayesian view of Amazon Resellers (52 mentions) johndcook.com/blog (47 mentions) Computing the distance between two locations on Earth from coordinates (44 mentions) Math.h in POSIX, ISO, and Visual Studio (38 mentions) Three algorithms for converting color to grayscale (26 mentions) johndcook.com (21 mentions) Computing skewness and kurtosis in one pass (20 mentions) What‚Äôs so hard about finding a hypotenuse? (19 mentions) Random number generation in C++ (19 mentions) R language for programmers (19 mentions) ","date":"2016-01-14T02:59:00Z","permalink":"https://tdhopper.com/blog/mentions-of-john-cook-on-github/","title":"Mentions of John Cook on Github"},{"content":"I\u0026#39;ll never forget the great words of one of my best math professors:\u0026#10;\u0026#10;\u0026quot;If all I wanted was the answer, I sure as hell wouldn\u0026#39;t ask you.\u0026quot;\n\u0026mdash; Tim Hopper (@tdhopper) December 9, 2013 Another great former professor quote:\u0026#10;\u0026#10;\u0026quot;I\u0026#39;m not smarter than you. I can just recover from mistakes faster than you.\u0026quot;\n\u0026mdash; Tim Hopper (@tdhopper) December 9, 2013 ","date":"2016-01-12T00:00:00Z","permalink":"https://tdhopper.com/blog/quotes-from-former-professors/","title":"Quotes from Former Professors"},{"content":"@tdhopper my father wrote his PhD on this question, and I believe his answer was \u0026quot;depends\u0026quot;\n\u0026mdash; Fred Benenson (@fredbenenson) December 7, 2015 ","date":"2015-12-07T00:00:00Z","permalink":"https://tdhopper.com/blog/i-love-twitter/","title":"I Love Twitter"},{"content":"Many new Python programmers rely on their system install of Python to run their scripts. There are several good reasons to stop using the system Python. First, it\u0026rsquo;s probably an old version of Python. Secondly, if you install 3rd party packages with pip, every package is installed into the same globally accessible directory. While this may sound convenient, it causes problems if you (1) install different packages with the same name (2) need to use different versions of the same package (3) upgrade your operating system (OS X will delete all the packages you have installed).\nFor many years, best practice for Python developers was to use virtualenv to create a sandbox-ed environment for each project. If you use virtualenv, each project you work on can have its own version of Python with its own 3rd party packages (hopefully specified in an requirements.txt file). In my experience, getting started with virtualenv is cumbersome and confusing; to this day, I have to look up the command to create a Python 3 virtualenv. virtualenv also provides no helping in actually managing Python versions. You have to install each version yourself and then tell virtualenv to use it.\nIn 2015, I have almost exclusively used Python installations provided through Continuum Analytics\u0026rsquo;s Conda/Anaconda platform. I have also switched from using virtualenvs to using conda environments, and I am loving it.\nBefore explaining my workflow, here\u0026rsquo;s a quick glossary of the similarly-named products that Continuum offers.\nconda: \u0026ldquo;Conda is an open source package management system and environment management system for installing multiple versions of software packages and their dependencies and switching easily between them. It works on Linux, OS X and Windows, and was created for Python programs but can package and distribute any software. \u0026quot; A conda install provides a whole suite of command line tools for installing and managing packages and environments. Because conda works for any software, it can even install different versions of Python (unlike pip). Anaconda: \u0026ldquo;Anaconda is a completely free Python distribution (including for commercial use and redistribution). It includes more than 300 of the most popular Python packages for science, math, engineering, and data analysis.\u0026rdquo; It is available across platforms and installable through a binary. Anaconda Cloud: Also known as Anaconda.org and formerly known as Binstar, \u0026ldquo;Anaconda Cloud is a package management service where you can host software packages of all kinds.\u0026rdquo; Anaconda Cloud is a package repository analogous to PyPI. Packages are installed via the conda command line tool instead of Pip. By default, the conda install command installs packages from a curated collection of packages (a superset of those in Anaconda). Continuum allows users to host their own packages on Anaconda Cloud; these packages can also be installed through conda install using the -n flag with the username. Conda, Anaconda, and Anaconda cloud are distinct but interrelated tools; keeping them straight can be hard, but is helpful.\nConda (the package manager) can be installed in two ways. Through the Miniconda installer or the Anaconda installer. Both install the package manager, but the latter also installs the 300+ packages for scientific Python. (Installing Anaconda is equivalent to installing Miniconda and then running conda install anaconda.)\nConda Environment Files It has become standard for pip users to create a requirements.txt file for specifying dependencies for a particular project. Often, a developer working a project will (1) create and activate a virtual environment (2) run pip install -r requirements.txt to build an isolated development environment with the needed packages.\nConda provides an analogous (but more powerful) file: environment.yml.\nA simple environment.yml file might look like this:\nname: numpy-env dependencies: - python=3 - numpy If you are in a directory containing this file, you can run $ conda env create to create a Conda environment named numpy-env that runs Python 3 and has numpy installed[^numpy]. Run $ source activate numpy-env to activate this environment. Once activated, running $ python will run Python 3 from your environment instead of the globally installed Python for your system. Moreover, you will be able to import numpy but not any of the 3rd party packages installed globally.\nenvironment.yml can also install packages via pip with this syntax:\nname: pip-env dependencies: - python - pip - pip: - pypi-package-name I see environment.yml files as a positive development from requirements.txt files for several reasons. Foremost, they allow you to specify the version of Python you want to use. At Pydata NYC 2015, many presenters provided their code in Github repositories without specifying anywhere whether they were using Python 2 or 3. Because I included a YAML file, attendees could see exactly what version I was using and quickly install it with conda env create. I also like being able to specify the name of the environment in the file; this is particularly helpful when working with others. Finally, because conda can install from PyPI via pip, environment.yml files provide no less functionality than a requirements.txt file provides.\nMy Python Environment Workflow Lately, whenever I am working on a new project (however big or small), I follow the following steps:\nCreate a project folder in the ~/repos/ directory on my computer. Create an environment.yml file in the directory. Typically the environment name will be the same as the folder name. At minimum, it will specify the version of Python I want to use; it will often include anaconda as a dependency. Create the conda environment with $ conda env create. Activate the conda environment with $ source activate ENV_NAME. Create a .env file containing the line source activate ENV_NAME. Because I have autoenv installed, this file will be run every time I navigate to the project folder in the Terminal. Therefore, my conda environment will be activated as soon as I navigate to the folder. Run $ git init to make the folder a Git repository. I then run $ git add environment.yml \u0026amp;\u0026amp; git commit -m 'initial commit' to add the YAML file to the repository. If I want to push the repository to Github, I use $ git create using Github\u0026rsquo;s hub commands. I then push the master branch with $ git push -u origin master. As I add dependencies to my project, I try to be sure I add them to my environment.yml file.\nA major benefit of all this is how easily reproducible a development environment becomes. If a colleague or conference attendee wants to run my code, they can setup the dependencies ( including Python version) by (1) cloning the repository, (2) running $ conda env create, (3) running $ source activate ENV_NAME. It\u0026rsquo;s easy enough for me to drop those instructions and further instructions for running the code in a README file. If I\u0026rsquo;m feeling especially helpful, I\u0026rsquo;ll create a Makefile or Fabfile to encapsulate commands for core functionality of the code.\nAn even larger benefit is that I can return to a project after, days, months, or years and quickly start developing without first having to hunt for print statements to figure out whether I was using Python 2 or 3.\nI\u0026rsquo;ve come to love environment.yml files, and I think you might too.\n","date":"2015-11-24T14:41:00Z","image":"https://tdhopper.com/conda.png","permalink":"https://tdhopper.com/blog/my-python-environment-workflow-with-conda/","title":"My Python Environment Workflow with Conda"},{"content":"In my nonlinear optimization class in grad school at North Carolina State University, I wrote a paper on the famed SMO algorithm for support vector machines. In particular, I derive the Lagrangian dual of the classic formulation of the SVM optimization model and show how it can be solved using the stochastic gradient descent algorithm.\nYou can find the paper here.\n","date":"2015-11-21T00:00:00Z","permalink":"https://tdhopper.com/blog/sequential-minimal-optimization-algorithm-for-support-vector-machines/","title":"Sequential Minimal Optimization Algorithm for Support Vector Machines"},{"content":"True story: I\u0026rsquo;m a closet knitter. I don\u0026rsquo;t have much time for it these days, but it helped keep me sane in grad school. Here are some things I\u0026rsquo;ve made over the years.\n","date":"2015-11-20T00:00:00Z","permalink":"https://tdhopper.com/blog/knitting/","title":"Knitting"},{"content":"When I worked at RTI International, I worked on an exploratory analysis of Twitter discussion of electronic cigarettes. A paper on our work was just published in the Journal of Internet Medical Research: Using Twitter Data to Gain Insights into E-cigarette Marketing and Locations of Use: An Infoveillance Study.1\nMarketing and use of electronic cigarettes (e-cigarettes) and other electronic nicotine delivery devices have increased exponentially in recent years fueled, in part, by marketing and word-of-mouth communications via social media platforms, such as Twitter. \u0026hellip; We identified approximately 1.7 million tweets about e-cigarettes between 2008 and 2013, with the majority of these tweets being advertising (93.43%, 1,559,508/1,669,123). Tweets about e-cigarettes increased more than tenfold between 2009 and 2010, suggesting a rapid increase in the popularity of e-cigarettes and marketing efforts. The Twitter handles tweeting most frequently about e-cigarettes were a mixture of e-cigarette brands, affiliate marketers, and resellers of e-cigarette products. Of the 471 e-cigarette tweets mentioning a specific place, most mentioned e-cigarette use in class (39.1%, 184/471) followed by home/room/bed (12.5%, 59/471), school (12.1%, 57/471), in public (8.7%, 41/471), the bathroom (5.7%, 27/471), and at work (4.5%, 21/471).\nI have no idea what \u0026ldquo;Infoveillance\u0026rdquo; means.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2015-11-06T00:00:00Z","image":"https://tdhopper.com/images/ecig.png","permalink":"https://tdhopper.com/blog/my-first-publication/","title":"Using Twitter Data to Gain Insights into E-cigarette Marketing and Locations of Use"},{"content":"Today is my last day at Qadium. Next week, I am joining the data science team at Distil Networks.\nI\u0026rsquo;ve been privileged to work with Eric Jonas on the data microscopes project for the past 8 months. In particular, I contributed the implementation of Nonparametric Latent Dirichlet Allocation.\nI published a collection of notes on nonparametric Bayesian methods and Latent Dirichlet Allocation at dp.tdhopper.com. I hope this will be useful to other students and researchers of these methods.\n","date":"2015-10-16T00:00:00Z","permalink":"https://tdhopper.com/blog/wrapping-up-on-nonparametric-bayes/","title":"Nonparametric Latent Dirichlet Allocation"},{"content":"I have published some notes on the Dirichlet distribute, Dirichlet processes, Gibbs sampling for mixture models and nonparametric mixture models, and the Gibbs sampler for nonparametric Latent Dirichlet Allocation.\nThis is related to my work on a Python implementation of Hierarchical Dirichlet Process Latent Dirichlet Allocation.\n","date":"2015-10-16T00:00:00Z","permalink":"https://tdhopper.com/blog/wrapping-up-on-nonparametric-bayes/","title":"Notes on Dirichlet Processes"},{"content":"On MCMC:\nAny sufficiently advanced MCMC is indistinguishable from magic. \u0026#10;\u0026#10;Very slow magic.\n\u0026mdash; Tim Hopper (@tdhopper) October 13, 2015 On nonparametric Bayesian methods:\nNonparametric methods are an advanced technique for replacing intuitive parameters with unintuitive hyperparameters \u0026amp; way more computation.\n\u0026mdash; Tim Hopper (@tdhopper) October 1, 2015 On code:\nThe cost of a line of code is larger than the time it takes to write it.\n\u0026mdash; Tim Hopper (@tdhopper) October 14, 2015 On key pressers:\n\u0026quot;We call people like these two young programmers \u0026#39;key pressers.\u0026#39;\u0026quot;\u0026#10;\u0026#10;(from SEAL Team Six) pic.twitter.com/VFxZbafLlG\n\u0026mdash; Tim Hopper (@tdhopper) October 13, 2015 ","date":"2015-10-16T00:00:00Z","permalink":"https://tdhopper.com/blog/tweets-im-proud-of-2/","title":"Tweets I'm Proud Of (2)"},{"content":"Even though I\u0026#39;ve been coding for 12 years, I\u0026#39;d still just consider myself a grammer.\n\u0026mdash; Tim Hopper (@tdhopper) November 10, 2014 ","date":"2015-09-24T00:00:00Z","permalink":"https://tdhopper.com/blog/a-joke/","title":"A Joke"},{"content":"I\u0026rsquo;m excited to see tools developed for the web being applied to offline domains like agriculture and health. I posed a question on Twitter yesterday:\nWho is hiring at the intersection data science and agriculture?\n\u0026mdash; Tim Hopper (@tdhopper) September 16, 2015 I got a number of replies. Companies in this space (not all hiring for DS) include:\nThe Climate Corporation: Known for their high profile acquisition by Monsanto, Climate Corp \u0026ldquo;combines hyper-local weather monitoring, agronomic modeling, and high-resolution weather simulations\u0026rdquo; \u0026ldquo;that help farmers improve profitability by making better informed operating and financing decisions\u0026rdquo;. They are based in San Fransisco. FarmLogs: A YCombinator-backed startup in Ann Arbor, MI, FarmLogs is combining satellite maps, soil surveys, GPS data, and more to \u0026ldquo;expose critical operational data insights\u0026rdquo; to row crop farmers. Dairymaster: In Ireland, Darymaster is hiring data scientists for their business of dairy equipment manufacturing. aWhere: aWhere \u0026ldquo;delivers agricultural intelligence into the hands of farmers, commercial growers, and policymakers everywhere\u0026rdquo; by collecting detailed weather data from around the world. They are based in Denver. pulsepod: This small startup is building hardware to help farmers \u0026ldquo;know when to plant, fertilize, irrigate, and harvest to achieve quality and yield goals using data from your own field\u0026rdquo;. They are based in Princeton, NJ. Granular: \u0026ldquo;Granular, a new kind of farm management software and analytics platform, helps you improve efficiency, profit and yield so you are ready to farm more acres.\u0026rdquo; They are based in San Fransisco. PrecisionHawk: Based in my home of Raleigh, NC, PrecisionHawk is \u0026ldquo;an information delivery company that combines unmanned aerial systems, cutting-edge artificial intelligence and remote sensing technologies to improve business operations and day-to-day decision making.\u0026rdquo; mavrx: \u0026ldquo;We use aerial imagery to provide actionable insights to the global agriculture industry.\u0026rdquo; They are based in San Fransisco. AgSmarts: Memphis-based Agsmarts \u0026ldquo;is a Precision Agriculture company that automates existing agricultural irrigation systems with our universal retrofit solution to optimize crop yield, save water and AgSmarts minimize input costs via mesh networks of IP-enabled controllers \u0026amp; environmental sensors.\u0026rdquo; ","date":"2015-09-17T00:00:00Z","permalink":"https://tdhopper.com/blog/data-science-and-agriculture/","title":"Data Science and Agriculture"},{"content":"I appreciated James Hague\u0026rsquo;s post on Computer Science Courses that Don\u0026rsquo;t Exist, But Should.\nI really like Dave Thomas\u0026rsquo;s idea of a Software Archeology class. I have spent a huge amount of time as a developer reading (vs. writing) code. I wish I\u0026rsquo;d been taught how to read code effectively.\nSimilarly, I have thought there should be a class (or series of classes) in \u0026ldquo;interacting with others\u0026rsquo; code\u0026rdquo;. Topics could include inheriting a software project, handing off a software project, writing code using 3rd party libraries, using package repositories, and understanding software licenses. These are such important skills in real world software development, but they seem to be rarely taught in the classroom. Perhaps a follow-up class could be \u0026ldquo;contributing to open source\u0026rdquo;.\n","date":"2015-09-12T01:55:00Z","permalink":"https://tdhopper.com/blog/classes-future-programmers-should-take/","title":"Classes Future Programmers Should Take"},{"content":"From Testicular volume is inversely correlated with nurturing-related brain activity in human fathers in PNAS:\nOne participant\u0026rsquo;s testes volume measurement was excluded because his value was 2.8 SDs above the mean (mean = 38,064; SD = 11,183) and was more than 13,000 mm^3 larger than any recorded value found in the literature. Of the more than 1,500 healthy, age-matched men in these studies, the largest reported value was 56,000 mm^3, and this participant‚Äôs measurement was 69,736 mm^3.\n","date":"2015-09-08T00:00:00Z","permalink":"https://tdhopper.com/blog/dealing-with-outliers/","title":"Dealing with Outliers"},{"content":"I recently had the honor of being interviewed by Michael Swenson for his interview series called \u0026ldquo;Profiles in Computational Imagination\u0026rdquo;. I talked a bit about my current work, my wandering road to data science, and my love for remote work. You can read it here.\n","date":"2015-09-01T00:00:00Z","permalink":"https://tdhopper.com/blog/profile-in-computational-imagination/","title":"Profile in Computational Imagination"},{"content":"I\u0026#39;ll never forget the great words of one of my best math professors:\u0026#10;\u0026#10;\u0026quot;If all I wanted was the answer, I sure as hell wouldn\u0026#39;t ask you.\u0026quot;\n\u0026mdash; Tim Hopper (@tdhopper) December 9, 2013 ","date":"2015-08-12T00:00:00Z","permalink":"https://tdhopper.com/blog/on-showing-your-work/","title":"On Showing Your Work"},{"content":"I am convinced that a programming student hoping to get a job in that field should be actively building a portfolio online. Turn those class projects, presentations, and reports into Github repositories or blog posts! I felt vindicated as I read this anecdote in Peopleware:\nIn the spring of 1979, while teaching together in western Canada,we got a call from a computer science professor at the local technical college. He proposed to stop by our hotel after class one evening and buy us beers in exchange for ideas. That\u0026rsquo;s the kind of offer we seldom turn down. What we learned from him that evening was almost certainly worth more than whatever he learned from us.\nThe teacher was candid about what he needed to be judged a success in his work: He needed his students to get good job offers and lots of them. \u0026ldquo;A Harvard diploma is worth something in and of itself, but our diploma isn\u0026rsquo;t worth squat. If this year\u0026rsquo;s graduates don\u0026rsquo;t get hired fast, there are no students next year and I\u0026rsquo;m out of a job.\u0026rdquo; So he had developed a formula to make his graduates optimally attractive to the job market. Of course he taught them modern techniques for system construction, including structured analysis and design, data-driven design, information hiding, structured coding, walk throughs, and metrics. He also had them work on real applications for nearby companies and agencies. But the center piece of his formula was the portfolio that all students put together to show samples of their work.\nHe described how his students had been coached to show off their portfolios as part of each interview:\n\u0026ldquo;I\u0026rsquo;ve brought along some samples of the kind of work I do. Here, for instance, is a subroutine in Pascal from one project and a set of COBOL paragraphs from another. As you can see in this portion, we use the loop-with-exit extension advocated by Knuth, but aside from that, it\u0026rsquo;s pure structured code, pretty much the sort of thing that your company standard calls for. And here is the design that this code was written from. The hierarchies and coupling analysis use Myers\u0026rsquo; notation. I designed all of this particular subsystem, and this one little section where we used some Orr methods because the data structure really imposed itself on the process structure. And these are the leveled data flow diagrams that makeup the guts of our specification, and the associated data dictionary. \u0026hellip;\u0026rdquo;\nIn the years since, we\u0026rsquo;ve often heard more about that obscure technical college and those portfolios. We\u0026rsquo;ve met recruiters from as far away as Triangle Park, North Carolina, and Tampa, Florida,who regularly converge upon that distant Canadian campus for a shot at its graduates.\nOf course, this was a clever scheme of the professor\u0026rsquo;s to give added allure to his graduates, but what struck us most that evening was the report that interviewers were always surprised by the portfolios. That meant they weren\u0026rsquo;t regularly requiring all candidates to arrive with portfolios. Yet why not? What could be more sensible than asking each candidate to bring along some samples of work to the interview?\n","date":"2015-08-05T00:00:00Z","permalink":"https://tdhopper.com/blog/a-programmers-portfolio/","title":"A Programmer's Portfolio"},{"content":"I love this new post on Quora\u0026rsquo;s engineering blog. The post states \u0026ldquo;high code quality is the long-term boost to development speed\u0026rdquo; and goes on to explain how they go about accomplishing this.\nI\u0026rsquo;ve inherited large code bases at each of my jobs out of grad school, and I\u0026rsquo;ve spent a lot of thinking about this question. At least on the surface, I love the solutions Quora has in place for ensuring quality code: thoughtful code review, careful testing, style guidelines, static checking, and intentional code cleanup.\n","date":"2015-07-30T00:00:00Z","permalink":"https://tdhopper.com/blog/high-quality-code-at-quora/","title":"High Quality Code at Quora"},{"content":"How to be a 10x engineer: Incur technical debt fast enough to appear 10x as productive as the ten engineers tasked with cleaning it up.\n\u0026mdash; Brian Degenhardt (@bmdhacks) January 29, 2015 ","date":"2015-07-28T00:00:00Z","permalink":"https://tdhopper.com/blog/10x-engineering/","title":"10x Engineering"},{"content":"I\u0026rsquo;m starting to love single-page informational websites. For example:\nKeep a CHANGELOG: Olivier Lacan\u0026rsquo;s guide to writing a CHANGELOG.md for open source projects. strftime: Will McCutchen\u0026rsquo;s little page of the easily forgettable date formatting codes for Python programmers. PyFormat: Ulrich Petri and Horst Gutmann\u0026rsquo;s guide to new style (.format()) string formatting in Python. Two Factor Auth: \u0026ldquo;List of websites and whether or not they support 2FA.\u0026rdquo; My website Should I Get a Phd? is in this same vein.\nPublishing a site like this is very cheap with static hosting on AWS. I would love to see more of them created!\n","date":"2015-07-27T00:00:00Z","permalink":"https://tdhopper.com/blog/handy-one-off-webpages/","title":"Handy One-off Webpages"},{"content":"Having worked from home for the last few years, I have a hard time understanding how people get anything done in open-floor plan offices. I would be overwhelmed and frustrated by the noise and commotion.\nI assumed open-floor plans for software shops were a relatively new invention. However, I just started reading Peopleware: Productive Projects and Teams, first published in 1987, and discovered that the first third of the book rails against open floor plan offices. I particularly enjoyed this quote:\nIn my years at Bell Labs, we worked in two-person offices. They were spacious, quiet, and the phones could be diverted. I shared my office with Wendl Thomis, who went on to build a small empire as an electric toy maker. In those days, he was working on the Electronic Switching System fault dictionary. The dictionary scheme relied upon the notion of n-space proximity, a concept that was hairy enough to challenge even Wendel\u0026rsquo;s powers of concentration. One afternoon, I was bent over a program listing while Wendl was staring into space, his feet propped up on his desk. Our boss came in and asked, \u0026ldquo;Wendl! What are you doing?\u0026rdquo; Wendl said, \u0026ldquo;I\u0026rsquo;m thinking.\u0026rdquo; And the boss said, \u0026ldquo;Can\u0026rsquo;t you do that at home?\u0026rdquo;\nThe difference between that Bell Labs environment and a typical modern-day office plan is that in those quiet offices, one at least had the option of thinking on the job. In most of the office space we encounter today, there is enough noise and interruption to make any serious thinking virtually impossible. More is the shame: Your people bring their brains with them every morning. They could put them to work for you at no additional cost if only there were a small measure of peace and quiet in the workplace.\n","date":"2015-07-22T00:00:00Z","permalink":"https://tdhopper.com/blog/thinking-at-work/","title":"Thinking at Work"},{"content":"Caution: the following post is laden with qualitative extrapolation of anecdotes and impressions. Perhaps ironically (though perhaps not), it is not a data driven approach to measuring the efficacy of math majors as data scientists. If you have a differing opinion, I would greatly appreciate you to carefully articulate it and share it with the world.\nI recently started my third \u0026ldquo;real\u0026rdquo; job since finishing school; at my first and third jobs I have been a \u0026ldquo;data scientist\u0026rdquo;. I was a math major in college (and pretty good at it) and spent a year in the math Ph.D. program at the University of Virginia (and performed well there as well). These two facts alone would not have equipped me for a career in data science. In fact, it remains unclear to me that those two facts alone would have prepared me for any career (with the possible exception of teaching) without significantly more training.\n\u0026ldquo;There has never been a better time to be a mathematician\u0026rdquo;? When I was in college Business Week published an article declaring \u0026ldquo;There has never been a better time to be a mathematician.\u0026rdquo; At the time, I saw an enormous disconnect between the piece and what I was being taught in math classes (and thus what I considered to be a \u0026ldquo;mathematician\u0026rdquo;). I have come across other pieces lauding this as the age of the mathematicians, and more often than not, I\u0026rsquo;ve wondered if the author knew what students actually studied in math departments.\nBackground on Me The math courses I had as an undergraduate were:\nLinear algebra Discrete math Differential equations (ODEs and numerical) Theory of statistics 1 Numerical analysis 1 (numerical linear algebra) and 2 (quadrature) Abstract algebra Number theory Real analysis Complex analysis Intermediate analysis (point set topology) My program also required a one semester intro to C++ and two semesters of freshman physics. In my year as a math Ph.D. student, I took analysis, algebra, and topology classes; had I stayed in the program, my future coursework would have been similar: pure math where homework problems consistent almost exclusively of proofs done with pen and paper (or in LaTeX).\nWhat is Data Science? Though my current position occasionally requires mathematical proof, I suspect that is rare among data scientist. While the \u0026ldquo;data science\u0026rdquo; demarcation problem is challenging (and I will not seek to solve it here), it seems evident that my curriculum lacked preparation in many essential areas of data science. Chief among these are programming skill, knowledge of experimental statistics, and experience with math modeling.\nData Science Requires Programming and Engineering Few would argue that programming ability is not a key skill of data science. As Drew Conway has argued, a data scientist need not have a degree in computer science, but \u0026ldquo;Being able to manipulate text files at the command-line, understanding vectorized operations, thinking algorithmically; these are the hacking skills that make for a successful data hacker.\u0026rdquo; Many of my undergrad peers, having briefly seen C++ freshman year and occasionally used Mathematica to solve ODEs for homework assignments, would have been unaware that manipulation of a file from the command-line was even possible, much less have been able to write a simple sed script; there was little difference with my grad school classmates.\nMany data science positions require even more than the ability to solve problems with code. As Trey Causey has recently explained, many positions require understanding of software engineering skills and tools such as writing reusable code, using version control, software testing, and logging. Though I gained a fair bit of programming skill in college, these skills, now essential in my daily work, remained foreign to me until years later.\nData Science Requires Applied Statistics My math training had a lack of statistics courses. Though my brief exposure to mathematical statistics has been valuable in picking up machine learning, experimental statistics was missing altogether. Many data science teams are interested in questions of causal inference and design and analysis of experiments; some would make these essential skills for a data scientist. I learned nothing about these topics in math departments. Moreover, machine learning, also a cornerstone of data science, is not a subject I could have even defined until after I was finished with my math coursework; at the end of college, I would have said artificial intelligence was mostly about rule-based systems in Lisp and Prolog.\nData Science Involves Very Applied Math Even if statistics had play a more prominent role in my coursework, those who have studied statistics know there is often a gulf between understanding textbook statistics and being able to effectively apply statistical models and methods to real world problems. This is only an aspect of a bigger issue: mathematical (including statistical) modeling is an extraordinarily challenging problem, but instruction on effectively model real world problems is absent from many math programs. To this day, defining my problem in mathematical terms one of the hardest problems I face; I am certain that I am not alone on this. Though I am now armed with a wide variety of mathematical models, it is rarely clear exactly which model can or should be applied in a given situation.\nI suspect that many people, even technical people, are uncertain as to what academic math is beyond undergraduate calculus. Mathematicians mostly work in the logical manipulation of abstractly defined structures. These structures rarely bear any necessary relationship to physical entities or data sets outside the abstractly defined domain of discourse. Though some might argue I am speaking only of \u0026ldquo;pure\u0026rdquo; mathematics, this is often true of what is formally known as \u0026ldquo;applied mathematics\u0026rdquo;. John D. Cook has made similar observations about the limitations of pure and applied math (as proper disciplines) in dubbing himself a \u0026ldquo;very applied mathematician\u0026rdquo;. Very applied mathematics is \u0026ldquo;an interest in the grubby work required to see the math actually used and a willingness to carry it out. This involves not just math but also computing, consulting, managing, marketing, etc.\u0026rdquo; These skills are conspicuously absent from most math curricula I am familiar with.\nMath ‚Üí Data Science Given this description of how my schooling left me woefully unprepared for a career in data science, one might ask how I have had two jobs with that title. I can think of several (though probably not all) reasons.\nFirst, the academic study of mathematics provides much of the theoretical underpinnings of data science. Mathematics underlies the study of machine learning, statistics, optimization, data structures, analysis of algorithms, computer architecture, and other important aspects of data science. Knowledge of mathematics (potentially) allows the learner to more quickly grasp each of these fields. For example, learning how principle component analysis‚Äîa math model that can be applied and interpreted by someone without formal mathematical training‚Äîworks will be significantly easier for someone with earlier exposure linear algebra. On a meta-level, training in mathematics forces students to think carefully and solve hard problems; these skills are valuable in many fields, including data science.\nMy second reason is connect to the first: I unwittingly took a number of courses that later played important roles in my data science toolkit. For example, my current work in Bayesian inference has been made possible by my knowledge of linear algebra, numerical analysis, stochastic processes, measure theory, and mathematical statistics.\nThird, I did a minor in computer science as an undergraduate. That provided a solid foundation for me when I decided to get serious about building programming skill in 2010. Though my academic exposure to computer science lacked any software engineer skills, I left college with a solid grasp of basic data structures, analysis of algorithms, complexity theory, and a handful of programming languages.\nFourth, I did a masters degree in operations research (after my year as a math PhD student convinced me pure math wasn\u0026rsquo;t for me). This provided me with experience in math modeling, a broad knowledge of mathematical optimization (central to machine learning), and the opportunity to take graduate-level machine learning classes.1\nFifth, my insatiable curiosity in computers and problem solving has played a key role in my career success. Eager to learn something about computer programming, I taught myself PHP and SQL as a high school student (to make Tolkien fan sites, incidentally). Having been given small Mathematica-based homework assignments in freshman differential equations, I bought and read a book on programming Mathematica. Throughout college and grad school, I often tried‚Äîand sometimes succeeded‚Äîto write programs to solve homework problems that professors expected to be solved by hand. This curiosity has proven valuable time and time again as I\u0026rsquo;ve been required to learn new skills and solve technical problems of all varieties. I\u0026rsquo;m comfortable jumping in to solve a new problem at work, because I\u0026rsquo;ve been doing that on my own time for fifteen years.\nSixth, I have been been fortunate enough to have employers who have patiently taught me and given me the freedom to learn on my own. I have learned an enormous amount in my two and a half year professional career, and I don\u0026rsquo;t anticipate slowing down any time soon. As Mat Kelcey has said: always be sure you\u0026rsquo;re not the smartest one in the room. I am very thankful for three jobs where I\u0026rsquo;ve been surrounded by smart people who have taught me a lot, and for supervisors who trust me enough to let me learn on my own.\nFinally,2 it would be hard for me to overvalue the four and a half years of participation in the data science community on Twitter. Through Twitter, I have the ear of some of data science\u0026rsquo;s brightest minds (most of whom I\u0026rsquo;ve never met in person), and I\u0026rsquo;ve built a peer network that has helped me find my current and last job. However, I mostly want to emphasize the pedagogical value of Twitter. Every day, I\u0026rsquo;m updated on the release of new software tools for data science, the best new blog posts for our field, and the musings of of some of my data science heros. Of course, I don\u0026rsquo;t read every blog post or learn every software tool. But Twitter helps me to recognize which posts are most worth my time, and because of Twitter, I know something instead of nothing about Theano, Scalding, and dplyr.3\nConclusions I don\u0026rsquo;t know to what extent my experience generalizes4, in either the limitations of my education or my analysis of my success, but I am obviously not going to let that stop me from drawing some general conclusions.\nHiring Data Scientists For those hiring data scientists, recognize that mathematics as taught might not be the same mathematics you need from your team. Plenty of people with PhDs in mathematics would be unable to define linear regression or bloom filters. At the same time, recognize that math majors are taught to think well and solve hard problems; these skills shouldn\u0026rsquo;t be undervalued. Math majors are also experienced in reading and learning math! They may be able to read academic papers and understand difficult (even if new) mathematical more quickly than a computer scientist or social scientist. Given enough practice and training, they would probably be excellent programmers.\nStudying Math For those studying math, recognize that the field you love, in its formal sense, may be keeping you away from enjoyable and lucrative careers. Most of your math professors have spent their adult lives solving math problems on paper or on a chalkboard. They are inexperienced and, possibly, unknowledgeable about very applied mathematics. A successful career in pure mathematics will be very hard and will require you to be very good. While there seem to be lots of jobs in teaching, they will rarely pay well.\nIf you\u0026rsquo;re still an student, you have a great opportunity to take control of your career path. Consider taking computer science classes (e.g. data structures, algorithms, software engineering, machine learning) and statistics classes (e.g. experimental design, data analysis, data mining).\nFor both students and graduates, recognize your math knowledge becomes very marketable when combined skills such as programming and machine learning; there are a wealth of good books, MOOCs, and blog posts that can help you learn these things. More over, the barrier to entry for getting started with production quality tools has never been lower. Don\u0026rsquo;t let your coursework be the extent of your education. There is so much more to learn!5\nUpdate (Oct. 2017) : I gave a talk based on this post\nUpdate (Mar. 2018) : I get a lot of emails with questions about this post, so I wrote FAQ post trying to answer some of them.\nAt the same time, my academic training in operations research failed me, in some aspects, for a successful career in operations research. For example, practical math modeling was not sufficiently emphasized and the skills of computer programming and software development were undervalued.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOf course, I have plenty of data science skills left to learn. My knowledge of experimental design is still pretty fuzzy. I still struggle with effective mathematical modeling. I haven\u0026rsquo;t deployed a large scale machine learning system to production. I suck at software logging. I have no idea how deep learning works.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI have successfully answered more than one interview question by regurgitating knowledge gleaned from tweets.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAmong other reasons, I didn\u0026rsquo;t really plan to get where I am today. I changed majors no fewer than three times in college (physics, CS, and math) and essentially dropped out of two PhD programs!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, install Anaconda and start playing with some of these IPython notebooks.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2015-05-11T00:00:00Z","image":"https://tdhopper.com/math-class.png","permalink":"https://tdhopper.com/blog/how-i-became-a-data-scientist/","title":"How I Became a Data Scientist Despite Having Been a Math Major"},{"content":"Start Using Landsat on AWS: \u0026ldquo;The Landsat program has been running since 1972 and is the longest ongoing project to collect such imagery. Landsat 8 is the newest Landsat satellite and it gathers data based on visible, infrared, near-infrared, and thermal-infrared light. ‚Ä¶ You can now access over 85,000 Landsat 8 scenes\u0026rdquo; on AWS.\nBeginner\u0026rsquo;s Guide to Linkers: I‚Äôm getting back into doing a little C++ programming. Having spent the last 5 years in scripting languages, this was a helpful refresher on compilation.\nHow to Auto-Forward your Gmail Messages in Bulk: Use Google App Scripts to autoforward emails by simply adding a label. I use this to add things to my Omnifocus task link.\nWhich one result in mathematics has surprised you the most?: On Mathematics Stack Exchange. It might have been Huffman Coding for me.\nRuby Midwest 2013 The Most Important Optimization: Happiness: Ernie Miller explains why he doesn‚Äôt let his career trump his happiness.\nSake by tonyfischetti: Something of a modern GNU Make: \u0026ldquo;Sake is a way to easily design, share, build, and visualize workflows with intricate interdependencies. Sake is self-documenting because the instructions for building a project also serve as the documentation of the project\u0026rsquo;s workflow.\u0026rdquo;\nn1k0/SublimeHighlight: \u0026ldquo;An humble SublimeText package for exporting highlighted code as RTF or HTML.\u0026rdquo;\n","date":"2015-03-23T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-march-23-2015/","title":"Links for March 23, 2015"},{"content":"TIL `cat` will correctly join MP3 files http://t.co/mmc4NN2Kmp\n\u0026mdash; Tim Hopper (@tdhopper) March 15, 2015 From Stackoverflow:\nAn MP3 file is nothing more than the raw MPEG2-Layer 3 (audio) stream data, there is no file level header structure with, for example, duration, original source, encoding info.\nThus\n1 cat file1.mp3 file2.mp3 \u0026gt; out.mp3 can join MP3 files.\n","date":"2015-03-15T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/concatenate-mp3-files/","title":"Concatenate MP3 Files"},{"content":"Dynamically Update a Plot in IPython: One thing I miss about Mathematica is Animate and Manipulate. IPython is slowing getting similar functionality. Here‚Äôs how to dynamically update a plot.\nJiahao\u0026rsquo;s IPython Notebook customizations: Drop this CSS file on your machine, and suddenly your IPython notebooks look quite beautiful!\nDuet Display: I tried Air Display a few years ago, and it wasn‚Äôt worth the hassle. But Duet Display is a fantastic way to turn your iPad into an external display.\nCreating publication-quality figures with Matplotlib: Plotting in Python frustrates me to no end. But here‚Äôs a nice tutorial on creating nice figures in with Matplotlib.\nretrying 1.3.3 : Python Package Index: Python decorators \u0026ldquo;to simplify the task of adding retry behavior to just about anything.\u0026rdquo; These work like a charm!\n","date":"2015-03-13T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-march-13-2015/","title":"Links for March 13, 2015"},{"content":"I gave a talk at the Research Triangle Analysts meetup about Pyspark. It wasn\u0026rsquo;t recorded, but you can see the IPython notebook I presented from.\n","date":"2015-02-28T00:00:00Z","permalink":"https://tdhopper.com/blog/intro-to-pyspark/","title":"Introduction to PySpark"},{"content":"Matt Blodgett: But Where Do People Work in This Office?: \u0026ldquo;After looking through tons of cool office photos of many of the hottest companies in the Valley, I started to play a fun game I made up called \u0026lsquo;spot the desks‚Äô. I‚Äôll show you what I mean.\u0026rdquo;\nWhy We (Still) Believe in Private Offices: Joel Spolsky and Fog Creek Software¬†have been relentless defenders of quite, private offices for developers. They continue that here.\nPandoctor: An Alfred GUI for Pandoc: If you use Pandoc and Alfred, this is worth trying.\nAlfred Hop: I use a little bash tool called Hop to bookmark frequently used directories. I made this tool to give me quick access to my bookmarks from Alfred.\nDiscover Flask: Flask, the lightweight Python framework, is a joy to use. Here‚Äôs a nice introduction to it.\nIntroduction to dplyr: I haven‚Äôt used R much since leaving my last job, but the ecosystem has been booming with great tools; dplyr is one of them.\n","date":"2015-01-19T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-january-19-2015/","title":"Links for January 19, 2015"},{"content":"Sublime: Nice Features \u0026amp; Plugins: A brief talk introducing my favorite editor.\nAlfred Workflow for Pinboard: I\u0026rsquo;ve started using Pinboard a bit for organizing links. Here\u0026rsquo;s something that has the chance of getting me much deeper into pinboard: a powerful Alfred Workflow for interacting with Pinboard from your Mac\u0026rsquo;s keyboard. HT Aaron Bachya\nfullPage.js: I\u0026rsquo;ve been using this jquery plugin in a forthcoming project. It makes it really easy to create slide-like single page websites.\nObfuscating \u0026ldquo;Hello world!\u0026rdquo;: The author attempts to write the worst \u0026lsquo;hello world\u0026rsquo; possibe in Python. He does a good job.\nShow Time in Multiple Time Zones with TextExpander: As I spend more time working with people in different time zones, tools like this help remove the cognitive challenge of translating time.\n","date":"2014-12-29T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-december-29-2014/","title":"Links for December 29, 2014"},{"content":"Time: Programmers all hate time, timezones, etc. Here are some helpful \u0026ldquo;notes about time\u0026rdquo;.\nPython strftime reference: Speaking of time: \u0026ldquo;A quick reference for Python\u0026rsquo;s strftime formatting directives.\u0026rdquo; I have to look this stuff each time I need it.\ngitup: \u0026ldquo;A console script that allows you to easily update multiple git repositories at once\u0026rdquo;\nThe ‚ÄúHow Does a Google Coder Work?‚Äù Edition : I enjoyed this interview. My favorite quote? \u0026ldquo;When you\u0026rsquo;re reading code is it as clear as reading English?\u0026rdquo; \u0026ldquo;If I\u0026rsquo;m reading C++ code, it\u0026rsquo;s clearer.\u0026rdquo;\nSunset Salvo: John Turkey discusses practical data analysis and statistical humility.\n10th Conference on Bayesian Nonparametrics: This is coming up in my own back yard. I‚Äôm excited!\nMachine Learning: The High-Interest Credit Card of Technical Debt: I haven‚Äôt read this in detail, but the premise makes tons of sense to me: \u0026ldquo;It is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning.\u0026rdquo;\n","date":"2014-12-22T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-december-22-2014/","title":"Links for December 22, 2014"},{"content":"Last year, I published nine interviews with Internet friends about how an academically-minded, 22-year old college senior should work on a Ph.D. Many people have told me the interviews have been helpful for them or that they\u0026rsquo;ve emailed them to others.\nI decided to make a dedicated website to host the interviews. You can find it at shouldigetaphd.com.\nI hope this continues to be a valuable resource. I\u0026rsquo;d encourage you to share this with anyone you know who is thinking through this question.\n","date":"2014-12-08T00:00:00Z","permalink":"https://tdhopper.com/blog/shouldigetaphdcom/","title":"ShouldIGetAPhD.com"},{"content":"Sketching as a Tool for Numerical Linear Algebra: A neat paper on sketching algorithms for linear algebra. No, not that kind of sketching. \u0026ldquo;One first compresses it to a much smaller matrix by multiplying it by a (usually) random matrix with certain properties. Much of the expensive computation can then be performed on the smaller matrix, thereby accelerating the solution for the original problem.\u0026rdquo;\nMaps and the Geospatial Revolution: Coursera is teaching a class in the spring on how geospatical technology has changed our world.\nGeoprocessing with Python using Open Source GIS: Speaking of geospatial technology, here are some slides and problems from a class on \u0026ldquo;Geoprocessing with Python\u0026rdquo;.\nHow to use the bash command line history: Bash\u0026rsquo;s history can do more than I realized!\nA geometric interpretation of the covariance matrix: Superb little post explaning covariance matrices with pictures and geometry.\n","date":"2014-12-06T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-december-06-2014/","title":"Links for December 6, 2014"},{"content":"How do I draw a pair of buttocks?: Have you ever wondered how to plot a pair of buttocks in Mathematica? Of course you have.\nFrequentism and Bayesianism: A Python-driven Primer: Jake Vanderplas wrote a \u0026ldquo;brief, semi-technical comparison\u0026rdquo; of frequentist and Bayesian statistical inference using examples in Python.\nskll: Dan Blanchard released version 1.0 of his very cool command line tool for doing experiments with scikit-learn.\nPersonalized Recommendations at Etsy: A fantastic post from Etsy\u0026rsquo;s engineering blog on building scalable, personalized recommendations using linear algebra and locally sensitive hashing. I like math.\nPythonic Clojure: Andrew Montalenti wrote a post analyzing Clojure from a Python programmer\u0026rsquo;s perspective. It\u0026rsquo;s great.\n","date":"2014-12-04T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-december-04-2014/","title":"Links for December 4, 2014"},{"content":"brushfire: Avi Bryant has been building a \u0026lsquo;Brushfire is a framework for distributed supervised learning of decision tree ensemble models in Scala.\u0026rsquo; Fun stuff!\nWhat are the lesser known but useful data structures?: I always enjoy StackOverflow questions like this, but it is not considered a good, on-topic question for this site, of course.\nFree Programming Books: A huge, crowd-sourced list of free programming books by language and topic.\nPhD Dissertations-Machine Learning Department: Seven years of ML PhD dissertations from Carnegie Mellon University. I wish I had time to read Tools for Large Graph Mining.\n","date":"2014-11-24T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-november-24-2014/","title":"Links for November 24, 2014"},{"content":"There\u0026rsquo;s no magic: virtualenv edition: I didn\u0026rsquo;t really get virtualenvs until long after I started programming Python, though they\u0026rsquo;re now an essential part of my toolkit. This is a great post explaining how they work.\nTraps for the Unwary in Python‚Äôs Import System: \u0026ldquo;Python‚Äôs import system is powerful, but also quite complicated.\u0026rdquo;\npyfmt: I recently learned about gofmt for auto-formatting Go code. Here\u0026rsquo;s a similar tool for Python.\nQ: Setting User-Agent Field?: A 1996 question in comp.lang.java on how to set the user agent field for a Java crawler. The signature on the question? Thanks, Larry Page\nalecthomas/importmagic: Python tool and Sublime extension for automatically adding imports.\n","date":"2014-11-17T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-november-17-2014/","title":"Links for November 17, 2014"},{"content":"Amazon Picking Challenge: Kiva Systems (where I interned in 2011) is setting up a robotics challenging for picking items off warehouse shelves.\ncontexttimer 0.3.1: A handy Python context manger and decorator for timing things.\nHow-to: Translate from MapReduce to Apache Spark: This is a helpful bit from Cloudera on moving algorithms from Mapreduce to Spark.\ncombinatorics 1.4.3: Here\u0026rsquo;s a Python module adding some combinatorial functions to the language.\nSpecial methods and interface-based type system: Guido van Rossum explains (in 2006) why Python uses len(x) instead of x.len().\n","date":"2014-11-12T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-november-12-2014/","title":"Links for November 12, 2014"},{"content":"Public Data Sets : Amazon Web Services: Amazon hosts a number of publicly datasets on AWS (including the common crawl corpus and the \u0026ldquo;Marvel Universe Social Graph\u0026rdquo;).\nRapid Web Prototyping with Lightweight Tools: I\u0026rsquo;ve shared this before, but my boss Andrew did a fantastic tutorial last year on Flask, Jinja2, MongoDB, and Twitter Bootstrap. Combined with Heroku, it\u0026rsquo;s surprisingly easy to get a website running these days.\nrest_toolkit: REST has been my obsession of late. Here\u0026rsquo;s a little Python package for quickly writing RESTful APIs.\nThe Joys of the Craft: A quote from Fred Brooks\u0026rsquo; The Mythical Man-Month on why programming is fun.\nHow do I use pushd and popd commands?: I recently learned bash has push and popd commands for temporarily changing directories. This is very handy for scripting.\n","date":"2014-11-03T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-november-03-2014/","title":"Links for November 3, 2014"},{"content":"The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!): I guess the title says it all. By Joel Spolsky.\nUnix Shells - Hyperpolyglot: Very cool comparison of basic command syntax in Bash, Fish, Ksh, Tcsh, and Zsh.\nBetter Bash history: I\u0026rsquo;m pretty stuck on Bash at the moment. Here\u0026rsquo;s a way to get a better history in Bash. (Other shells often improve on Bash\u0026rsquo;s history.)\nusaddress 0.1: I always love seeing a Python library for something I\u0026rsquo;ve tried to do poorly on my own: \u0026ldquo;usaddress is a python library for parsing unstructured address strings into address components, using advanced NLP methods.\u0026rdquo;\nmore-itertools: A great extension to the helpful itertools module in Python. Some particularly helpful functions: chunked, first, peekaboo, and take. Unfortunately, it doesn\u0026rsquo;t have Python 3 support at the moment.\n","date":"2014-11-01T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-november-01-2014/","title":"Links for November 1, 2014"},{"content":"The pyspark documentation doesn\u0026rsquo;t include an example for the aggregateByKey RDD method. I didn\u0026rsquo;t find any nice examples online, so I wrote my own.\nHere\u0026rsquo;s what the documentation does say:\naggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None)\nAggregate the values of each key, using given combine functions and a neutral \u0026ldquo;zero value\u0026rdquo;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U\u0026rsquo;s, The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.\nreduceByKey and aggregateByKey are much more efficient than groupByKey and should be used for aggregations as much as possible.\nIn the example below, I create an RDD that is a short list of characters. My functions will aggregate the functions together with concatenation. I added brackets to the two types of concatenation to help give you an idea of what aggregateByKey is doing.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ \u0026#39;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 1.1.0 /_/ Using Python version 2.7.5 (default, Mar 9 2014 22:15:05) SparkContext available as sc. In [1]: # Create rdd that is a list of characters In [2]: sc.parallelize(list(\u0026#34;aaaaabbbbcccdd\u0026#34;)) \\ ...: .map(lambda letter: (letter, {\u0026#34;value\u0026#34;: letter})) \\ ...: .aggregateByKey( ...: # Value to start aggregation (passed as s to `lambda s, d`) ...: \u0026#34;start\u0026#34;, ...: # Function to join final data type (string) and rdd data type ...: lambda s, d: \u0026#34;[ %s %s ]\u0026#34; % (s, d[\u0026#34;value\u0026#34;]), ...: # Function to join two final data types. ...: lambda s1, s2: \u0026#34;{ %s %s }\u0026#34; % (s1, s2), ...: ) \\ ...: .collect() Out[2]: [(\u0026#39;a\u0026#39;, \u0026#39;{ { [ start a ] [ [ start a ] a ] } [ [ start a ] a ] }\u0026#39;), (\u0026#39;c\u0026#39;, \u0026#39;{ [ start c ] [ [ start c ] c ] }\u0026#39;), (\u0026#39;b\u0026#39;, \u0026#39;{ { [ [ start b ] b ] [ start b ] } [ start b ] }\u0026#39;), (\u0026#39;d\u0026#39;, \u0026#39;[ [ start d ] d ]\u0026#39;)] ","date":"2014-10-03T00:00:00Z","permalink":"https://tdhopper.com/blog/pysparks-aggregatebykey-method/","title":"Pyspark's AggregateByKey Method"},{"content":"Hammock: A lightweight wrapper around the Python requests module to convert REST APIs into \u0026ldquo;dead simple programmatic APIs\u0026rdquo;. It\u0026rsquo;s a clever idea. I\u0026rsquo;ll have to play around with it before I can come up with a firm opinion.\npipsi: Pipsi wraps pip and virtualenv to allow you to install Python command line utilities without polluting your global environment.\nWriting a Command-Line Tool in Python: Speaking of Python command line utilities, here\u0026rsquo;s a little post from Vincent Driessen on writing them.\nIterables vs. Iterators vs. Generators: Vincent has been on a roll lately. He also wrote this \u0026ldquo;little pocket reference on iterables, iterators and generators\u0026rdquo; in Python.\nDesign for Continuous Experimentation: Talk and Slides: I didn\u0026rsquo;t watch the lecture, but Dan McKinley\u0026rsquo;s slides on web experimentation are excellent.\nApache Spark: A Delight for Developers: I\u0026rsquo;ve been playing with PySpark lately, and it really is fun.\n","date":"2014-09-30T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-september-30-2014/","title":"Links for September 30, 2014"},{"content":"Philosophy of Statistics (Stanford Encyclopedia of Philosophy): I suspect that a lot of the Bayesian vs Frequentist debates ignore the important epistemological underpinnings of statistics. I haven‚Äôt finished reading this yet, but I wonder if it might help.\nConnect Sunlight Foundation to anything: ‚ÄúThe Sunlight Foundation is a nonpartisan non-profit organization that uses the power of the Internet to catalyze greater U.S. Government openness and transparency.‚Äù They now of an IFTTT channel. Get push notifications when the president signs a bill!\nfurbo.org ¬∑ The Terminal: Craig Hockenberry wrote a massive post on how he uses the Terminal on OS X for fun and profit. You will learn things.\nA sneak peek at Camera+ 6‚Ä¶ manual controls are coming soon to you! : I‚Äôve been a Camera+ user on iOS for a long time. The new version coming out soon is very exciting.\nGitHut - Programming Languages and GitHub: A very clever visualization of various languages represented on Github and of the properties of their respective repositories.\n","date":"2014-09-25T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-september-25-2014/","title":"Links for September 25, 2014"},{"content":"Open Sourcing a Python Project the Right Way: Great stuff that should be taught in school: ‚ÄúMost Python developers have written at least one tool, script, library or framework that others would find useful. My goal in this article is to make the process of open-sourcing existing Python code as clear and painless as possible.‚Äù\nelasticsearch/elasticsearch-dsl-py: Elasticsearch is an incredible datastore. Unfortunately, its JSON-based query language is tedious, at best. Here‚Äôs a nice higher-level Python DSL being developed for it. It‚Äôs great!\nEquipment Guide ‚Äî The Podcasting Handbook: Dan Benjamin of 5by5 podcasting fame is writing a book on podcasting. Here‚Äôs his brief equipment guide.\nbachya/pinpress: Aaron Bach put together a neat Ruby script that he uses to generate his link posts. This is similar to but better than my sundry tool.\nMarkdown Resume Builder: I haven‚Äôt tried this yet, but I like the idea: a Markdown based resume format that can be converted into HTML or PDF.\nGit - Tips and Tricks: Enabling autocomplete in Git is something I should have done long ago.\nApache Storm Design Pattern‚ÄîMicro Batching: Micro batching is a valuable tool when doing stream processing. Horton Works put up a helpful post outlining three ways of doing it.\n","date":"2014-09-20T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-september-20-2014/","title":"Links for September 20, 2014"},{"content":"Recently, I\u0026rsquo;ve spent a lot of time going back and forth between Python dicts and JSON. For some reason, I decided last week that I\u0026rsquo;d be useful to be able to quickly convert a Python dict to pretty printed JSON.\nI created a TextExpander snippet that takes a Python dict from the clipboard, converts it to JSON, and pastes it.\nHere are the details:\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/usr/bin/env python import os, json import subprocess def getClipboardData(): p = subprocess.Popen([\u0026#39;pbpaste\u0026#39;], stdout=subprocess.PIPE) retcode = p.wait() data = p.stdout.read() return data cb = eval(getClipboardData()) print json.dumps(cb, sort_keys=True, indent=4, separators=(\u0026#39;,\u0026#39;, \u0026#39;: \u0026#39;)) ","date":"2014-09-18T00:00:00Z","permalink":"https://tdhopper.com/blog/quickly-converting-python-dict-to-json/","title":"Quickly Converting Python Dict to JSON"},{"content":"textract: textract is a Python module and a command line tool for text extraction from many file formats. It cleverly pulls together many libraries into a consistent API.\nFlask Kit: I\u0026rsquo;ve been reading a lot about Flask (the Python web server) lately. Flask Kit is a little tool to give some structure to new Flask projects.\ncookiecutter: I was looking for this recently, but it I couldn\u0026rsquo;t find it. \u0026ldquo;A command-line utility that creates projects from cookiecutters (project templates). E.g. Python package projects, jQuery plugin projects.\u0026rdquo; There\u0026rsquo;s even a Flask template!\nOver 50? You Probably Prefer Negative Stories About Young People: A research paper from a few years ago show that older people prefer to read negative news about young people. \u0026ldquo;In fact, older readers who chose to read negative stories about young individuals actually get a small boost in their self-esteem.\u0026rdquo;\nEpisode 564: The Signature: The fantastic Planet Money podcast explains why signatures are meaningless in a modern age. My scribbles have become even worse since listening to this.\ngithub-selfies: Here\u0026rsquo;s a Chrome and Firefox extension that allows you to quickly embed gif selfies in Github posts. Caution: may lead to improved team morale.\n","date":"2014-09-10T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-september-10-2014/","title":"Links for September 10, 2014"},{"content":"TIL how to checkout the last branch you were on in Git (before the current):\ngit checkout -\n\u0026mdash; Tim Hopper (@tdhopper) September 10, 2014 ","date":"2014-09-10T00:00:00Z","image":"https://tdhopper.com/images/til.png","permalink":"https://tdhopper.com/blog/shortcut-to-switch-to-previous-git-branch/","title":"Shortcut to Switch to Previous Git Branch"},{"content":"Ggplot2 To Ggvis: I\u0026rsquo;m a huge fan of ggplot2 for data visualization in R. Here\u0026rsquo;s a brief tutorial for ggplot2 users to learn ggvis for generating interactive plots in R using the grammar of graphics.\nFrom zero to storm cluster for scikit-learn classification | Daniel Rodriguez: This is a very cool, if brief, blog post on using streamparse, my company\u0026rsquo;s open source wrapper for Apache Storm, and scikit-learn, my favorite machine learning library, to do machine learning on data streams.\nPythonic means idiomatic and tasteful: My boss Andrew recently shared an old blogpost he wrote on what it means for code to be Pythonic; I think he\u0026rsquo;s right on track.\nPythonic isn‚Äôt just idiomatic Python ‚Äî it‚Äôs tasteful Python. It‚Äôs less an objective property of code, more a compliment bestowed onto especially nice Python code.\ngit workflow: In my ever continuing attempt to be able to run my entire life from Alfred, I recently installed this workflow that makes git repositories on my computer easily searchable.\nAlfred-Workflow: Speaking of Alfred, here\u0026rsquo;s a handy Python library that makes it easy to write your own (if you\u0026rsquo;re a Python programmer).\nSquashing commits with rebase: Turns out you can use git rebase to clean up your commits before you push them to a remote repository. This can be a great way to make the commits your team sees more meaningful; don\u0026rsquo;t abuse it.\n","date":"2014-08-30T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-august-30-2014/","title":"Links for August 30, 2014"},{"content":"As I have noted before, body weight is a noisy thing. Day to day, your weight will probably fluctuate by several pounds. If you\u0026rsquo;re trying to lose weight, this noise can cause unfounded frustration and premature excitement.\nWhen I started a serious weight loss plan a year and a half ago, I bought a wifi-enabled Withings Scale. The scale allows me to automatically sync my weight with Montior Your Weight, MyFitnessPal, RunKeeper, and other fitness apps on my phone. IFTTT also has great Withings support allowing me to push my weight to various other web services.\nOne IFTTT rule I have appends my weight to a text file in Dropbox. This file looks like this:\n1 2 3 4 5 6 7 8 9 10 263.86 August 21, 2014 at 05:56AM 264.62 August 22, 2014 at 08:27AM 264.56 August 23, 2014 at 09:41AM 263.99 August 24, 2014 at 08:02AM 265.64 August 25, 2014 at 08:08AM 267.4 August 26, 2014 at 08:16AM 265.25 August 27, 2014 at 09:08AM 264.17 August 28, 2014 at 07:21AM 264.03 August 29, 2014 at 08:43AM 262.71 August 30, 2014 at 08:47AM For a few months, I have been experimenting with using this time series to give myself a less-noisy update on my weight, and I\u0026rsquo;ve come up with a decent solution.\nThis R script will take my weight time series, resample it, smooth it with a rolling median over the last month, and write summary stats to a text file in my Dropbox. It\u0026rsquo;s not the prettiest script, but it gets the job done for now.1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 INPUT_PATH \u0026lt;- \u0026#34;~/Dropbox/Text Notes/Weight.txt\u0026#34; OUTPUT_PATH \u0026lt;- \u0026#34;~/Dropbox/Text Notes/Weight Stats.txt\u0026#34; library(lubridate) library(ggplot2) library(zoo) # READ FILE con \u0026lt;- file(INPUT_PATH, \u0026#34;rt\u0026#34;) lines \u0026lt;- readLines(con) close(con) # PARSE INTO LISTS OF WEIGHTS AND DATES parse.line \u0026lt;- function(line) { s \u0026lt;- strsplit(line, split=\u0026#34; \u0026#34;)[[1]] date.str \u0026lt;- paste(s[2:10][!is.na(s[2:10])], collapse=\u0026#34; \u0026#34;) date \u0026lt;- mdy_hm(date.str, quiet=TRUE) l \u0026lt;- list(as.numeric(s[1]), date) names(l) \u0026lt;- c(\u0026#34;weight\u0026#34;, \u0026#34;date\u0026#34;) l } list.weight.date \u0026lt;- lapply(lines, parse.line) weights \u0026lt;- lapply(list.weight.date, function(X) X$weight) dates \u0026lt;- lapply(list.weight.date, function(X) X$date) # BUILD DATA FRAME df \u0026lt;- data.frame(weight = unlist(weights), date = do.call(\u0026#34;c\u0026#34;, dates) ) # CREATE TIME SERIES AND RESAMPLE ts \u0026lt;- zoo(c(df$weight), df$date) ts \u0026lt;- aggregate(ts, time(ts), tail, 1) g \u0026lt;- round(seq(start(ts), end(ts), 60 * 60 * 24), \u0026#34;days\u0026#34;) ts \u0026lt;- na.approx(ts, xout = g) # FUNCTION TO GET WEIGHT N-DAYS AGO IF WEIGHT IS SMOOTHED BY ROLLING MEDIAN # OVER A GIVEN (smooth.n) NUMBER OF DAYS days.ago \u0026lt;- function(days, smooth.n) { date \u0026lt;- head(tail(index(ts),days + 1),1) smoothed \u0026lt;- rollmedianr(ts, smooth.n) as.numeric(smoothed[date]) } # SMOOTH WEIGHT BY 29 DAYS AND GENERATE SOME SUMMARY STATS days = 29 current.weight \u0026lt;- days.ago(0, days) x \u0026lt;- c(current.weight, current.weight-days.ago(7, days), current.weight-days.ago(30, days), current.weight-days.ago(365, days), current.weight-max(ts)) x = round(x, 1) names(x) = c(\u0026#34;current\u0026#34;, \u0026#34;7days\u0026#34;, \u0026#34;30days\u0026#34;, \u0026#34;365days\u0026#34;, \u0026#34;max\u0026#34;) fileConn\u0026lt;-file(OUTPUT_PATH) w \u0026lt;- c(paste(\u0026#34;Weight (lbs):\u0026#34;, x[\u0026#34;current\u0026#34;]), paste(\u0026#34;Total Œî:\u0026#34;, x[\u0026#34;max\u0026#34;]), paste(\u0026#34;1 Week Œî:\u0026#34;, x[\u0026#34;7days\u0026#34;]), paste(\u0026#34;1 Month Œî:\u0026#34;, x[\u0026#34;30days\u0026#34;]), paste(\u0026#34;1 Year Œî:\u0026#34;, x[\u0026#34;365days\u0026#34;])) writeLines(w,fileConn) close(fileConn) The output looks something like this:\n1 2 3 4 5 Weight (lbs): 265.7 Total Œî: -112 1 Week Œî: -0.8 1 Month Œî: -4.8 1 Year Œî: -75 I want this script to be run every time my weight is updated, so I created a second IFTTT rule that will create a new file in my Dropbox, called new_weight_measurement, every time I weigh in. On my Mac Mini, I have a Hazel rule to watch for a file of this name to be created. When Hazel sees the file, it runs my R script and deletes that file.\nMy Hazel rule looks like this:\nThe \u0026rsquo;embedded script\u0026rsquo; that is run is the R script above; I just have to tell Hazel to use the Rscript shell.2\nAt this point, every time I step on my scale, a text file with readable statistics about my smoothed weight appear in my Dropbox folder.\nOf course, I want this updated information to be pushed directly too me. Hazel is again the perfect tool for the job. I have a second Hazel rule that watches for Weight Stats.txt to be created. Hazel can pass the path of the updated file into any script of your choice. You could, for example, use Mailgun to email it to yourself or Pushover to push it to your mobile devices. Obviously, I want to tweet mine.\nI have a Twitter account called @hopsfitness where I\u0026rsquo;ve recently been tracking my fitness progress. On my Mac Mini, I have t configured to access @hopsfitness from the command line. Thus, tweeting my updated statistics is just a matter of a little shell script executed by Hazel:\nSince this data goes to Twitter, I can get it painlessly pushed to my phone: Twitter still allows you subscribe to accounts via text message, which I\u0026rsquo;ve done with @hopsfitness. A minute or so after I step on my scale, I get a text with useful information about where I am and where I\u0026rsquo;m going; this is much preferable to the noisy weight I see on my scale.\nUpdate (2014-12-06): I replaced my R script with a Python/pandas script. It requires Python 3 (to render the delta characters).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import dateutil import pandas as pd import random from os.path import expanduser, join home = expanduser(\u0026#34;~\u0026#34;) with open(join(home, \u0026#34;Dropbox/Text Notes/Weight.txt\u0026#34;), \u0026#34;r\u0026#34;) as f: lines = f.readlines() def parse_line(line): s = line.split(\u0026#34; \u0026#34;) weight = float(s[0]) date = dateutil.parser.parse(\u0026#39; \u0026#39;.join(s[1:4])) return date, weight weight = pd.DataFrame([parse_line(l) for l in lines], columns=[\u0026#34;date\u0026#34;, \u0026#34;weight\u0026#34;]) \\ .set_index(\u0026#34;date\u0026#34;) \\ .resample(\u0026#34;1D\u0026#34;, how=\u0026#34;mean\u0026#34;) weight[\u0026#34;missing\u0026#34;] = weight.weight.isnull() weight.weight = weight.weight.interpolate(method=\u0026#34;linear\u0026#34;) std = weight.weight.diff().dropna().std() noise = weight.missing.map(lambda missing: random.normalvariate(0, std) if missing else 0) weight.weight = weight.weight + noise smoothed = pd.ewma(weight.weight, span=30) current = smoothed[-1] stats = \u0026#34;\u0026#34;\u0026#34; Weight (lbs): %(weight).1f Total Œî: %(total).1f 1 Week Œî: %(week).1f 1 Month Œî: %(month).1f 1 Year Œî: %(year).1f \u0026#34;\u0026#34;\u0026#34;.strip() % {\u0026#34;weight\u0026#34;: current, \u0026#34;total\u0026#34;: current - smoothed[0], \u0026#34;week\u0026#34;: current - smoothed[-8], \u0026#34;month\u0026#34;: current - smoothed[-32], \u0026#34;year\u0026#34;: current - smoothed[-366], } with open(join(home, \u0026#34;Dropbox/Text Notes/Weight Stats.txt\u0026#34;), \u0026#34;wb\u0026#34;) as f: f.write(bytes(stats, \u0026#39;UTF-8\u0026#39;)) This assumes your input file is formatted like mine, but you could easily adjust the first part of the code for other formats.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can download R here; installing it should add Rscript to your system path.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2014-08-30T00:00:00Z","permalink":"https://tdhopper.com/blog/tracking-weight-loss-with-r-hazel-withings-and-ifttt/","title":"Tracking Weight Loss with R, Hazel, Withings, and IFTTT"},{"content":"I spend a lot of time in IPython Notebooks for work. One of the few annoyances of IPython Notebooks is that they require keeping a terminal window open to run the notebook server and kernel. I routinely launch a Notebook kernel in a directory where I keep my work related notebooks. Earlier this week, I started to wonder if there was a way for me to keep this kernel running all the time without having to keep a terminal window open..\nIf you\u0026rsquo;ve ever tried to do chron-like automation on OS X, you\u0026rsquo;ve surely come across launchd, \u0026ldquo;a unified, open-source service management framework for starting, stopping and managing daemons, applications, processes, and script\u0026rdquo;. You\u0026rsquo;ve probably also gotten frustated with launchd and given up.\nI recently started using LaunchControl \u0026ldquo;a fully-featured launchd GUI\u0026rdquo; for launchd; it\u0026rsquo;s pretty nice and worth $10. It occurred to me that LaunchControl would be a good way to keep my Notebook kernel running in the background.\nI created a LaunchControl to run the following command.\n1 /usr/local/bin/IPython notebook --matplotlib inline --port=9777 --browser=false This launches an IPython Notebook kernel accessible on port 9777; setting the browser flag to something other than an installed browser prevents a browser window from opening when the kernel is launch.\nI added three other launchd keys in LaunchControl:\nA Working Directory key to tell LaunchControl to start my notebook in my desired folder. A Run At Load key to tell it to start my kernel as soon as I load the job. And a Keep alive key to tell LaunchControl to restart my kernel should the process ever die. Here\u0026rsquo;s how it looks in LaunchControl:\nAfter I created it, I just had to save and load, and I was off to the races; the IPython kernel starts and runs in the background. I can access my Notebooks by navigating to 127.0.0.1:9777 in my browser. Actually, I added 127.0.0.1 parsely.scratch to my hosts file so I can access my Notebooks at parsely.scratch:9777. This works nicely with Chrome\u0026rsquo;s autocomplete feature. I\u0026rsquo;m avoiding the temptation to run nginx and give it an even prettier url.\n","date":"2014-08-28T00:00:00Z","permalink":"https://tdhopper.com/blog/keeping-ipython-notebooks-running-in-the-background/","title":"Keeping IPython Notebooks Running in the Background"},{"content":"How do I generate a uniform random integer partition?: This week, I wanted to generate random partitions of integers. Unsurprisingly, stackoverflow pulled through with a Python snippet to do just that.\nFirefox and Chrome Bookmarks: I love Alfred as a launcher in OS X. I use it many, many times a day. I just found this helpful workflow for quickly searching and opening my Chrome bookmarks.\nYNAB for iPad is Here: YNAB has been the best thing to ever happen to my financial life. I use it to track all my finances. They just released a beautiful iPad app. Importantly, it brings the ability to modify a budget to mobile!\nDistributed systems theory for the distributed systems engineer: I work on distributed systems these days. I need to read some of these papers.\n","date":"2014-08-28T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-august-28-2014/","title":"Links for August 28, 2014"},{"content":"How can I pretty-print JSON at the command line?: I needed to pretty print some JSON at the command line earlier today. The easiest way might be to pipe it through python -m json.tool.\nIntegrating Alfred \u0026amp; Keyboard Maestro: I love Keyboard Maestro for automating all kinds of things on my Mac, but I\u0026rsquo;m reaching a limit of keyboard shortcuts I can remember. Here\u0026rsquo;s an Alfred workflow for launching macros instead.\nstreamparse 1.0.0: My team at Parsely is building a tool for easily writing Storm topologies (for processing large volumes of streaming data) in Python. We just released 1.0.0!\nTextExpander Tools: Brett Terpstra, the king of Mac hacks, has some really handy tools for TextExpander.\nGNU Parallel: GNU parallel is a shell tool for executing jobs in parallel using one or more computers using xargs-like syntax. Pretty cool. HT @oceankidbilly.\n","date":"2014-08-25T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-august-25-2014/","title":"Links for August 25, 2014"},{"content":"Arrow: better dates and times for Python: Arrow is a slick Python library \u0026ldquo;that offers a sensible, human-friendly approach to creating, manipulating, formatting and converting dates, times, and timestamps\u0026rdquo;. It\u0026rsquo;s a friendly alternative to datetime.\nDocker via Homebrew: I\u0026rsquo;m starting to use Docker (\u0026ldquo;Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications\u0026rdquo;) on occasion. Here are easy install instructions for Mac users.\nMining Massive Datasets MOOC: I\u0026rsquo;m terrible at completing MOOCs, but I\u0026rsquo;m really interested in this new on on Mining Massive Datasets.\nURL Pinner - Chrome Web Store: URL Pinner is one of my favorite Chrome Extensions. I use it to automatically pin my Gmail and Rdio windows (which I almost always have open).\nUsing multitail for monitoring multiple log files: If you work with distributed systems, you\u0026rsquo;re probably used to SSH-ing into multiple machines to access logs. Multitool might save you some time.\nSaturday Morning Breakfast Cereal: SBMC shows how job interviews would go if we were more honest.\n","date":"2014-08-23T00:00:00Z","permalink":"https://tdhopper.com/blog/sundry-links-for-august-23-2014/","title":"Links for August 23, 2014"},{"content":"Back in July, I posted some analysis of my attempt at weight loss. Now that I\u0026rsquo;m four months further down the line, I thought I\u0026rsquo;d post a follow-up.\nI continue to be fascinated with how noisy my weight time series is. While I\u0026rsquo;ve continued to lose weight over time, my weight goes up two out of five mornings.\nHere\u0026rsquo;s a plot of the time series of my change in weight. Note how often the change is positive, i.e. I appear to have gained weight:\nThis volatility can hide the fact that I\u0026rsquo;m making progress! When I put a regression line through the points, you can see that the average change slightly below zero:1\nI have wondered recently if my average change in weight is correlated with the day of the week. My hypothesis is that my weight tends to go up over the weekends, so I created a boxplot of my change in weight categorized by day.\nIndeed, on Sundays and Mondays (i.e. weight change from Saturday morning to Sunday morning and Sunday morning to Monday morning) my median weight change is slightly above zero. This makes sense to me: on Saturdays, I\u0026rsquo;m more likely to be doing things with friends, and thus I have less control over my meals.2\nI wish I had a good explanation for why the change on Friday is so dramatic, but I don\u0026rsquo;t. Any guesses?\nI mentioned this to my college roommate who is a financial planner. He noted how similar this is to investing; it\u0026rsquo;s a constant battle for him to convince his clients to look at average behavior instead of daily changes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlso, beer.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2013-11-28T00:00:00Z","permalink":"https://tdhopper.com/blog/noisy-series-and-body-weight-take-2/","title":"Noisy Series and Body Weight Part 2"},{"content":"I recently discovered the Twitter account @primes. Every hour, they tweet the subsequent prime number. This made me wonder two things. First, what is the largest prime that you can tweet (in base-10 encoding in 140 characters).1 Second, how long until they get there.\nDoing some quick calculations in Mathematica, I believe the largest 140 digit prime is the following:\n9999999999999999999999999999999999999999999999 9999999999999999999999999999999999999999999999 999999999999999999999999999999999999999999999997\nWolfram Alpha confirms that this is prime and that the next prime is 141 characters.\nAs for how long it would take, recall that the number of primes less than $n$ is approximately $\\frac{n}{\\ln n}$. The number of primes less than $10^{141}$ is approximately\n$$\\pi(10^{140}) = \\frac{10^{140}}{140\\cdot \\ln 10} = 3.1\\cdot 10^{137}.$$\nThat\u0026rsquo;s $3\\cdot 10^{57}$ times the estimated number of atoms in the universe. Looks like @primes should be able to tweet for a while.\nThe largest known prime is $2^{57,885,161} ‚àí 1$ and has 17,425,170 digits.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2013-11-08T00:00:00Z","permalink":"https://tdhopper.com/blog/tweeting-primes/","title":"Tweeting Primes"},{"content":"I have largely moved from Textmate to Sublime Text 2 for text editing. Among other reasons, Sublime Text is cross platform, and I use Windows at work and a Mac at home. I have also started writing as much as I can in Markdown.\nI intended to write a blog post about using Sublime Text as a tool for writing Markdown. However, the inimitable Federico Viticci, of macstories.net, has already written that post, so I will simply refer you there.\n","date":"2013-10-19T00:00:00Z","permalink":"https://tdhopper.com/blog/sublime-text-and-markdown/","title":"Sublime Text and Markdown"},{"content":"Ralph Keyes, The Height of Your Life:\nI\u0026rsquo;ve heard this sort of thing repeatedly from tall men. It\u0026rsquo;s not the incessant commentary about their height that is so annoying, it\u0026rsquo;s the stupefying boredom of it all. Were anyone to say something original or witty or different in any way, the constant chatter thrown their way might at least be entertaining. But soon after reaching their full height, tall people realize to their horror that the lifetime\u0026rsquo;s commentary to which they\u0026rsquo;ve been sentenced comes mostly from those with least to say.\n","date":"2013-10-02T00:00:00Z","permalink":"https://tdhopper.com/blog/the-incessant-commentary-on-being-tall/","title":"The Incessant Commentary on Being Tall"},{"content":"I put on some weight during my time in grad school, and this spring, I decided to do something about it. In April, I started using MyFitnessPal to track my food intake and exercise, and I run a net calorie deficit every day. Thankfully this seems to be working.\nIn May, I bought a Withings WS-30 wireless scale. When I first heard about this wifi scales, I thought they sounded like a gimmick, however the Withings has become a helpful tool in the weight loss process.\nEvery morning, I step on the scale and my weight is automatically broadcast to MyFitnessPal, Monitor Your Weight on iOS, and a text file in my Dropbox folder (via IFTTT and Withings\u0026rsquo; API). MyFitnessPal adjusts my daily calorie limit by my weight, Monitor Your Weight is a great tool for visualizing progressing, and I use the text file to import a ggplot time series of my weight into Day One each month.\nAn interesting aspect of my weight time series is how noisy it is. (No doubt this is true for others as well.) On many mornings, my weight is up from the day before (despite a fairly consistent net caloric deficit). As you can see from the plot, my weight jumps up and down daily even though the overall trend is downward.\nI have been wondering what percentage of days I actually lose weight, so I decided to find out. The plot below is a histogram of my weight change from day to day.1\nThe data appears nearly Gaussian around 0! (In fact, the p-value on the Shapiro-Wilk normality test is 0.11, arguably not small enough to reject the null hypothesis that the data are not normally distributed.) Fortunately the mean of the differences is actually about -0.24 (pounds/day), and my progress is downward.\nIn total, I lost weight on 48 days, gained on 33, and stayed the same on 4% of the days. That means I\u0026rsquo;ve steadly lost weight while only moving down on 56% of days. I guess I don\u0026rsquo;t need to be depressed every time my weight jumps up slightly\u0026hellip;.\nThis isn\u0026rsquo;t 100% true. I\u0026rsquo;m hiding the fact that I missed weighing-in on some days.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2013-07-26T00:00:00Z","permalink":"https://tdhopper.com/blog/noisy-series-and-body-weight/","title":"Noisy Series and Body Weight"},{"content":"I have started to realize that Monte Carlo methods of various kinds keep coming up in my work. Despite significant application of Monte Carlo in my grad school research, I think I only know enough to be dangerous. I\u0026rsquo;d like to get a better grasp on Monte Carlo methods (especially MCMC and simulation).\nI asked on Twitter if anyone had a recommended reference that was readable and practical. Despite my love of measure theory, what I want is Monte Carlo Methods for the Very Applied Mathematician, not a theoretical text.\nI got several recommendations. I\u0026rsquo;m not sure that any are exactly what I\u0026rsquo;m looking for, but I am certainly going to look deeper into them. Interestingly, they are all Springer books.\nSeveral people recommended Glasserman\u0026rsquo;s Monte Carlo Methods in Financial Engineering. I don\u0026rsquo;t work in the financial sector, so it\u0026rsquo;s hard for me to evaluate the table of contents to tell how well it generalizes.\nSomeone else recommended both Explorations in Monte Carlo Methods and Handbook of Markov Chain Monte Carlo for two levels of MCMC.\nFinally, I got a recommendation for Introducing Monte Carlo Methods with R. This might be closest to what I\u0026rsquo;m looking for. It appears to cover a breadth of topics, and it includes lots of code.\n","date":"2013-07-23T00:00:00Z","permalink":"https://tdhopper.com/blog/guide-to-monte-carlo-methods/","title":"Guide to Monte Carlo Methods?"},{"content":"I had the honor of being a technical reviewer for John Myles White\u0026rsquo;s outstanding book, \u0026ldquo;Bandit Algorithms for Website Optimization: Developing, Deploying, and Debugging.\u0026rdquo; I strongly suggest it to anyone curious about the subject. This book is an excellent start to understanding the field and provides practical advice on applying these algorithms effectively.\nCheck it out!\n","date":"2013-01-29T00:00:00Z","image":"https://tdhopper.com/images/bandit.png","permalink":"https://tdhopper.com/blog/bandit-algorithms-for-website-optimization/","title":"Bandit Algorithms for Website Optimization"},{"content":"I gave a talk at a recent Research Triangle Analysts meetup on scikit-learn, the excellent machine learning library for Python. You can see the IPython notebook that I presented from.\n","date":"2013-01-21T00:00:00Z","permalink":"https://tdhopper.com/blog/scikit/","title":"Introduction to Scikit-Learn"},{"content":"I presented at INFORMS 2012 on Bringing Operations Research into the 21st Century with Online Video. You can see the recording on Youtube.\n","date":"2012-12-01T00:00:00Z","permalink":"https://tdhopper.com/blog/orvideo/","title":"Bringing Operations Research into the 21st Century with Online Video"},{"content":"I gave a talk at PyCarolinas 2012 about using Pickle and Redis to persist data with Python. It wasn\u0026rsquo;t recorded, but you can [see the IPython notebook I presented from](http://nbviewer.jupyter.org/github/tdhopper/Pickle-and-Redis/ blob/master/Pickle%20and%20Redis.ipynb).\n","date":"2012-10-22T00:00:00Z","permalink":"https://tdhopper.com/blog/pickle-and-redis/","title":"Pickle and Redis"},{"content":"For part of my sophomore year of college, I was a computer science major. When I realized that I loved my CS theory courses while my classmates hated them, I decide to major in math instead. I enjoyed the programming classes enough, but programming is not what I wanted to spend my time doing.\nThe summer after my junior year, I was accepted to a math REU at Rochester Institute of Technology. The first thing my adviser Stanislaw Radziszowski asked me was whether or not I could program! I spent the whole summer programming combinatorial graph theory-related algorithms in C1.\nNow I, like many of my operations research classmates, spend much of my time programming. Despite the importance of writing code for solving operations research problems, I am surprised how little programming is discussed. The admissions page for my program says nothing about programming ability, but it is implicitly assumed that programming is a skill that students have.\nMoreover, I suspect the operations research-specific parts of the research behind many journal articles is only a fraction of the actual work done by the authors. Much of the required work is implementation and debugging of their algorithms. Yet, articles contain little-to-no discussion of the actual code. Even worse, the code is often not published or reviewed. I can only imagine how many coding errors underly the results of peer-reviewed papers.\nMarc Kuo recently blogged about how operations researchers need to get with the program (pun intended). His post kicked of tons of discussion in its comments, on Google+, on Hacker News, and on OR-Exchange.\nThis discussion came at a good time for me. I\u0026rsquo;m in the middle of my first big coding project of my PhD research. Despite completing a computer science minor and spending two summers doing nothing but coding, I never learned good software engineering practices. I decided at the beginning of the summer to force myself not to just write this code to get the job done but to write good code.\nTo start, I finally started using git and github for version control. I have tried several times before, but I have always found it rather confusing2. This git tutorial finally got me over the hump. Now I can easily branch my code into different versions, and I have the ability to go back to old versions when I screw something up.\nSecond, I started teaching myself about unit testing. Code testing was never mentioned in any of my classes in college, and I never hear operations researchers talk about it. Again, I have no doubt that the code behind much published work is full of mistakes. Operations researchers need good testing practices?3\nThird, I\u0026rsquo;m trying to write clean, object-oriented, well-commented code. My intention is to publish this code on github when the corresponding paper is published. I want my results to be easily reproducible by others and open to scrutiny. I would also like my code to be reusable for future research. My design patterns might not be quite there yet, but I\u0026rsquo;m trying to move in that direction.\nI realized that I\u0026rsquo;ve used the word I as much as Stephen Wolfram blog post. I have no desire to toot my own horn here; I\u0026rsquo;m just thankful this conversation is happening, and I want to continue it. Good software is crucial to good operations research (both in the academy and out), and yet academic operations researchers, in my experience, talk very little about good software engineering practices. We can do better.\nI\u0026rsquo;m eternally indebted to my brilliant research partner Evan who taught me how to use bash, vim, and subversion, among other things.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI feel vindicated by a recent thread on Hacker News.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIncidentally, here\u0026rsquo;s an interesting Quora thread about testing stochastic algorithms.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-07-12T00:00:00Z","permalink":"https://tdhopper.com/blog/operations-research-and-computer-programming/","title":"Operations Research and Computer Programming"},{"content":"Here are my notes on the derivation of the Least Squares Policy Iteration (LSPI) algorithm. The notes are based on the original paper by Lagoudakis and Parr.\nPrevious Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","date":"2012-06-18T00:00:00Z","image":"https://tdhopper.com/images/lspi.png","permalink":"https://tdhopper.com/blog/notes-on-the-derivation-of-least-squares-policy-iteration/","title":"Notes on the Derivation of Least Squares Policy Iteration"},{"content":"I offered my calculus students bonus points to write a limerick or haiku on their final exam. I got some great answers! And the students seem to enjoy it. My favorite limerick (on improper integrals):\nAn integral has bounds from zero to one\nYou finish the problem and think you are done\nThen your mind double checks\nThe equation is one over x And the improper integral has a different sum\nMy favorite haiku gives the formula for integration by parts:\nInt of mu dv:\nEqual to mu v minus\nInt of v d mu\nHere are some other good ones:\nDoing Integrals‚Ä¶ Oh How I shall miss thee, Calc! Goodbye, It\u0026rsquo;s been real?\nCalculus is hard Derivative and limit ¬°Yo no se hombre!\nMacLaurin Series is an infinite series Centered at zero\nCalculus is great\nI\u0026rsquo;m sure I\u0026rsquo;ll use this in life\nOr maybe I won\u0026rsquo;t\nStudying \u0026rsquo;til 5\nAnd almost sleeping thru test\nWas a real bad call\nFinding integrals\nIs used to find area\nUnderneath the curve\nCalc is fun to do\nIf you like to integrate\nx from a to b\nCalculus two, sigh\nWhy must you torment me so?\nI thought we were friends\nF of e to x\nDerived is e to the x\nLone function like it\nI like calculus\nThe feeling is mutual\nBut not all the time\nIntegrals are fun\nu-sub can be tricky‚Ä¶ yes!\nDon\u0026rsquo;t forget plus C\nCalculus is hard\nEight A.M. is too early\nHad a good time though\nThe ratio test:\nIf the limit equals one\nUse another test\nDoes this series telescope?\nFor my sake, I truly hope\nComparisons are icky,\nIntegrals make me sickly,\nTurns out the answer is nope. :-(\nHere are a few from when I taught at UVA:\nAn integral is\nA derivative reversed\nDon‚Äôt forget constants!\nThanks to L‚ÄôHopital\nWe can use derivatives\nto find a limit!\nFind the area,\nBetween the two stated curves,\nUsing integrals.\nNatural log of Zero,\ndoes not exist, but\nLn of 1 doesn\u0026rsquo;t\nCalculus is great\nBut only if taught by\nTim At U of V.A.\n","date":"2012-05-08T00:00:00Z","image":"https://tdhopper.com/images/poems.png","permalink":"https://tdhopper.com/blog/calculus-haikus-and-limericks/","title":"Calculus Haikus and Limericks"},{"content":"I am teaching the dreaded calculus II this semester. I\u0026rsquo;ve known many students who flew through calc I in college (having taken calculus in high school) only to¬†receive¬†a reality check from calc II the next semester. In the US, calc II often involves a significant section on \u0026ldquo;techniques of integration\u0026rdquo; where students learn techniques such as partial fractions, trig¬†substitutions, integration by parts. Unlike much of differential calculus, which is taught in calc I, and unlike much of the math taught before college, integration is harder to do algorithmically. That is, a calc II professor cannot simply outline surefire steps guaranteed to give an antiderivative for any function. The inimitable Robert Ghrist explains it this way in his \u0026ldquo;funny little calculus text\u0026rdquo;:\nOf course, algorithms for computing many antiderivatives do exist (and are used in Maple, Mathematica, and Wolfram Alpha), but I\u0026rsquo;d be fired if I tried to take my undergrads through¬†Symbolic Integration I: Transcendental Functions..\nInstead, calculus II teachers teach a handful of methods and attempt to teach students intuition for where to use what technique. Even more important, I try to teach my students the skill of trying a method, seeing that the method does not work, then trying something else. Try-fail-try-fail-try again. I do not think that high school students learn that skill‚Äîa skill vital to success not just in calc II, but in every discipline requiring analytical problem solving. Yesterday, my adviser and I were discussing the first big research problem that I\u0026rsquo;ll be tackling this summer. He noted that our first attempt at solving a massive problem would probably fail; they usually do.\nFortunately, I\u0026rsquo;ve been failing at solving problems at least since taking number theory with Dan Krider in 2003. I know what Edison meant by, \u0026ldquo;I have not failed. I\u0026rsquo;ve just found 10,000 ways that won\u0026rsquo;t work.\u0026rdquo; I hope my students are learning how to fail and how to try again. However, I think that kids need to learn earlier. High school assignments should not be set up for students to succeed the first time, every time. Somehow, teachers need to allow students to take risks, learn from their mistakes, and rebound. I\u0026rsquo;d love to hear feedback from students who are learning these lessons and teachers trying to teach them.\n","date":"2012-04-13T00:00:00Z","permalink":"https://tdhopper.com/blog/teaching-students-to-fail/","title":"Teaching Students to Fail"},{"content":"Yesterday, someone on MathOverflow asked\nConsider $n$ points generated randomly and uniformly on a unit square. What is the expected value of the area (as a function of $n$) enclosed by the convex hull of the set of points?\nSomeone quickly cited¬†2004 paper provides an analytical result for the cases where $n=3$ and $n=4$:\nFor $n=3$ the expected value is $11/144$ and for $n=4$ it is $11/72$.\nThis is certainly a nontrivial result. However, the value can be approximated by generating a large number of random points, finding the area of the convex hull, and averaging the areas. Of course, finding the convex hull and the area of the convex hull of a set of points requires a little work. Mathematica provides functions for generating random points and finding the area of the convex hull of a set of points quickly. As a result, I was able to perform a Monte Carlo simulation for the $n=3$ and $n=4$ case in a couple of lines of Mathematica code:\nSampling 5000 cases for each returned results fairly close to the predicted average.\n","date":"2012-04-05T00:00:00Z","permalink":"https://tdhopper.com/blog/average-area-of-a-random-hull/","title":"Average Area of a Random Hull"},{"content":"Last month, Stephen Wolfram did a blog post on the Personal Analytics of his life. For years, he\u0026rsquo;s recorded every phone call, keyboard stroke, email, and step. He made beautiful graphs to show his activity over the years. A Wolfram Alpha developer just posted a Mathematica notebook on the Wolfram blog allowing anyone to do the same email analysis that Wolfram did with any IMAP email account. Of course, I dropped what I was doing to try it out with my Gmail account. At first, it failed to finish processing my incoming email because the JVM ran out of memory. It took me a while to figure out how to tell JLink to let Java have more memory1. Here\u0026rsquo;s a plot of emails sent by from Gmail me over the last six years:\nI started using Gmail regularly after I graduated from college in 2008 (once my college Exchange-based email account was gone). My emailing was noticeably sparse from May 2008-May 2009. During that time I was a teacher, and I didn\u0026rsquo;t spend nearly as much time on a computer as I do now. You can also see a gap during the summer of 2011. I was working at Kiva Systems at the time and primarily used my company email. On the horizontal axis, you notice I\u0026rsquo;m pretty silent between 11 PM and 7 AM. I need my sleep, and I never work at night! My emailing is light from 6-9 PM too. Here\u0026rsquo;s a graph of my email received over the past six years. It comes in pretty heavy from 8 AM to midnight!\nThe thick line just under 6 AM is the Google Calendar email updates I used to get every morning. I stopped getting those once I got an iPod Touch this past Christmas. I can\u0026rsquo;t remember what email used to come at 3 AM for a few years. This next graphic shows the average number of emails I receive per day for each month. My amount of emailing ramped up once I started using email. Notice the downward trend on incoming emails recently: I\u0026rsquo;ve been unsubscribing from unnecessary mailing lists and circulars. My emailing had pretty serious peak last May right before I moved to Boston. Not sure why.\nHere we have a histogram of the time at which I send emails. Apparently I\u0026rsquo;m most likely to send an email just after 10 PM. I wouldn\u0026rsquo;t have guess that. Don\u0026rsquo;t expect to hear from me after midnight!\nAs a good operations researcher, I wondered if I received email according to a Poisson process. I pulled the email time stamp data into R. I get email pretty steadily between 8 AM and 10 PM. I looked at the emails that arrived in that interval since September 2011. The mean interarrival time is 0.53 hours. The standard deviation is 0.92. If it were a poisson process, interarrival times would be exponentially distributed, and the mean and standard deviation would be equal. Below is a histogram of the interarrival times of my emails. The red line is an exponential distribution with the rate set to 1 over the mean interarrival time of my emails. It\u0026rsquo;s not a terrible fit!2 One reason the email arrival rate might not be exponential is that I frequently have back-and-forth email conversations with people, which skews the distribution towards short interarrival times. I might do some more statistics later, but I have homework to do.\nReinstallJava[CommandLine -\u0026gt; \u0026ldquo;java\u0026rdquo;, JVMArguments -\u0026gt; \u0026ldquo;-Xmx4024m\u0026rdquo;]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThat\u0026rsquo;s not a official statistical statement!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-04-05T00:00:00Z","permalink":"https://tdhopper.com/blog/my-email-analytics/","title":"My Email Analytics"},{"content":"Over the past 18 months, I\u0026rsquo;ve been slowly learning some machine learning. One thing I\u0026rsquo;ve noticed is that most of the math in machine learning is optimization. Regression is typically minimization of some error term. Support vector machines are a quadratic optimization problem with linear constraints. Learning a neural network is simply¬†solving a nonconvex optimization problem. Clustering often takes the form of¬†expectation-maximization. I\u0026rsquo;m currently learning Bayesian network structure learning which is an extremely difficult combinatorial optimization problem.\nYesterday on Twitter, I commented that I am surprised at how little operations research people and machine learning people talk. Most of the math of OR is, like machine learning, optimization. All the same theorems apply, and we use many of the same algorithms; we just apply them in different ways. I got helpful feedback from the nerds that follow me. Jeff Linderoth pointed to the recent book¬†Optimization for Machine Learning¬†by his colleague¬†(et al)¬†Stephen J. Wright at University of Wisconsin, Madison. From what I can tell, Wright is an OR guy in a computer scientists clothing. There\u0026rsquo;s a two-hour lecture by Wright on the same topic that I look forward to watching. Jeff also pointed to the work of his¬†colleague¬†Ben Recht¬†who\u0026rsquo;s looking at the optimization problems in ML from a theoretical standpoint.¬†Paul Kerl¬†linked to¬†Jorge Nocedal\u0026rsquo;s work at Northwestern. Nocedal and Recht seem to have feet in both worlds. John Myles White noted that the legendary optimizer Stephen Boyd came to the New York Academy of Science\u0026rsquo;s Machine Learning event last year. I also came across a 2006 paper on¬†The Interplay of Optimization and Machine Learning Research. The authors note some difference between an OR and ML perspective on optimization:\nWe observe that the qualities of good optimization algorithms¬†from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom¬†line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms¬†that work in reasonable computational time for specific classes of problems1.\nA bigger question might be where optimization lies¬†as a discipline. Since I\u0026rsquo;ve been in OR, I\u0026rsquo;ve always considered optimization as a subfield of OR. But as I read applied OR literature, I find it jarring to see the details of solving a difficult optimization problem mixed with the application of the solution to a real world problem. Of course, both ML and OR require practitioners to understand how the algorithms work. Optimization problems are hard, and a black box solution rarely works for any of us. But perhaps optimization will become a field of its own that OR and ML can both feed from instead of the two working independently.\nI haven\u0026rsquo;t read the whole paper (that\u0026rsquo;s from the abstract), but I\u0026rsquo;m not entirely convinced that is true. Modern machine learning often requires large scale problems to be solved quickly on-line, while optimizers often solve a problem offline and speed is negotiable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-04-03T00:00:00Z","permalink":"https://tdhopper.com/blog/operations-research-machine-learning-and-optimization/","title":"Operations Research, Machine Learning, and Optimization"},{"content":"Stephen Wolfram, of Wolfram Research and Mathematica fame, did a Q\u0026amp;A (i.e. AMA) on Reddit today. I just enjoyed reading through his answers. A few interesting answers stood out to me.\nSomeone ask Wolfram\u0026rsquo;s opinion on P=NP. He thinks it\u0026rsquo;s undecidable1.\nSome smart aleck threw the Riemann hypothesis at him. Interestingly, Wolfram also suspects this is undecidable2.\nOne questioner asked about open sourcing old versions of Mathematica. Wolfram responded very winsomely, in my view. I didn\u0026rsquo;t know that they\u0026rsquo;ve thought about making the core language more freely available. I\u0026rsquo;d like to see that.\nHis most interesting answer is his opinion on Matlab. He argues that Matlab has remained matrix-centric when so much of contemporary mathematics goes beyond that. \u0026ldquo;In the complete web of algorithms in Mathematica, things that can reasonably be represented as numerical matrices are perhaps 5 or 10% of the total.\u0026rdquo;¬†However, Wolfram believes that Mathematica isn\u0026rsquo;t outdone by Maple in the realm of matrices.\nWolfram relays that a major goal of Mathematica is \u0026ldquo;to make a single coherent system in which one can work, and in which everything fits nicely together.\u0026rdquo; I argued that that\u0026rsquo;s one thing they\u0026rsquo;ve done quite well.\nI appreciate Wolfram doing this. I continue to be optimistic about Mathematica as a product, and I hope they have a bright future ahead of them.\nSee the Wikipedia page on undecidability for more.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBoth the Riemann hypothesis and P=NP have been around for many years and have a big bounty on solving them:¬†http://www.claymath.org/millennium/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-03-05T00:00:00Z","permalink":"https://tdhopper.com/blog/stephen-wolframs-ama/","title":"Stephen Wolfram's AMA"},{"content":"Although I was a computer science minor, I\u0026rsquo;d never heard of statistical machine learning until after college. Now I dabble in machine learning on the side. In the long run, I\u0026rsquo;m interested in studying the intersection of operations research and learning, i.e. intelligent optimization systems. Two years ago, I stumbled across Peter Norvig\u0026rsquo;s essay How to Write a Spelling Corrector. Google is a notoriously good spelling corrector; just try googling \u0026ldquo;spellign.\u0026rdquo; I find that Google knows what word I\u0026rsquo;m trying to spell even when an application\u0026rsquo;s built-in spell check fail. Norvig explains how to use Bayes theorem to write a pretty-good spelling corrector in 21 lines of Python. In college, I had good grounding in probability and computing, but I don\u0026rsquo;t recall having seen the two mixed so elegantly. I\u0026rsquo;m pleased to now know that Norvig was only scratching the surface! This week, I\u0026rsquo;ve been watching Sebastian Thurn\u0026rsquo;s lectures on Kalman Filters¬†for AI. Probabilistic techniques for filtering noisy data can be used, for example, in a robot keeping track of objects moving around it.\n","date":"2012-03-02T00:00:00Z","permalink":"https://tdhopper.com/blog/the-spelling-corrector-that-got-me-interested-in-machine-learning/","title":"The Spelling Corrector that Got Me Interested in Machine Learning"},{"content":"As an undergraduate, our math department used Wolfram Research\u0026rsquo;s¬†Mathematica heavily for instruction in a number of classes. Initially, I found it perplexing and frustrating. While most of my peers remained in that state (and never used it again after those classes), I soon found myself ordering and reading An Introduction to Programming with Mathematica.\nSeven years later, I find myself using Mathematica almost daily. As a student, it is one of the most helpful tools at my disposal, and it has saved me countless hours of tedious computation by hand. I‚Äôm not sure I can express all the ways in which I appreciate it, but I hope to share some.\nI admit that I primarily use Mathematica as a glorified calculator. Most of my code is single use code to help me with a homework assignment. I have written some longer code for class projects, but rarely more than a few hundred lines. However, for the work that I have had over the past seven years, it is exactly the right tool, and I don‚Äôt know of any other language which comprehensively offers all the features I need within its core language.\nOne other note to the Redditors and cynics (but I repeat myself): I\u0026rsquo;m not recommending or encouraging programmers to jump ship from their main languages to Mathematica. I‚Äôm not suggesting that Mathematica doesn‚Äôt have any shortcomings. I‚Äôm not arguing that Mathematica is good for everything. I‚Äôm well aware that Mathematica is an expensive, closed platform. I‚Äôm well aware that Mathematic has the worst undo ever. I‚Äôm not writing an advertisement or getting paid by Wolfram. I‚Äôm simply shared the story of a program that has become an invaluable part of my schooling.\n1. Powerful Symbolic Computations Perhaps the thing Mathematica is most well-known for is symbolic computations. The oldest Mathematica file I have on my computer is a single line of code that I apparently used on a differential equations quiz in 2005. In it, I did a partial fraction decomposition: the bane of calculus 2 students, but easy for a computer.\nOne of the benefits of Mathematica, is the elegance of typesetting in both the input and the output. Wolfram has taken great care to make Mathematica an aesthetically excellent experience, and I‚Äôm grateful for that.\nThese days, I avoid doing algebraic manipulations by hand at all costs. It‚Äôs not worth it to me to risk making errors that might trickle down into my work. I let computers handle such things for me. Thus, when I‚Äôm doing homework, I usually have a notebook open filled with one-off expressions like\nOf course, it can solve much harder problems too. Integration is no problem. Here‚Äôs a triple integral I solved in my electricity and magnetism class sophomore year. (I wish I remembered what it all means.)\nThe output is messy because Mathematica tried to solve the integral as generally as possible. We can get a more clear answer by clarifying some assumptions we‚Äôre making about the parameters.\n2. Functional Programming Like R, Mathematica allows procedural programming.\nHowever, again like R, Mathematica is really built for functional programming. Wolfram has a great tutorial on the topic, but let me share a brief example from my own use. On a homework assignment this week, I wanted to measure the total tardiness for various schedules in a single machine problem. Each of the four jobs had a total processing time, given by {2,4,6,8}, and a due date {4,14,10,16}. The tardiness of a job for a given schedule (i.e. ordering of the jobs) is 0 if the job is completed on time and how late it is otherwise. First I set processing time and due dates for the four jobs:\nAny permutation of the job indices {1,2,3,4} gives a valid schedule. Suppose we want to know the total lateness of the schedule x={1,4,3,2}. The ordering of processing times is given by p[[x]] so the time when each job is completed is a running total of the processing times:\nThe lateness of each job is defined by the completion time minus the due date:\nTo get the tardiness, we want the max of the lateness and zero. There are a number of ways to do this, but one is to apply a max function to each element of the list:\n(The # and \u0026amp; are part of Mathematica‚Äôs notation for pure functions.) Or, more succinctly,\nIn a non-functional language, this would have required a for-loop and several lines of code. In a functional language, it naturally fits into a single line. Getting the total tardiness adds no more complexity:\nsince % returns the last line evaluated.\nThis is only a simple example of a functional operation in Mathematica. Expressions can become much more complex. All my Mathematica code is littered with functional expressions, but rarely will you see a for-loop or a while-loop in my code. And I like it that way.\nOh, and if you want to parallelize these operations: not a problem.\n3. Optimization As a student of operations research, I spend a lot of time solving optimization problems. Solving optimization problems of many flavors is built right into Mathematica. Solving linear programs given the matrices is easy with the LinearProgramming function. Because most of the problems I‚Äôve solved up to this point have been ‚Äútoy‚Äù problems for class, I can‚Äôt attest to Mathematica‚Äôs ability to handle large-scale problems, but they claim to be able to handle large problems. Mathematica‚Äôs ExampleData function gives easy access to many data sets, including NetLib.org‚Äôs LP problems. Mathematica could solve this problem with 6072 rows and 12230 columns in 60 seconds on my 11-inch Macbook Air.\nThe built-in solver certainly isn‚Äôt as robust as CPLEX or other commercial solver, but it does, at least, provide several solution methods.\nI most often find myself using the Minimize and Maximize functions with explicit constraints:\nIn the future, I hope to do a post on using Mathematica as a pseudo-modeling language. You can see the documentation for a number of other optimization related functions.\nRecently, I‚Äôve been working with stochastic dynamic programming problems (i.e. Markov decision processes). Mathematica offers the easiest memoization I‚Äôve ever seen in any language. Combined with functional aspects, I can solve dynamic programs with relatively little code.\n4. Graphics When I am doing school work, I want to be able to do complicated computations and then visualize the results quickly. Because of how tightly knit the native Mathematica graphics are built into the core language, I don‚Äôt have to go out of my way to do this.\nLast semester, I wanted to demonstrate a Monte Carlo algorithm for navigating a maze. Over 200 iterations, a relatively simple solver (built, of course, in Mathematica) could find an optimal path through a 4 by 3 maze. for a report I was writing.\nWorking straight from the output of the solver, in about twenty lines of code, I output a grid showing candidate solution every other iteration (the green cells indicate cells where the action is optimal). Followed by an Export function, the graphic was ready to be included in my file. All of this without having to open another program or import any graphics packages.\n5. Documentation Wolfram has been careful to write readable and thorough documentation for Mathematica. Though Mathematica is not free software, its 10,000+ pages of documentation are available online. Not only does the documentation for every function (usually including bullet points with Basic Examples, Scope, Generalizations \u0026amp; Extensions, Applications, Properties \u0026amp; Relations, and Neat Examples), it‚Äôs full of tutorials on various aspects of the language. Of you read the help inside of Mathematica, the files are simply notebooks, so the code can be evaluated within the documentation. I think you‚Äôd be hard pressed to find a language with better documentation.\n6. Naming Conventions If Mathematica wins one debate hands down, its naming conventions. By their own standards, ‚ÄúAs with most Mathematica functions, the names are usually complete English words, fully spelled out.‚Äù If you know the mathematical name for something, you can probably guess the Mathematica form.\nStephen Wolfram wrote a blog post a few years ago on his personal role in naming Mathematica functions. Perhaps not too different from his late friend Steve Jobs, Wolfram desires intense control of the finest details of his products.\nI just realized that over the course of the decade during which were developing Mathematica 6‚Äîand accelerating greatly towards the end‚ÄîI spent altogether about 10,000 hours doing what we call ‚Äúdesign reviews‚Äù for Mathematica 6, trying to make all those new functions and pieces of functionality in Mathematica 6 be as clean and simple as possible, and all fit together.\nI think this has paid off.\nSome people would complain about a language such an enormous number of named expressions, but Wolfram (the man and the company) have been so careful in constructing it that it doesn‚Äôt feel bloated.\n7. Interactivity In version 7, Wolfram introduced interactivity into Mathematica. The Manipulate function is one I have found extremely valuable. It allows you to parametrize an expression and adjust the parameters while seeing results in real-time. For example, you could use Manipulate to adjust the region over which a function is plotted:\nA side benefit to the careful construction of the language is that functions with related behavior often have interchangeable expression lists. Manipulate can be replaced with Animate with no other changes.\nOr Table for that matter:\n8. Continual Development Thankfully, Wolfram hasn‚Äôt given up on Mathematica. It‚Äôs been in development now for nearly 24 years. Mathematica 7 (released in November 2008) introduced interactivity features, access to many data sets, and built-in parallel computing, among other things.\nMathematica 8, released in November 2010, brought integration with Wolfram Alpha and free form input. I find myself using this frequently when I‚Äôm teaching calculus. For example:\nMathematica 8 also brought incredible probability computations. What‚Äôs the probability that a standard normal random variable is less than a Uniform(0,1) random variable? No problem.\nThe list of things new in version 8 goes on.\n9. Comprehensiveness A feature of Mathematica that is hard to articulate is the comprehensiveness of the features I‚Äôve already mentioned plus many more. It‚Äôs a full featured programming language, but the core language also extends to the depths of applied and pure mathematics. Symbolic manipulation? Check. Numerical methods? Check. Abstract algebra? Check. Graph theory? Statistics? Visualization? Optimization? String Processing? Differential equations? Computational chemistry? Calculus? Check. Check. Check.\nThe comprehensiveness of Mathematica\u0026rsquo;s functionality along with dynamic typing and functional programming allows me to write code to do complicated tasks very quickly.¬†I love it.\nConclusion No doubt, Mathematica has its limitations1: Worlds worst undo. Not object-oriented. Closed platform. Expensive. No autosave. No data frame structure.\nHowever, for me, it‚Äôs an invaluable tool. Last semester, I saw a less computer savvy fellow graduate student writing out a huge table by hand. I don‚Äôt recall the name of what he was doing, but it was something to do with measuring the distance between permutations. I told him I could do it for him in a single line of Mathematica.\nIn just a few minutes I wrote him the following code. It ended up taking me more than one line, but I wrote the code much faster than he was generating it by hand. (His table was actually for the 4-permutation case, so it was 24x24 instead of 6x6.)\nUsing Mathematica for little things like this. It allows me to spend my time and brain power on the things that computers can‚Äôt handle.\nI love Mathematica. And maybe you will too.\n(I wrote this post in Mathematica. You can check out the notebook here.)\nI might follow with a post on that very point\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-02-10T00:00:00Z","permalink":"https://tdhopper.com/blog/mathematica-a-love-story/","title":"Mathematica: A Love Story"},{"content":"I assume most people who are nerdy enough to read this blog are nerdy enough to know about the $\\mathcal{P}$ vs $\\mathcal{NP}$ problem1.¬†I first learned about this problem taking computer science classes in college, and it all seemed very theoretical at the time. Now that I study operations research, the problem is very real. Operations researchers are often limited in their pursuits by the challenges of $\\mathcal{NP}$-hard problems, and many operations researchers spend their careers trying to solve hard problems. Last semester, I came across Laurence Wolsey\u0026rsquo;s beautiful description of how various people might view this problem. (Of course, the¬†inimitable Randall Munroe¬†has offered a similar look at the issue.)\nA pessimist might say that as most problems appear to be hard (i.e., their decision version lies in $\\mathcal{NPC}$), we have no hope of solving instances of large size (because in the worst case we cannot hope to do better than enumeration), and so we should give up. A mathematician (optimist) might set out to become famous by proving that $\\mathcal{P=NP}$. A mathematician (pessimist) might set out to become famous by proving that $\\mathcal{P\\neq NP}$. A mathematician (thoughtful) might decide to ask a different question: Can I find an algorithm that is guaranteed to find a solution \u0026ldquo;close to optimal\u0026rdquo; in polynomial time in all cases2. A probabilist (thoughtful) might also ask a different question: Can I find an algorithm that runs in polynomial time with high probability and that is guaranteed to find an optimal or \u0026ldquo;close to optimal\u0026rdquo; solution with high probability? An engineer would start looking for a heuristic algorithm that produces practically usable solutions. Your boss might say: I don\u0026rsquo;t care a damn about integer programming theory. You just worry about our scheduling problem. Give me a feasible production schedule for tomorrow in which William Brown and Daughters\u0026rsquo; order is out of the door by 4 P.M. A struggling professor might say: Great. Previously I was trying to develop one algorithm to solve all integer programs, and publishing one paper every two years explaining why I was not succeeding. Now I know that I might as well study each $\\mathcal{NP}$ problem individually. As there are thousands of them, I should be able to write twenty papers a year. Needless to say they are all right. There is no easy and rapid solution, but the problems will not go away, and more and more fascinating and important practical problems are being formulated as integer programs. So in spite of the $\\mathcal{NP}$-completeness theory, using an appropriate combination of theory, algorithms, experience, and intensive calculation, verifiably good solutions for large instances can and must be found3.\nIf that\u0026rsquo;s not the case, you might check out this page: http://simple.wikipedia.org/wiki/P_versus_NP.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee http://en.wikipedia.org/wiki/Polynomial_approximation_scheme.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFrom Integer Programming, Page 87.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-02-02T00:00:00Z","permalink":"https://tdhopper.com/blog/mathematicians-engineers-and-businessmen-on-npc-problems/","title":"Mathematicians, Engineers, and Businessmen on NPC Problems"},{"content":"In economics, some have asserted the efficient market hypothesis. The idea is that market prices take into account all the information currently available. If the efficient market hypothesis holds, an investor couldn\u0026rsquo;t consistently beat the market because his knowledge about the markets is no better than anyone else\u0026rsquo;s. An accompanying \u0026ldquo;joke\u0026rdquo; is that an economist would never bend over to pick up a $20 bill on the street. If there was really $20 to be taken, someone would have taken it already.\nAs I\u0026rsquo;m moving towards research and away from classes in my PhD program, I sometimes find myself believing the efficient research hypothesis: if an idea I have is good and correct, someone must have had it already. I have the same temptation outside of school. On occasion, I consider an idea for a website or computer program but then decide that if it were really a good idea, someone would have had it already. This can\u0026rsquo;t always be right. I don\u0026rsquo;t know if markets are efficient or not; I\u0026rsquo;m neither an economist nor an investor. But the progression of knowledge certainly is not efficient. New things are always available to be learned and studied, and these things aren\u0026rsquo;t always obvious from the information currently available. Research (and other projects) requires more than just thinking of a good idea; it requires sweat and elbow grease.\n","date":"2012-01-27T00:00:00Z","image":"https://tdhopper.com/images/money.png","permalink":"https://tdhopper.com/blog/the-efficient-research-hypothesis/","title":"The Efficient Research Hypothesis"},{"content":"In 1953, the eminent mathematician John Von Neumann (as they say, \u0026ldquo;Most mathematicians prove what they can, von Neumann proves what he wants\u0026rdquo;) wrote a letter to T. V. Moore of Standard Oil. Apparently, Moore had written Von Neumann about an operations problem he had. Eighteen tankers would transport fuel from La Salina to Las Piedras to Aruba. Moore wanted to \u0026ldquo;determine the economic value of increasing the number of berths for the loading of these tankers in La Salina from 3 to 4 or to 5.\u0026rdquo;1\nHowever, due to \u0026ldquo;weather, conditions in the ports of call, etc\u0026hellip; the comings and goings of the tankers are described in statistical terms only.\u0026rdquo; Significant uncertainty existed in the time between ports and in loading and unloading times. Because of the complexity added by these uncertainties, Von Neumann speculated that the system would be very difficult to describe analytically. A few years earlier, in 1946, Von Neumann and Stanislaw Ulam worked to solve the radiation shielding problem. And, \u0026ldquo;Despite having most of the necessary data\u0026hellip; the problem could not be solved with analytical calculations.\u0026rdquo;2\nIn this similarly probabilistic system, Ulam and Von Neumann decided to approximate the results they needed instead by repeatedly generating possible outcomes based on statistical manipulations and a (pseudo) random number generator. Their code name for this project was \u0026ldquo;Monto Carlo.\u0026rdquo; Von Neumann suggested that Standard Oil could build a probabilistic model of the tanker problem. For example, a trip from La Salina to Las Piedras might take 8 hours with a probability of .8 (good weather conditions), 10 hours with a probability of .15 (moderate weather), and 15 hours with a probability of 0.05 (poor weather). Based on this model and given a table of random numbers, he could simulate possible outcomes of a trip that (roughly) match reality.\nThe procedure would have to be somewhat like this: Represent each tanker by some form of record, e.g., by a punch card, showing its exit time from La Salina\u0026hellip;. Program calculations which will develop the further history of this tanker, always deriving those quantities which depend on chance\u0026hellip;, with the use of suitable tables of random numbers.\nOf course, Moore would not want to generate just one possible trip. But if the process was done repeatedly, the average benefit of having some number of berths in the simulation would be an approximation of the real benefit:\nIt is thus possible to trace the history of as many days of operation as desired (say, a few years). One can then work out the behavior of sufficiently large samples for any assumed number of berths and thereby get an evaluation of the economic significance of any particular arrangement (i.e. any particular number of berths).\nThe same method Ulam and Von Neumann used to understand radioactive particles (which behave probabilistically) could be used to model full shipments. Fortunately, Monte Carlo simulation now requires neither punch cards or random number tables. Von Neumann\u0026rsquo;s punch card method would quickly be replaced by computer programs and pseudo-random number generation algorithms. This early application of Monte Carlo methods to operations problems is one of many.\nI have recently been looking at online stochastic scheduling problems. In these problems, a system is asked to fulfill some scheduling requests while there is uncertainty (perhaps uncertainty about what other requests will come or how long the job might take to complete). A common approach to these problems is to solve deterministic scheduling problems based on Monte Carlo simulations of the future. The system somehow combines these various schedules to make a decision. There are many applications of Monte Carlo methods in other fields as well.\nJohn von Neumann (2005). Mikl√≥s R√©dei. ed. John von Neumann: Selected letters. History of Mathematics. 27. American Mathematical Society. p. 123. ISBN 0-8218-3776-1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://en.wikipedia.org/wiki/Monte_Carlo_method#History\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-01-25T00:00:00Z","image":"https://tdhopper.com/images/tanker.png","permalink":"https://tdhopper.com/blog/from-nuclear-weapons-to-operations-research/","title":"From Nuclear Weapons to Operations Research"},{"content":"My friend John Cook¬†asked me an interesting question recently:\nIf you had a room full of people with a graduate degree in [operations research], what¬†things would nearly everyone in the room know?\nOperations research is notoriously hard to define. According to the Institute for Operations Research and Management Science, ‚ÄúIn a nutshell, operations research (O.R.) is the discipline of applying advanced analytical methods to help make better decisions.‚Äù I suspect graduate programs spend most of their time teaching those ‚Äúanalytical¬†methods,‚Äù i.e. mathematical and computational techniques for modeling and solving problems related to decisions. Examples include Received exception: linear programming, nonlinear programming, integer programming, dynamic programming, stochastic programming, stochastic models, queueing theory, game theory, and simulation. The course requirements for OR PhD students at¬†my university provide an upper bound for this problem: the only courses everyone must take are linear programming, nonlinear programming, and stochastic modeling. Some topics are surprisingly optional; in particular: simulation, statistics, integer programming/combinatorial optimization. John suggests that statistics PhD programs are similar. Topics diverge rather quickly after first year courses. Are all graduate programs like this? Is this a necessary evil (or evil at all)?\n","date":"2012-01-16T00:00:00Z","permalink":"https://tdhopper.com/blog/what-do-all-or-folks-know/","title":"What do all operations researchers know?"},{"content":"A former grad school classmate of mine lived largely off of oatmeal and carrots. Suppose he wanted to be sure to get 2000 calories/day, 60 milligrams of vitamin C per day, and no more than 40 grams of fat per day. How should he balance his intake of oatmeal and carrots while minimizing his expense at the grocery store? Suppose he eats \\(S\\) pounds of carrots per day and \\(Q\\) pounds of dry oatmeal. A pound of carrots contains 172 calories; a pound of oatmeal contains 1742 calories1. The total number of calories he gets in a day is \\(172\\cdot C+1732\\cdot Q.\\) To be sure he gets the adequate number of calories, he needs $$172\\cdot C+1732\\cdot Q\\geq 2000.$$ A pound of carrots contains 0.839 grams of fat, and a pound of oatmeal contains 25 grams of fat. For the total amount of fat has to be less than 40: $$0.839\\cdot C+25\\cdot Q\\leq 40.$$ A pound of carrots contains 19 milligrams of vitamin C, and a pound of oatmeal contains 15 milligrams of vitamin C. Since the total amount of vitamin C must be at least 60: $$19\\cdot C+15\\cdot Q\\geq 60.$$ If you graph these three inequalities, you get the blue region shown below. Any point in the blue region represents a combination of carrots and oatmeal that would provide sufficient nutrients without giving too much fat. What we want to know is which combination is least expensive.\nA pound of carrots and a pound of oatmeal both cost about $0.50. So, we want to minimize¬†the function $0.5\\cdot C+0.5\\cdot Q$ while still staying inside the blue region. We can write all of these things in a form that operations researchers call a \u0026ldquo;linear program.\u0026rdquo;2\n$$ \\begin{align}\\text{minimize }\\;\\; \u0026amp;¬†0.5\\cdot C+0.5\\cdot Q\\\\ \\text{subject to }\\;\\; \u0026amp;172\\cdot C+1742\\cdot Q\\geq 2000\\\\ \u0026amp;0.839\\cdot C+25\\cdot Q\\leq 40\\\\ \u0026amp;19\\cdot C+15\\cdot Q\\geq 60\\\\ \u0026amp;C\\geq 0, \\;Q\\geq 0\\end{align} $$\nIt turns out, the optimal solution is that my friend should eat 2.44¬†pounds of carrots per day and 0.91¬†pounds of oatmeal. With that combination, he\u0026rsquo;ll get all his nutrients, restrict his fat, and keep his grocery store bill as low as possible (about $1.73 per day!). Below, I show another graph of the possible carrot-oatmeal combinations. This time, the color of a point represents the cost of that diet. The optimal diet occurs at the black dot, i.e. where the graph is most red. The most expensive diet occurs at the bottom right corner, where the graph is most blue. That diet corresponds to eating nearly 48 pounds of carrots per day!\nOperations researchers use this sort of linear programming mathematical model to solve all kinds of problems. While I don\u0026rsquo;t know of anyone who uses linear programming to fix their diet, the economist George Stigler suggested many years ago it is possible. I will follow up with another post talking about Stigler\u0026rsquo;s Diet problem.\nSee http://www.wolframalpha.com/input/?i=pound+of+carrots%2C+pound+of+oatmeal\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nProgram¬†doesn\u0026rsquo;t refer to computer programming. It actually goes back to an older use of the word related to planning something out.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-01-09T00:00:00Z","permalink":"https://tdhopper.com/blog/carrots-oatmeal-operations-research/","title":"Carrots, Oatmeal, Operations Research"},{"content":"üëã Hey, I\u0026rsquo;m Tim Hopper! I\u0026rsquo;m an experienced machine learning platform engineer and Python developer. You can check out my resume at resume.tdhopper.com.\nFor over 10 years, I\u0026rsquo;ve helped companies solve business problems with machine learning in domains such as banking, cybersecurity, environmental science, and weather forecasting. I see my role as helping data scientists and researchers shorten feedback loops and spend time on their business problems (instead of fussing with cloud resources).\nI\u0026rsquo;m also excited about developer productivity, especially in Python development. I am (slowly) working on an ebook on Python developer tooling and like to help teams use Python more effectively.\n‚úçüèª Writings:\ntdhopper.com has been a place for my thoughts and writings since grad school. If you\u0026rsquo;re new here, start with these:\nSome Reflections on Being Turned Down for a Lot of Data Science Jobs How I Became a Data Scientist Despite Having Been a Math Major and A Subjective and Anecdotal FAQ on Becoming a Data Scientist Goodnight Zoom Entropy of a Discrete Probability Distribution üñ•Ô∏è Personal Projects:\nI have an occasional podcast in which I talk to friends about things they\u0026rsquo;re interested in. You can find it in your podcast directory or at podcast.tdhopper.com.\nYears ago, I created Should I Get a Phd? where I interviewed nine friends about whether a young, bright student should consider pursuing a PhD. This is the resource I wish I\u0026rsquo;d had before starting a PhD program, and it\u0026rsquo;s been useful to many.\nPython Plotting for Exploratory Data Analysis is a Rosetta Stone for Python plotting libraries, and it also compares them to the GOAT of plotting libraries: ggplot.\nI created Notes on Dirichlet Processes after working on a DARPA-funded open source project for developing Bayesian nonparametric models in Python. I did a lot of work to understand Bayesian nonparametrics and derive the Gibbs sampler for Hierarchical Dirichlet Processes. Notes on Dirichlet Processes shares what I learned for the benefit of others.\nI enjoy wildlife and nature photography in my free time. dothopper photo is my gallary.\nFree Disk Space is a little site I maintain with commands for freeing up disk space on your computer.\n‚å®Ô∏è Open Source:\nI love to contribute to open source as I\u0026rsquo;m able. I\u0026rsquo;ve contributed libraries like cpython, datamicroscopes, Streamparse, Conda, lda, and Pandas.\nüë®üèª‚Äçüíª Social Media:\nYou can find me on Twitter and LinkedIn.\nI love Twitter and have written some bangers over the years.\nThis tweet was cited in a Nature article.\nGive a man a fish and you feed him for a day.\nWrite a program to fish for him and you maintain it for a lifetime.\n\u0026mdash; Tim Hopper (@tdhopper) November 3, 2015 Here are some oldies that went viral in their day:\nIf you like it then you shoulda put a ring on it.\u0026#10;\u0026#10;If you love it then you shoulda also endowed non-0 members with a multiplicative inverse.\n\u0026mdash; Tim Hopper (@tdhopper) November 22, 2013 Data science is the art of getting open source tools you\u0026#39;ve previously only read about on Twitter to work in your company\u0026#39;s pipeline.\n\u0026mdash; Tim Hopper (@tdhopper) December 12, 2013 A developer is a device for turning StackOverflow code snippets into programs.\n\u0026mdash; Tim Hopper (@tdhopper) August 20, 2013 üó£Ô∏è Talks:\nI\u0026rsquo;ve been speaking at conferences and meetups for many years. I keep a list of my recorded talks here.\nHere are a couple I\u0026rsquo;m particularly proud of:\n","date":"2010-01-01T00:00:00Z","permalink":"https://tdhopper.com/blog/welcome/","title":"üëã Hey, I'm Tim Hopper!"}]